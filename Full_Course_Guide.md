---

## **GeoAI and Earth Vision: Foundations to Frontier Applications**

**Duration:** 12 weeks (~8 hours/week)

**Format:** Self-paced, project-driven with peer exchange

**Capstone Deliverable:** *Earth Vision Synthesis Portfolio* ‚Äî a reproducible GeoAI workflow demonstrating responsible, interpretable analysis of environmental change in Chile.

**Anchor Case Study:**

*Land Parcelization and Environmental Change in the Los Lagos Region, Chile* ‚Äî tracking the transformation of agricultural lands into peri-urban parcels.

**Supporting Cases:**

- *Hydrological Change Detection:* Exploring the effects of the Chilean megadrought on vegetation and water systems.
- *Ecosystem Health in Lake Llanquihue:* Monitoring eutrophication and turbidity trends using multi-spectral and foundation models.

**Core Stack:**

QGIS ‚Ä¢ Google Earth Engine API ‚Ä¢ Hugging Face Transformers ‚Ä¢ PyTorch Lightning ‚Ä¢ TorchGeo ‚Ä¢ geemap ‚Ä¢ UMAP-learn ‚Ä¢ Plotly

---

### **üß≠ Course Logic**

The course is structured around four progressive *learning arcs* that reflect both the evolution of computer vision and the cognitive progression from perception to synthesis.

Together, they culminate in 17 learning outcomes that bridge technical mastery, interpretive insight, and responsible practice.

---

### **Arc 1 ‚Äî Foundations of Seeing (Weeks 0‚Äì3)**

> From imagery to interpretation.
> 

Build technical and conceptual literacy in remote sensing, geospatial data handling, and classical computer vision. Students move from exploring spectral signatures to developing their first convolutional models.

**Focus Topics:**

- Spectral indices (NDVI, NDWI, NDBI)
- Radiometry and resolution trade-offs
- CNN architectures and interpretability (Grad-CAM)
- Bias, data geography, and the ethics of ‚Äúseeing‚Äù

**Capstone Contribution:**

Establishes the reproducible workspace, dataset boundaries, and baseline CNN model.

---

### **Arc 2 ‚Äî Learning to Represent (Weeks 4‚Äì6)**

> From labels to latent meaning.
> 

Transition from supervised learning toward unsupervised, transfer, and self-supervised approaches that allow models to *discover* structure within data.

Students visualize and interpret embeddings, laying the groundwork for multimodal reasoning.

**Focus Topics:**

- Transfer learning and multi-sensor data fusion
- Representation visualization (PCA, t-SNE, UMAP)
- Self-supervised learning (SimCLR, BYOL)
- Peer review and interpretive communication

**Capstone Contribution:**

Creates the first robust, reusable feature representation pipeline and visual interpretation framework.

---

### **Arc 3 ‚Äî Learning to Reason (Weeks 7‚Äì10)**

> From perception to planetary reasoning.
> 

Apply advanced architectures that combine vision, language, and global-scale pretraining to interpret Earth processes.

Students engage with transformer architectures and geospatial foundation models to reason about planetary patterns and ethical implications.

**Focus Topics:**

- Vision Transformers (ViT) and multimodal CLIP models
- Geospatial foundation models (Prithvi 2.0, Granite Geo, Alpha Earth)
- Cross-model integration and reproducibility audits
- Responsible AI and environmental ethics

**Capstone Contribution:**

Develops the full technical pipeline and comparative model evaluation forming the backbone of the final synthesis.

---

### **Arc 4 ‚Äî Learning to Integrate & Reflect (Weeks 11‚Äì12)**

> From models to meaning.
> 

Synthesize all learned methods into dynamic, interpretable Earth system analyses.

Students model temporal and relational processes (e.g., vegetation change, water quality, parcel connectivity) and produce a final open-science portfolio with ethical reflection and curriculum translation.

**Focus Topics:**

- Spatiotemporal models (ConvLSTM, Temporal Transformers)
- Graph neural networks for spatial connectivity (GAT)
- Regenerative and Responsible AI frameworks
- Capstone symposium and open-science publication readiness

**Capstone Contribution:**

Culminates in the *Earth Vision Synthesis Portfolio* ‚Äî an integrated pipeline, visualization dashboard, Responsible GeoAI essay, and graduate-course reflection memo.

---

### **üå± Course Ethos**

This course embodies three core principles that guide every technical and reflective component:

1. **Openness:** All workflows use open-source tools and FAIR-compliant practices for global accessibility.
2. **Interpretability:** Models are never black boxes; visualization and explanation are integral to learning.
3. **Regeneration:** Every analysis asks not only *what the model sees*, but *what futures it can support* ‚Äî emphasizing social and ecological wellbeing.

---

---

## **üéØ¬†Learning Outcomes for GeoAI and Earth Vision**

By the end of this 12-week course, learners will be able to demonstrate conceptual, technical, and integrative mastery of **GeoAI** ‚Äî from classical vision to foundation-model reasoning ‚Äî through open, ethical, and reproducible environmental applications.

---

### **A. Conceptual Understanding ‚Äî Foundations of Seeing (Arc 1)**

1. **Explain** how Earth observation imagery transforms physical processes (light, vegetation, water, soil) into digital representations.
2. **Differentiate** the main architectures and learning paradigms in computer vision (CNNs, self-supervised, transformer, foundation models).
3. **Describe** the unique challenges and opportunities of applying computer vision to geospatial and environmental contexts (multi-scale, temporal, ethical).
4. **Discuss** the historical evolution from object recognition to *Earth Vision* and the epistemic implications of algorithmic ‚Äúseeing.‚Äù

---

### **B. Technical Competence ‚Äî Learning to Represent (Arc 2)**

1. **Operate** an open-source GeoAI workflow integrating QGIS, Google Earth Engine, Hugging Face, and PyTorch for data ingestion, modeling, and visualization.
2. **Implement** and **fine-tune** convolutional neural networks (CNNs) and transfer-learning pipelines for land-cover and environmental mapping.
3. **Apply** self-supervised and contrastive learning techniques (e.g., SimCLR, BYOL) to generate and interpret geospatial embeddings.
4. **Visualize** and **analyze** latent feature spaces using dimensionality-reduction methods (PCA, t-SNE, UMAP) to explore learned representations.
5. **Evaluate** model performance with quantitative metrics (accuracy, IoU, F1, R¬≤) and qualitative interpretability tools (Grad-CAM, attention maps).

---

### **C. Applied Reasoning ‚Äî Learning to Reason (Arc 3)**

1. **Deploy** advanced architectures ‚Äî Vision Transformers, CLIP-style vision-language models, and geospatial foundation models (Prithvi 2.0, Granite Geo, Alpha Earth) ‚Äî to analyze environmental change.
2. **Integrate** multi-sensor and multi-modal data sources (Sentinel-2, MODIS, SRTM, HydroSHEDS) into unified analytic pipelines.
3. **Construct** spatiotemporal or graph-based models (ConvLSTM, Temporal Transformer, GNN) to represent dynamic Earth-system processes.
4. **Interpret** model outputs in relation to underlying physical, ecological, and socio-environmental processes, linking quantitative patterns to real-world change.

---

### **D. Integrative Practice ‚Äî Learning to Integrate & Reflect (Arc 4)**

1. **Design and execute** a reproducible end-to-end GeoAI project addressing a real environmental challenge (parcelization, drought, eutrophication).
2. **Communicate** technical findings through clear, annotated notebooks, visual dashboards, and narrative synthesis accessible to both scientific and community audiences.
3. **Collaborate** using open-science best practices ‚Äî version control, metadata standards, documentation, and FAIR data principles.
4. **Critically reflect** on the ethical, environmental, and epistemic implications of GeoAI, articulating a framework for *Responsible and Regenerative AI* in Earth observation.

---

### **üåê Alignment to Learning Arcs**

| **Learning Arc** | **Focus** | **Primary Outcomes** |
| --- | --- | --- |
| **Arc 1 ‚Äì Foundations of Seeing** | Remote sensing, spectral literacy, classical vision | 1‚Äì4 |
| **Arc 2 ‚Äì Learning to Represent** | Representation learning, embeddings, self-supervision | 5‚Äì9 |
| **Arc 3 ‚Äì Learning to Reason** | Foundation models, transformers, multimodal reasoning | 10‚Äì13 |
| **Arc 4 ‚Äì Learning to Integrate & Reflect** | Capstone synthesis, ethics, open science | 14‚Äì17 |

---

## **üåç¬†GeoAI and Earth Vision: Foundations to Frontier Applications**

**Duration:** 12 weeks (~8 hrs/week)

**Anchor Case:** *Land Parcelization and Environmental Change in the Los Lagos Region, Chile*

**Supporting Cases:** *Hydrological Change Detection (Megadrought)* and *Ecosystem Health in Lake Llanquihue*

**Stack:** QGIS ‚Ä¢ Google Earth Engine API ‚Ä¢ Hugging Face Transformers ‚Ä¢ PyTorch Lightning ‚Ä¢ TorchGeo ‚Ä¢ geemap

---

### **üß≠¬†Course Logic**

The course is organized around four progressive *learning arcs* that collectively achieve all 17 outcomes:

1. **Arc 1 ‚Äî Foundations of Seeing (Weeks 1‚Äì3):** Build spatial and conceptual literacy in remote sensing + classical CV
2. **Arc 2 ‚Äî Learning to Represent (Weeks 4‚Äì6):** Move from supervised to self-supervised and embedding-based understanding
3. **Arc 3 ‚Äî Learning to Reason (Weeks 7‚Äì10):** Engage transformer, vision-language, and foundation models
4. **Arc 4 ‚Äî Learning to Integrate and Reflect (Weeks 11‚Äì12):** Model dynamic Earth processes and synthesize ethical, reproducible, and curricular insights

---

## **üìò¬†Outcome-Aligned Weekly Outline**

| **Week** | **Theme & Guiding Question** | **Key Learning Outcomes Supported** | **Core Content & Activities** | **Deliverables** |
| --- | --- | --- | --- | --- |
| **0. Orientation** | *How do we build a reproducible GeoAI lab?* | 5, 13 | Environment setup (QGIS + GEE + HF), GitHub repo, data folder conventions | Validated runtime + documented environment setup notebook |
| **1. The Earth as an Image** | *What does ‚Äúseeing‚Äù mean in remote sensing?* | 1, 4, 5 | Radiometry, indices (NDVI, NDTI), spatial resolution, data ethics | Notebook #1: spectral feature exploration + reflection |
| **2. From Vision to GeoVision** | *How did computer vision evolve toward Earth Vision?* | 1, 2, 4, 14 | CNN revolution; land-cover classification; fairness and bias in imagery | Short essay: ‚ÄúWhy GeoAI matters for equitable Earth observation‚Äù |
| **3. Convolutional Intelligence for Landscapes** | *What can CNNs learn about the Earth‚Äôs surface?* | 2, 5, 6, 9, 10 | Train ResNet on EuroSAT/Los Lagos tiles; Grad-CAM interpretability | Notebook #2 + heatmap visualization |
| **4. Transfer Learning and Multi-Source Fusion** | *How do pretrained networks generalize to local context?* | 6, 9, 11 | Fine-tune EfficientNet with Sentinel-2 + MODIS; fusion pipelines | Mini-project 1: Land-use classifier + report |
| **5. Representation Learning without Labels** | *Can models learn Earth structure autonomously?* | 3, 7, 10 | Self-supervised contrastive learning; embedding visualization | Notebook #3: embedding map of Los Lagos |
| **6. From Features to Meaning** | *How do embeddings reveal process-level change?* | 3, 7, 9, 12 | Cluster analysis + UMAP; interpret emerging peri-urban zones | Two-page insight memo linking embeddings to parcelization |
| **7. Transformers for the Planet** | *How does attention change how models ‚Äúsee‚Äù?* | 2, 3, 6, 9, 10 | Vision Transformers, positional encoding, intuitive math | Notebook #4: ViT land-cover experiment |
| **8. Vision‚ÄìLanguage Grounding** | *How can models connect imagery with language?* | 2, 3, 6, 12, 15 | CLIP / BLIP; text-query retrieval (e.g., ‚Äúfields near water‚Äù) | Geo-CLIP demo + discussion reflection |
| **9. Geospatial Foundation Models I (Prithvi & Granite)** | *How do we harness global-scale pretrained models?* | 2, 5, 6, 9, 11, 16 | Prithvi fine-tuning (parcelization); Granite few-shot flood mapping | Project 2 report + model comparison dashboard |
| **10. Geospatial Foundation Models II (Alpha Earth & Ethics)** | *What responsibilities come with planetary AI?* | 4, 6, 9, 14, 16 | Alpha Earth embeddings; sustainability cost analysis; bias auditing | Reflection essay + notebook: water-quality mapping |
| **11. Spatiotemporal & Graph-Based Modeling** | *How do we capture change through time and space?* | 3, 8, 9, 11, 12 | ConvLSTM + Temporal Transformer + Graph Attention Network | Notebook #5: NDVI time-series prediction |
| **12. Synthesis & Future Pathways** | *How do we integrate, reflect, and teach GeoAI?* | 10 ‚Üí 17 (all capstone outcomes) | Final integration; open-science packaging; curriculum design reflection | Capstone: full pipeline repo + presentation + syllabus reflection draft |

---

---

## **üß©¬†Assessment and Evidence of Mastery**

| **Outcome Category** | **Evidence of Learning** | **Evaluation Method** |
| --- | --- | --- |
| Conceptual (1‚Äì4) | Short essays, annotated notebooks, discussion reflections | Instructor peer-review rubric |
| Technical (5‚Äì9) | Five lab notebooks + two mini-projects + capstone | Quantitative model metrics + code reproducibility |
| Applied (10‚Äì14) | Case-based projects (Chile parcelization, drought, eutrophication) | Project deliverables + reflection memos |
| Meta (15‚Äì17) | Research-question brief + curriculum-design reflection | Presentation + written synthesis |

---

---

# Course Flow

```jsx
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                         ‚îÇ GEOAI AND EARTH VISION: FOUNDATIONS TO FRONTIER        ‚îÇ
                         ‚îÇ "Seeing ‚Üí Representing ‚Üí Reasoning ‚Üí Integrating"       ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ARC 1: FOUNDATIONS OF SEEING‚îÇ ARC 2: LEARNING TO REPRESENT ‚îÇ ARC 3: LEARNING TO REASON    ‚îÇ ARC 4: INTEGRATE & REFLECT   ‚îÇ
‚îÇ (Weeks 0‚Äì3)                 ‚îÇ (Weeks 4‚Äì6)                  ‚îÇ (Weeks 7‚Äì10)                ‚îÇ (Weeks 11‚Äì12)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

  WEEK 0  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Orientation + Data Onboarding
     ‚Ä¢ QGIS + GEE + Hugging Face environment setup
     ‚Ä¢ Select capstone case (Los Lagos, drought, eutrophication)
     ‚Ä¢ GitHub repo + reproducibility plan
     [Outcomes: 5, 10, 13]

  WEEK 1  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  The Earth as an Image
     ‚Ä¢ Radiometry, indices, spatial resolution
     ‚Ä¢ Spectral exploration in Sentinel-2
     ‚Ä¢ Start recurring ‚ÄúEthics Thread‚Äù
     [Outcomes: 1, 4, 5]

  WEEK 2  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  From Vision ‚Üí GeoVision
     ‚Ä¢ Evolution of CV to GeoAI
     ‚Ä¢ Data bias, accessibility, sustainability
     ‚Ä¢ Reflection: ‚ÄúWhy GeoAI Matters for Equitable Earth Observation‚Äù
     [Outcomes: 1, 2, 4, 14]

  WEEK 3  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  CNNs for Landscapes
     ‚Ä¢ ResNet land-cover classification (Los Lagos tiles)
     ‚Ä¢ Grad-CAM interpretability
     [Outcomes: 2, 5, 6, 9, 10]

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ARC TRANSITION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     ‚ÄúFrom learning to see ‚Üí learning to represent‚Äù
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

  WEEK 4  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Transfer Learning + Multi-Source Fusion
     ‚Ä¢ EfficientNet fine-tuning with Sentinel + MODIS
     ‚Ä¢ Data fusion pipeline = capstone baseline
     [Outcomes: 6, 9, 11]

  WEEK 5  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Representation Integration + Intro to Self-Supervision
     ‚Ä¢ Bridge CNN ‚Üí SSL understanding
     ‚Ä¢ Begin contrastive learning setup
     [Outcomes: 2, 3, 6, 7, 9]

  WEEK 6  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Embeddings & Interpretation + Peer Review #1
     ‚Ä¢ SimCLR/BYOL + UMAP visualization
     ‚Ä¢ Peer feedback on baseline pipelines
     ‚Ä¢ Ethics Thread check-in
     [Outcomes: 3, 7, 9, 12, 13]

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ARC TRANSITION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     ‚ÄúFrom representation ‚Üí reasoning‚Äù
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

  WEEK 7  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Vision‚ÄìLanguage Fusion (ViT + CLIP)
     ‚Ä¢ Vision Transformers & attention visualization
     ‚Ä¢ CLIP for semantic search (e.g., ‚Äúfields near water‚Äù)
     [Outcomes: 2, 3, 6, 9, 10, 12]

  WEEK 8  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Foundation Models I ‚Äî Prithvi & Granite
     ‚Ä¢ Fine-tune for parcelization & flood detection
     ‚Ä¢ Model cards + documentation
     [Outcomes: 5, 6, 9, 11, 16]

  WEEK 9  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Foundation Models II ‚Äî Alpha Earth + Ethics Deep Dive
     ‚Ä¢ Embedding extraction & visualization
     ‚Ä¢ Sustainability, bias, and equitable access discussion
     [Outcomes: 4, 6, 9, 14, 16]

  WEEK 10 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Integration Sprint + Peer Review #2
     ‚Ä¢ Debug, harmonize models & datasets
     ‚Ä¢ Interim presentation + feedback
     [Outcomes: 6, 9, 12, 13]

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ARC TRANSITION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     ‚ÄúFrom reasoning ‚Üí integration and reflection‚Äù
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

  WEEK 11 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Spatiotemporal & Graph-Based Modeling + Capstone Design Review
     ‚Ä¢ ConvLSTM for NDVI time-series
     ‚Ä¢ Graph Attention Network for watershed links
     ‚Ä¢ Peer mini-presentations of preliminary capstone findings
     [Outcomes: 3, 8, 9, 11, 12]

  WEEK 12 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Final Integration + Reflection Symposium
     ‚Ä¢ Submit Capstone: Earth Vision Synthesis Portfolio
     ‚Ä¢ Responsible & Regenerative GeoAI essay
     ‚Ä¢ Curriculum translation reflection (graduate course design)
     [Outcomes: 10‚Äì17]

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     CONTINUOUS THREADS (SPAN ALL ARCS)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  ‚Ä¢ **Ethics Thread:** short weekly reflection connecting technical work to equity, bias, sustainability  
  ‚Ä¢ **Open Science Practices:** GitHub versioning, documentation, reproducibility standards  
  ‚Ä¢ **Visualization & Communication:** storytelling with QGIS + Python visual tools  
  ‚Ä¢ **Peer Learning:** feedback checkpoints (Week 6, Week 10) to exchange code and insight  
  ‚Ä¢ **Capstone Build-Up:** progressive assembly of datasets, models, and narratives feeding into final portfolio  
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
```

# **Week 0 ‚Äî Orientation + Data Onboarding**

---

---

### **1. Theme / Title**

**‚ÄúPreparing to See: Tools, Teams, and the Data Universe‚Äù**

---

### **2. Guiding Questions**

- What tools and workflows will we use to explore and model Earth data?
- How can we ensure our work is open, reproducible, and transparent from day one?
- How do our regions of interest (ROIs) connect to real environmental challenges?
- What does it mean to work ethically and collaboratively in GeoAI?

---

### **3. Learning Objectives**

By the end of Week 0, you will be able to:

1. Install and configure your full software stack (QGIS, Python environment, Hugging Face, Earth Engine API, GitHub).
2. Set up a reproducible workflow using GitHub version control and environment management.
3. Select and document your *capstone area of interest* (e.g., Los Lagos parcelization, Chilean megadrought, or Llanquihue Lake).
4. Organize your project folder and define your data management plan.
5. Reflect on your motivations and goals for mastering Earth Vision and GeoAI.

---

### **4. Lecture / Reading Topics**

- **Mini-Lecture:** *‚ÄúWelcome to Earth Vision: Why We Learn from the Planet.‚Äù*
- **Core Concepts:**
    - What is GeoAI? How it differs from conventional computer vision
    - Overview of course architecture and capstone expectations
    - Principles of open science and reproducibility
    - Setting up the learning ecosystem: tools and repositories
- **Readings & Resources:**
    - Wilkinson et al. (2016) *The FAIR Guiding Principles for Scientific Data Management.*
    - GitHub Docs: *Getting Started with Git and Markdown.*
    - QGIS 3.34 Manual: *Interface Overview and Plugins.*
    - Hugging Face Docs: *Introduction to Models and Datasets.*
    - Earth Engine Docs: *Python API Quickstart.*
    - Ethics Reading (Optional): *‚ÄúOpen Doesn‚Äôt Mean Equal: Access, Power, and the Ethics of Sharing.‚Äù*

---

### **5. Hands-On Lab / Technical Exercise**

**Lab 1: Environment Setup & Repository Initialization**

**Objective:**

Establish your computational and organizational environment for the course and capstone.

**Tasks**

1. Install core tools:
    - QGIS (latest stable version)
    - Python (‚â•3.10), Conda or Mamba environment
    - earthengine-api, torch, torchgeo, huggingface_hub, rasterio, matplotlib, geopandas, umap-learn
2. Create a GitHub repository named earth-vision-2025.
3. Add folders: /data, /notebooks, /outputs, /docs.
4. Push your first commit: ‚ÄúWeek 0 setup complete.‚Äù
5. Export a list of installed packages (environment.yml).

**Lab 2: Data Onboarding & AOI Definition**

**Objective:**

Identify, describe, and visualize your primary study area.

**Tasks**

1. In QGIS or Earth Engine, define your **Area of Interest (AOI)**.
    - Option 1: Los Lagos Region ‚Äî parcelization
    - Option 2: Central Chile ‚Äî vegetation response to megadrought
    - Option 3: Llanquihue Lake ‚Äî eutrophication and water quality
2. Create a boundary shapefile or GeoJSON.
3. Save a baseline satellite image (Sentinel-2, true color composite).
4. Write a short ‚ÄúAOI summary‚Äù describing your chosen case and why it matters.

**Outputs**

- environment.yml
- GitHub repo structure screenshot or link
- AOI_boundary.geojson
- AOI_summary.md (‚âà 1 page narrative)

---

### **6. Discussion / Reflection Prompt**

**Prompt 1:** ‚ÄúWhat drew you to GeoAI and Earth Vision ‚Äî and what change do you hope your work will make possible?‚Äù

**Prompt 2:** ‚ÄúWhat principles of open science and responsible AI do you want to embody in your work?‚Äù

*(Post reflections to the shared Notion ‚ÄòIntroductions & Intentions‚Äô page; reply to two peers.)*

---

### **7. Weekly Deliverables**

1. Functional local environment verified by running a test notebook (import torch, rasterio, ee).
2. GitHub repository with structured folders and environment file.
3. AOI shapefile/GeoJSON + 1-page AOI summary.
4. Reflection post on motivation and ethics.

---

### **8. Evaluation / Feedback**

- **Rubric Weights:**
    - Environment completeness ‚Äì 30 %
    - Repository organization & reproducibility ‚Äì 25 %
    - AOI definition & clarity ‚Äì 25 %
    - Reflection engagement ‚Äì 20 %
- **Feedback Mode:** Instructor verification of environment and GitHub repo; written feedback on AOI summaries.
- **Outcome:** Students are technically ready, conceptually oriented, and personally invested in the course journey.

---

### **9. Capstone Linkage**

| **Capstone Component** | **How Week 0 Advances It** |
| --- | --- |
| **1. Executive Narrative** | Captures the student‚Äôs personal motivation and research question. |
| **2. Technical Pipeline** | Establishes repository and environment scaffolding. |
| **3. Spatiotemporal Extension** | Defines spatial extent for later temporal data downloads. |
| **4. Visualization & Communication** | Produces initial AOI map and baseline imagery. |
| **5. Open-Science Reflection** | Introduces FAIR data principles and reproducibility habits. |

---

## **üß≠¬†Week 0 Rhythm**

| **Day** | **Focus** | **Output** |
| --- | --- | --- |
| Mon | Orientation lecture + introductions | Notion intro post |
| Tue | Install and configure tools | Working environment |
| Wed | Create and push GitHub repo | earth-vision-2025 repo |
| Thu | Define and visualize AOI | AOI_boundary.geojson |
| Fri | Reflection + Q&A | AOI_summary.md |
| Weekend | Optional system stress test | Run sample Earth Engine script |

---

### **‚úÖ¬†Outcome Alignment**

- **Primary Outcomes Mastered:** 5, 10, 13
- **Secondary Outcomes Advanced:** 1, 4
- **Capstone Readiness Level:** ‚âà 0‚Äì3 % ‚Äî environment, dataset, and ethical foundation established.

---

Would you like me to continue with **Week 11 and 12 merged summary** (a two-week capstone wrap-up guide), or move forward to generate the **course-wide assessment framework** (rubrics, weights, and grading scheme) to ensure alignment across all 12 weeks?

# **Week 1 ‚Äî The Earth as an Image**

---

### **1. Theme / Title**

**‚ÄúSpectral Eyes: Understanding the Earth as an Image‚Äù**

---

### **2. Guiding Questions**

- How do satellites capture the Earth‚Äôs surface ‚Äî what are we really ‚Äúseeing‚Äù?
- What do spectral bands represent, and how can they reveal land, water, and vegetation patterns?
- How do spatial, temporal, and spectral resolutions influence what we can know?
- What makes an image a *data object* in GeoAI ‚Äî and what gets lost in the process?

---

### **3. Learning Objectives**

By the end of Week 1, you will be able to:

1. Explain how remote-sensing imagery encodes physical processes into digital bands.
2. Load, visualize, and interpret multi-band satellite imagery (Sentinel-2, Landsat).
3. Compute and interpret basic spectral indices (NDVI, NDWI, NDBI).
4. Use QGIS and Python (e.g., Rasterio or Earth Engine) to explore spatial patterns.
5. Communicate findings through annotated visualizations and short reflections.

---

### **4. Lecture / Reading Topics**

- **Mini-Lecture:** *‚ÄúThe Earth as a Sensor: Turning Light into Data.‚Äù*
- **Core Concepts:**
    - Electromagnetic spectrum and surface reflectance
    - Spectral signatures of vegetation, water, soil, built-up areas
    - Radiometric, geometric, and atmospheric correction (conceptual overview)
    - Spatial, temporal, and spectral resolution trade-offs
    - Visualization as interpretation
- **Readings & Resources:**
    - Jensen (2015). *Introductory Digital Image Processing*, Ch. 1‚Äì3.
    - ESA Sentinel-2 User Guide.
    - Google Earth Engine Tutorial: *Introduction to Earth Observation Data*.
    - Blog: *‚ÄúNDVI and Beyond: Reading the Landscape with Indices.‚Äù*
    - Optional: Short video ‚Äî NASA Earth Observatory, *‚ÄúSeeing Earth in a New Light.‚Äù*

---

### **5. Hands-On Lab / Technical Exercise**

**Lab: Exploring the Earth as Data**

**Objective:**

Develop an intuitive understanding of spectral imagery by visualizing and analyzing real satellite data.

**Tasks**

1. Select a region of interest (e.g., Los Lagos, Chile).
2. Retrieve recent Sentinel-2 imagery (via QGIS plugin or Earth Engine).
3. Visualize RGB composites (true color and false color).
4. Compute NDVI and NDWI indices in Python (rasterio, numpy, or GEE).
5. Generate and interpret a 2x2 panel visualization comparing raw vs. indices.
6. Annotate your map: What patterns stand out? What might they mean ecologically?

**Outputs**

- earth_as_image.ipynb
- ndvi_ndwi_composite.png
- Annotated figure with interpretation captions
- week1_reflection.md (‚âà 1 page summary)

---

### **6. Discussion / Reflection Prompt**

**Prompt 1:** ‚ÄúWhat does the satellite see that humans cannot?‚Äù

**Prompt 2:** ‚ÄúHow does translating light into numbers change our relationship to nature?‚Äù

**Prompt 3:** ‚ÄúWhat responsibilities come with transforming the Earth into data?‚Äù

*(Post to shared Notion board; respond to at least one peer.)*

---

### **7. Weekly Deliverables**

1. Jupyter notebook (earth_as_image.ipynb) with visualizations.
2. Annotated composite map (ndvi_ndwi_composite.png).
3. Reflection post (week1_reflection.md).

---

### **8. Evaluation / Feedback**

- **Rubric Weights:**
    - Technical completeness & correctness ‚Äì 35 %
    - Visualization quality ‚Äì 25 %
    - Interpretive depth ‚Äì 25 %
    - Reflection insight ‚Äì 15 %
- **Feedback Mode:** Instructor written notes on GitHub PR; optional short debrief call with peers.
- **Outcome:** Solid understanding of imagery as data and readiness for conceptual integration in Week 2.

---

### **9. Capstone Linkage**

| **Capstone Component** | **How Week 1 Advances It** |
| --- | --- |
| **1. Executive Narrative** | Establishes the conceptual language for describing Earth data and imagery. |
| **2. Technical Pipeline** | Introduces QGIS and Python workflows for image preprocessing. |
| **3. Spatiotemporal Extension** | Builds baseline spatial dataset for future temporal analysis. |
| **4. Visualization & Communication** | Teaches essential skills for map-based storytelling. |
| **5. Open-Science Reflection** | Begins culture of annotated, shareable, reproducible notebooks. |

---

## **üß≠¬†Week 1 Rhythm**

| **Day** | **Focus** | **Output** |
| --- | --- | --- |
| Mon | Mini-lecture + readings | Notes + initial dataset setup |
| Tue | QGIS and Sentinel-2 visualization | RGB + false-color composites |
| Wed | NDVI/NDWI computation | Index maps |
| Thu | Visual interpretation + annotation | Composite figure + captions |
| Fri | Reflection and discussion | Reflection post + peer reply |
| Weekend | Optional: Extra visualization challenge | Animated NDVI over time (optional) |

---

### **‚úÖ¬†Outcome Alignment**

- **Primary Outcomes Mastered:** 1, 4, 5
- **Secondary Outcomes Advanced:** 9, 12, 13
- **Capstone Readiness Level:** ‚âà 5 % ‚Äî students gain a tactile, visual, and conceptual foundation for the rest of the course.

---

# **üåé¬†Week 2 ‚Äî From Vision ‚Üí GeoVision**

---

### **1. Theme / Title**

**‚ÄúFrom Seeing to Knowing: The Evolution of Computer Vision into GeoAI‚Äù**

---

### **2. Guiding Questions**

- How did computer vision evolve from object recognition to environmental intelligence?
- What makes geospatial vision tasks distinct from conventional computer vision (context, scale, time, ethics)?
- How do bias, accessibility, and representation shape what we call ‚ÄúEarth observation‚Äù?
- What does ‚ÄúResponsible AI for Earth‚Äù mean, and how do we operationalize it?

---

### **3. Learning Objectives**

By the end of Week 2, you will be able to:

1. Explain the conceptual link between classic computer vision and emerging GeoAI approaches.
2. Identify the unique challenges of applying AI to geospatial data (scale, multi-modality, temporal variability).
3. Summarize the ethical and equity issues inherent in global Earth-observation datasets.
4. Articulate how GeoAI frameworks enable environmental insight and accountability.
5. Initiate the continuous *Ethics Thread* reflections for the remainder of the course.

---

### **4. Lecture / Reading Topics**

- **Mini-Lecture:** *‚ÄúFrom Object Recognition to Planetary Understanding.‚Äù*
- **Core Concepts:**
    - Overview of computer-vision history (feature engineering ‚Üí deep learning)
    - Transition to GeoAI ‚Äî contextual, multi-scale, and multi-sensor data
    - The role of scale and geography in feature meaning
    - Equity and power in data production ‚Äî who sees, who is seen
- **Readings & Resources:**
    - Zhu et al. (2017). *Deep Learning in Remote Sensing: A Comprehensive Review.*
    - Reichstein et al. (2019). *Deep Learning and Process Understanding in Earth System Science.*
    - UNESCO (2023). *Ethical AI for Sustainability.*
    - Blog: *‚ÄúThe Hidden Geography of Training Data.‚Äù*
    - **Ethics Thread Launch Reading:** Crawford (2021) *Atlas of AI*, Ch. 1 ‚ÄúThe Extractive View.‚Äù

---

### **5. Hands-On Lab / Technical Exercise**

**Lab: Revisiting Vision ‚Äî From ImageNet to GeoVision**

**Objective:** Explore how pre-trained visual models behave on Earth-observation data to highlight the need for domain adaptation.

**Tasks**

1. Load a pre-trained ImageNet model (e.g., ResNet-50) using Hugging Face or TorchVision.
2. Run inference on small Sentinel-2 or NAIP image patches (4‚Äì6 classes).
3. Observe mis-classifications and note how model assumptions fail on geospatial imagery.
4. Visualize activations (Grad-CAM) and discuss differences from Week 1 spectral patterns.
5. Write a short reflection connecting these errors to bias and scale mismatch.

**Outputs**

- imagenet_to_geovision.ipynb
- Sample activation maps (geovision_gradcam.png)
- reflection_bias_scale.md (‚âà 1 page)

---

### **6. Discussion / Reflection Prompt**

**Prompt 1:** ‚ÄúWhat happens when models trained to see cats and cars are asked to see forests and rivers?‚Äù

**Prompt 2:** ‚ÄúHow does data geography (who collects, where, when) influence model truth?‚Äù

**Prompt 3:** ‚ÄúWhat responsibilities arise when translating computer-vision models to Earth applications?‚Äù

*(Post to #EthicsThread to launch the continuous reflection series.)*

---

### **7. Weekly Deliverables**

1. imagenet_to_geovision.ipynb notebook with activation outputs.
2. 1-page bias + scale reflection memo.
3. Initial Ethics Thread post + two peer comments.

---

### **8. Evaluation / Feedback**

- **Rubric Weights:**
    - Conceptual understanding ‚Äì 30 %
    - Technical exercise ‚Äì 30 %
    - Interpretation & reflection ‚Äì 30 %
    - Peer engagement ‚Äì 10 %
- **Feedback Mode:** Instructor comments in Notion and GitHub PR notes.
- **Outcome:** Conceptual foundation for CNN design in Week 3 and ethical mindset that continues through Week 12.

---

### **9. Capstone Linkage**

| **Capstone Component** | **How Week 2 Advances It** |
| --- | --- |
| **1. Executive Narrative** | Establishes theoretical and ethical framing for interpreting model outputs. |
| **2. Technical Pipeline** | Introduces concept of transfer from generic vision ‚Üí Earth domain. |
| **3. Spatiotemporal Extension** | Raises awareness of data scale & context for later temporal analysis. |
| **4. Visualization & Communication** | Teaches visual critique & interpretive literacy. |
| **5. Open-Science Reflection** | Launches continuous Ethics Thread for responsible practice. |

---

## **üß≠¬†Week 2 Rhythm**

| **Day** | **Focus** | **Output** |
| --- | --- | --- |
| Mon | Mini-lecture + Readings | Notes + Ethics Thread draft |
| Tue | Run ImageNet inference on satellite data | Misclassification examples |
| Wed | Activation visualization | Grad-CAM plots |
| Thu | Write bias and scale reflection | reflection_bias_scale.md |
| Fri | Launch Ethics Thread discussion | Post + peer comments |
| Weekend | Prepare for CNN Week | Finalize repo and datasets |

---

### **‚úÖ¬†Outcome Alignment**

- **Primary Outcomes Mastered:** 1, 2, 4, 14
- **Secondary Outcomes Advanced:** 5, 13
- **Capstone Readiness Level:** ‚âà 10 % ‚Äî students now have a critical lens for applying computer vision to geospatial data and are ready to build their first CNN in Week 3.

---

# **Week 3 ‚Äî CNNs for Landscapes**

---

### **1. Theme / Title**

**‚ÄúTeaching the Machine to See: Convolutional Neural Networks for Earth Imagery‚Äù**

---

### **2. Guiding Questions**

- How do CNNs extract features from complex, high-dimensional satellite imagery?
- What patterns do convolutional filters actually detect?
- How do we measure model accuracy and interpret mistakes in a geospatial context?
- How can simple CNNs already reveal land-use dynamics and environmental boundaries?

---

### **3. Learning Objectives**

By the end of Week 3, you will be able to:

1. Build and train a CNN for land-cover or land-use classification from Sentinel-2 imagery.
2. Preprocess imagery (tiling, normalization, label encoding) for deep learning tasks.
3. Visualize feature maps and activation patterns to interpret what the network has learned.
4. Compute standard evaluation metrics (accuracy, IoU, F1, confusion matrix).
5. Document a reproducible workflow that serves as the first module of the Capstone Pipeline.

---

### **4. Lecture / Reading Topics**

- **Mini-Lecture:** *‚ÄúConvolution and Recognition: How Machines See Patterns.‚Äù*
- **Core Concepts:**
    - Convolution, filters, stride, padding, and pooling
    - Feature hierarchies (low-level edges ‚Üí high-level objects)
    - Regularization (dropout, batch norm)
    - Training loops and overfitting
    - Model interpretability (Grad-CAM, filter visualization)
- **Readings & Resources:**
    - Goodfellow et al. (2016) *Deep Learning*, Ch. 9 on Convolutional Networks
    - Oquab et al. (2014) *Learning and Transferring Mid-Level Image Representations.*
    - TorchGeo Tutorial ‚Äì *Training CNNs for Land-Cover Classification*
    - Blog: *‚ÄúVisualizing Filters and Activations in CNNs.‚Äù*
    - Ethics Thread Reading: *‚ÄúAlgorithmic Opacity in Environmental AI.‚Äù*

---

### **5. Hands-On Lab / Technical Exercise**

**Lab: CNN Land-Cover Classification and Model Interpretation**

**Objective:** Develop your first deep learning model to classify land cover types and interpret its decisions.

**Tasks**

1. Prepare Sentinel-2 imagery and labels (crops, forest, urban, water).
2. Split data into training/validation sets (70/30).
3. Design a simple CNN using PyTorch or Keras (3‚Äì4 conv layers).
4. Train and record loss/accuracy curves.
5. Evaluate on validation data; produce confusion matrix.
6. Visualize Grad-CAM activation maps for sample predictions.
7. Write a short interpretation: ‚ÄúWhat features does the model seem to use?‚Äù

**Outputs**

- cnn_landcover.ipynb
- gradcam_visuals.png
- metrics_summary.csv
- model_interpretation.md (‚âà 1 page)

---

### **6. Discussion / Reflection Prompt**

**Prompt 1:** ‚ÄúHow does ‚Äòseeing‚Äô through a CNN differ from how humans see?‚Äù

**Prompt 2:** ‚ÄúWhat biases might be encoded through data labels and sampling?‚Äù

**Prompt 3:** ‚ÄúHow can interpretability tools like Grad-CAM help build trust in AI for Earth science?‚Äù

*(Post to #EthicsThread and respond to two peers.)*

---

### **7. Weekly Deliverables**

1. CNN training notebook (cnn_landcover.ipynb).
2. Evaluation plots and Grad-CAM visuals.
3. Metrics summary file.
4. Interpretation memo (1 page).
5. Ethics Thread reflection post.

---

### **8. Evaluation / Feedback**

- **Rubric Weights:**
    - Model implementation ‚Äì 35 %
    - Evaluation & visualization ‚Äì 25 %
    - Interpretation depth ‚Äì 25 %
    - Ethics reflection ‚Äì 15 %
- **Feedback Mode:** Instructor GitHub comments + optional group demo session.
- **Outcome:** A fully trained and interpreted CNN ready for extension via transfer learning (Week 4).

---

### **9. Capstone Linkage**

| **Capstone Component** | **How Week 3 Advances It** |
| --- | --- |
| **1. Executive Narrative** | Provides first quantitative evidence of machine learning insight into land cover. |
| **2. Technical Pipeline** | Creates first functional model block for later integration. |
| **3. Spatiotemporal Extension** | Supplies base features for temporal change detection later. |
| **4. Visualization & Communication** | Develops activation and feature visualization skills. |
| **5. Open-Science Reflection** | Introduces structured experiment logging and reproducible code practices. |

---

## **üß≠¬†Week 3 Rhythm**

| **Day** | **Focus** | **Output** |
| --- | --- | --- |
| Mon | Mini-lecture + Readings | Annotated notes + Ethics draft |
| Tue | Data prep + CNN design | Model architecture ready |
| Wed | Training + Evaluation | cnn_landcover.ipynb + metrics |
| Thu | Interpretation (Grad-CAM) | Activation visuals + memo |
| Fri | Reflection + discussion | Ethics post |
| Weekend | Pipeline cleanup | Repo commit + Week 4 prep |

---

### **‚úÖ¬†Outcome Alignment**

- **Primary Outcomes Mastered:** 2, 5, 6, 9, 10
- **Secondary Outcomes Advanced:** 1, 3, 4, 12
- **Capstone Readiness Level:** ‚âà 15 % ‚Äî students now have a trained CNN and evaluation skills that anchor all subsequent model layers.

---

# **Week 4 ‚Äî Transfer Learning + Multi-Source Fusion**

---

### **1. Theme / Title**

**‚ÄúBuilding on What‚Äôs Been Learned: Transfer Learning and Multi-Sensor Fusion‚Äù**

---

### **2. Guiding Questions**

- How can pretrained models accelerate Earth observation tasks?
- Why and how do we combine data from different sensors (Sentinel-2, MODIS, Landsat)?
- What does ‚Äúdomain adaptation‚Äù mean in the context of geospatial imagery?
- How do we design a robust baseline pipeline that can evolve into a foundation model workflow later?
- How does data fusion expand our capacity to see relationships across scales?

---

### **3. Learning Objectives**

By the end of Week 4, you will be able to:

1. Implement **transfer learning** using pretrained CNNs (e.g., ResNet, EfficientNet).
2. Apply **multi-source data fusion** (e.g., combining Sentinel-2 and MODIS) for richer geospatial representation.
3. Fine-tune pretrained models for regional land-use or hydrological mapping tasks.
4. Evaluate transfer-learning performance using quantitative and qualitative metrics.
5. Construct and document a reproducible **baseline pipeline** to anchor future experiments.

---

### **4. Lecture / Reading Topics**

- **Mini-Lecture:** *‚ÄúStanding on Giants: Transfer Learning and Fusion in GeoAI.‚Äù*
- **Core Concepts:**
    - Transfer learning types: feature extraction vs. fine-tuning
    - Domain shift and dataset bias
    - Multispectral vs. multisensor fusion
    - Strategies for aligning imagery across time and scale
    - Introduction to multi-input models
- **Readings & Resources:**
    - Pan & Yang (2010) *A Survey on Transfer Learning.*
    - Schmitt & Zhu (2016) *Data Fusion in Remote Sensing: Challenges and Opportunities.*
    - TorchGeo documentation: loading multisource datasets.
    - Blog: *‚ÄúWhy You Should Fine-Tune Rather Than Train From Scratch.‚Äù*
    - Ethics Thread Reading: *‚ÄúData Inequality ‚Äî Whose Earth Are We Seeing?‚Äù*

---

### **5. Hands-On Lab / Technical Exercise**

**Lab: Building the Baseline Pipeline ‚Äî Transfer Learning + Data Fusion**

**Objective:**

Fine-tune a pretrained CNN on a multi-sensor dataset and establish a documented, reproducible pipeline as the **baseline for your capstone.**

**Tasks**

1. **Dataset Preparation:**
    - Collect paired Sentinel-2 and MODIS imagery for your AOI (Los Lagos, megadrought area, or Llanquihue Lake).
    - Align spatial resolution and projection (reproject in QGIS or Earth Engine).
2. **Transfer Learning:**
    - Load pretrained **EfficientNet-B0** or **ResNet-50** (ImageNet weights).
    - Replace output layer for your classification/regression task (e.g., land cover or NDVI trend).
    - Fine-tune using a subset of your AOI data.
3. **Data Fusion Experiment:**
    - Stack Sentinel-2 + MODIS bands (or indices such as NDVI, NDWI).
    - Compare performance of single-source vs. fused models.
4. **Evaluation:**
    - Compute accuracy, F1, and confusion matrix.
    - Visualize predictions in QGIS.
5. **Documentation:**
    - Save configuration files (config.yaml), code notebook, and result plots.

**Outputs**

- transfer_fusion_pipeline.ipynb
- config.yaml
- Model outputs (fusion_results.png)
- baseline_report.md (summary of methods, metrics, reflections)

---

### **6. Discussion / Reflection Prompt**

**Prompt 1:** ‚ÄúWhat did transfer learning allow you to achieve that training from scratch would not?‚Äù

**Prompt 2:** ‚ÄúWhat trade-offs emerged in combining multiple data sources?‚Äù

**Prompt 3:** ‚ÄúHow do domain differences across sensors mirror inequities in global data access?‚Äù

*(Post reflections to #EthicsThread; respond to one peer with technical + ethical insight.)*

---

### **7. Weekly Deliverables**

1. Transfer + fusion notebook (transfer_fusion_pipeline.ipynb).
2. Configuration and result files (config.yaml, fusion_results.png).
3. Baseline summary report (Markdown, 1‚Äì2 pages).
4. Ethics Thread reflection post.

---

### **8. Evaluation / Feedback**

- **Rubric Weights:**
    - Technical implementation (transfer + fusion) ‚Äì 35 %
    - Evaluation & visualization ‚Äì 25 %
    - Documentation & reproducibility ‚Äì 20 %
    - Reflection & ethics ‚Äì 20 %
- **Feedback Mode:** Instructor GitHub review + optional group critique during next week‚Äôs integration session.
- **Outcome:** Established a **baseline pipeline**‚Äîthe ‚Äúfirst complete prototype‚Äù for each learner‚Äôs capstone.

---

### **9. Capstone Linkage**

| **Capstone Component** | **How Week 4 Advances It** |
| --- | --- |
| **1. Executive Narrative** | Begins articulating how AI detects environmental patterns across scales. |
| **2. Technical Pipeline** | Builds the first fully reproducible model pipeline. |
| **3. Spatiotemporal Extension** | Prepares datasets for multi-date analysis later in the course. |
| **4. Visualization & Communication** | Produces interpretable baseline maps and evaluation plots. |
| **5. Open-Science Reflection** | Introduces structured documentation for reproducibility. |

---

## **üß≠¬†Week 4 Rhythm**

| **Day** | **Focus** | **Output** |
| --- | --- | --- |
| Mon | Mini-lecture + readings | Annotated notes + Ethics draft |
| Tue | Transfer learning setup | Model loaded, modified, fine-tuning started |
| Wed | Data fusion experiment | Multi-sensor model results |
| Thu | Visualization & evaluation | Maps + metrics |
| Fri | Documentation & discussion | Baseline report + reflection post |
| Weekend | Integration prep for Week 5 | Code cleaned, repo updated |

---

### **‚úÖ¬†Outcome Alignment**

- **Primary Outcomes Mastered:** 5, 6, 9, 11
- **Secondary Outcomes Advanced:** 2, 3, 10, 13
- **Capstone Readiness Level:** ‚âà 25 % ‚Äî students now have a working baseline model and complete pipeline architecture, which will evolve into self-supervised and transformer workflows.

---

# **Week 5 ‚Äî Representation Integration + Intro to Self-Supervision**

---

### **1. Theme / Title**

**‚ÄúSeeing Beyond Labels: Integrating Representations and Preparing for Self-Supervision‚Äù**

---

### **2. Guiding Questions**

- What does a neural network *actually learn* when it classifies landscapes?
- How can we visualize and compare learned representations across models?
- Why might we want to let a model learn without explicit labels?
- How can we start bridging CNN-based and self-supervised learning pipelines?
- What does ‚Äúrepresentation quality‚Äù mean in geospatial contexts?

---

### **3. Learning Objectives**

By the end of Week 5, you will be able to:

1. Extract and visualize **intermediate features** (embeddings) from CNN or EfficientNet models.
2. Compare feature representations learned from different model architectures.
3. Explain conceptually how self-supervised methods (SimCLR, BYOL) learn latent structure.
4. Prepare an unlabelled dataset and data-augmentation pipeline for SSL training.
5. Reflect on interpretability and bias in feature representations.

---

### **4. Lecture / Reading Topics**

- **Mini-Lecture:** *‚ÄúFrom Labels to Latents ‚Äî Why Representation Learning Matters.‚Äù*
- **Concepts:**
    - Feature maps, activations, and dimensionality reduction
    - Embedding space similarity and cluster structure
    - Transfer learning limits ‚Äî when labels constrain discovery
    - Data augmentation for self-supervision (rotation, crop, color jitter)
    - The idea of contrastive learning
- **Readings & Resources:**
    - Bengio et al. (2013). *Representation Learning: A Review and New Perspectives.*
    - A simple blog walkthrough: *‚ÄúVisualizing CNN Feature Spaces.‚Äù*
    - TorchGeo documentation: extracting features from pretrained CNNs.
    - Yannic Kilcher video: *‚ÄúSelf-Supervised Learning Explained.‚Äù*
    - Ethics Thread Reading: *‚ÄúHidden Bias in Learned Representations of Land.‚Äù*

---

### **5. Hands-On Lab / Technical Exercise**

**Lab 1: Visualizing Representations Across Models**

**Objective:** Explore what CNNs have learned by visualizing embeddings and feature activations.

**Tasks**

1. Select three models from Weeks 3‚Äì4 (e.g., ResNet-34, EfficientNet-B0, or your fine-tuned fusion model).
2. Extract the penultimate-layer embeddings for ~2 000 image patches.
3. Apply PCA or t-SNE to project into 2D and visualize clusters.
4. Color-code points by land-cover class, or by spectral index values (NDVI, NDWI).
5. Write a short interpretation of what the structure reveals about model understanding.

**Lab 2: Preparing for Self-Supervision**

1. Build an **unlabelled image dataset** from Sentinel-2 tiles for your AOI.
2. Design a **data-augmentation pipeline** (random crops, rotations, flips, spectral jitter).
3. Save the augmented dataset to your project directory for Week 6.

**Outputs**

- representation_viz.ipynb
- unlabelled_dataset_setup.ipynb
- 2‚Äì3 PNG visuals (PCA/t-SNE)
- representation_summary.md (‚âà 1 page)

---

### **6. Discussion / Reflection Prompt**

**Prompt 1:** ‚ÄúWhat hidden assumptions appear in the patterns your model learned?‚Äù

**Prompt 2:** ‚ÄúIn what ways do visualizations of representation space help (or mislead) our interpretation of reality?‚Äù

**Prompt 3:** ‚ÄúHow might unlabeled data reveal insights that labeled datasets conceal?‚Äù

*(Post reflections to the #EthicsThread channel and comment on two peers.)*

---

### **7. Weekly Deliverables**

1. Feature-space visualization notebook (representation_viz.ipynb).
2. Unlabelled dataset & augmentation notebook (unlabelled_dataset_setup.ipynb).
3. Representation summary memo (1 page).
4. Ethics Thread reflection post.

---

### **8. Evaluation / Feedback**

- **Rubric Weights:**
    - Feature-space visualization clarity ‚Äì 30 %
    - Technical correctness & dataset prep ‚Äì 30 %
    - Analytical interpretation ‚Äì 25 %
    - Reflection quality ‚Äì 15 %
- **Feedback Mode:** Instructor written comments + peer note exchange (short asynchronous review).
- **Outcome:** Conceptual readiness for self-supervised learning (Week 6).

---

### **9. Capstone Linkage**

| **Capstone Component** | **How Week 5 Advances It** |
| --- | --- |
| **1. Executive Narrative** | Builds interpretive language to describe what ‚Äúlearning‚Äù looks like inside a model. |
| **2. Technical Pipeline** | Adds feature-extraction and visualization stage to full workflow. |
| **3. Spatiotemporal Extension** | Produces reusable embeddings for temporal or spatial clustering later. |
| **4. Visualization & Communication** | Generates interpretive figures for capstone report. |
| **5. Open-Science Reflection** | Reinforces transparent documentation and interpretation of learned features. |

---

## **üß≠¬†Week 5 Rhythm**

| **Day** | **Focus** | **Output** |
| --- | --- | --- |
| Mon | Mini-lecture + Readings | Notes + Ethics draft |
| Tue | Feature extraction + PCA | representation_viz.ipynb |
| Wed | Interpretation + Augmentation pipeline | unlabelled_dataset_setup.ipynb |
| Thu | Visualization refinement | PCA/t-SNE figures + memo |
| Fri | Ethics discussion | Post reflection + peer comment |
| Weekend | Prep for Week 6 SSL | Dataset checked and ready |

---

### **‚úÖ¬†Outcome Alignment**

- **Primary Outcomes Mastered:** 2, 3, 6, 7, 9
- **Secondary Outcomes Advanced:** 4, 10, 12, 13
- **Capstone Readiness Level:** ‚âà 30 % ‚Äî students have now visualized what their models learn and are fully equipped to start self-supervised representation learning in Week 6.

---

# **üåé¬†Week 6 ‚Äî Embeddings & Interpretation**

---

### **1. Theme / Title**

**‚ÄúLearning Without Labels: Seeing Patterns Through Embeddings‚Äù**

---

### **2. Guiding Questions**

- How can models learn meaning from unlabelled satellite imagery?
- What do *embeddings* actually represent in a geospatial context?
- How can we visualize and interpret high-dimensional representations?
- What does ‚Äúgood representation‚Äù mean for environmental interpretation?
- How can peer feedback improve both our technical work and storytelling?

---

### **3. Learning Objectives**

By the end of Week 6, you will be able to:

1. Implement a **self-supervised learning (SSL)** workflow using SimCLR or BYOL on Sentinel-2 imagery.
2. Generate and visualize **embedding spaces** using PCA / UMAP / t-SNE.
3. Interpret clusters and neighborhoods in embedding space as meaningful geospatial or ecological groupings.
4. Compare self-supervised embeddings to supervised CNN features.
5. Conduct and respond constructively to a peer technical review.

---

### **4. Lecture / Reading Topics**

- **Mini-Lecture:** *‚ÄúRepresentation Learning: From Labels to Latent Meaning.‚Äù*
- **Key Concepts:**
    - Contrastive learning objectives and data augmentation for remote sensing
    - Positive vs. negative pairs; InfoNCE loss
    - Dimensionality reduction and manifold visualization
    - From embedding space ‚Üí environmental interpretation
- **Readings & Resources:**
    - Chen et al. (2020) *SimCLR: A Simple Framework for Contrastive Learning.*
    - Grill et al. (2020) *BYOL: Bootstrap Your Own Latent.*
    - TorchGeo SSL examples.
    - Hugging Face Course Section on *Self-Supervised Vision Models.*
    - Ethics Thread reading: *‚ÄúData, Discovery, and the Hidden Curriculum of Labels.‚Äù*

---

### **5. Hands-On Lab / Technical Exercise**

**Lab: Building and Interpreting Embeddings for Land-Use Structure**

**Objective:**

Train a self-supervised model to learn visual similarity structure within unlabelled Sentinel-2 tiles from the Los Lagos region.

**Tasks**

1. Prepare ~10 000 unlabelled image patches (cloud-filtered, normalized).
2. Train **SimCLR** or **BYOL** using PyTorch Lightning Bolts.
3. Extract **embedding vectors** and store as .npy or .csv.
4. Apply **UMAP / t-SNE** to visualize clusters (urban ‚Üî rural, forest ‚Üî pasture).
5. Overlay embedding clusters on QGIS map; interpret patterns.
6. Compare results with earlier supervised CNN features (Week 4).

**Outputs**

- ssl_embeddings.ipynb
- embedding_visualization.png / .html
- cluster_interpretation.md

---

### **6. Discussion / Reflection Prompt**

**Prompt 1:** ‚ÄúWhat surprises emerged when the model found its own structure in the data?‚Äù

**Prompt 2:** ‚ÄúWhere might human interpretation still add essential context to embeddings?‚Äù

**Prompt 3:** ‚ÄúHow does learning without labels challenge our assumptions about objectivity in environmental data?‚Äù

*(Post reflections + comment on two peers; tag #EthicsThread.)*

---

### **7. Weekly Deliverables**

1. Self-supervised embedding notebook (ssl_embeddings.ipynb).
2. Visualization artifact (UMAP / t-SNE figure + QGIS overlay).
3. 1-page cluster interpretation memo.
4. **Peer Review #1:** Exchange and critique notebooks (clarity, reproducibility, interpretation).
5. Ethics Thread Reflection post.

---

### **8. Evaluation / Feedback**

- **Peer Evaluation:** structured rubric (shared via Notion)
    - Code reproducibility ‚Äì 25 %
    - Visualization clarity ‚Äì 25 %
    - Interpretive insight ‚Äì 25 %
    - Feedback quality given ‚Äì 25 %
- **Instructor Feedback:** short memo highlighting conceptual grasp and communication growth.
- **Goal:** normalize collaborative learning and constructive critique as core GeoAI practice.

---

### **9. Capstone Linkage**

| **Capstone Component** | **How Week 6 Advances It** |
| --- | --- |
| **1. Executive Narrative** | Generates interpretive vocabulary for describing patterns without labels. |
| **2. Technical Pipeline** | Adds unsupervised representation module to overall workflow. |
| **3. Spatiotemporal Extension** | Provides embedding features for temporal modeling later. |
| **4. Visualization & Communication** | Produces first major 2-D visuals of abstract feature space. |
| **5. Open-Science Reflection** | Introduces peer review and transparent code-sharing workflow. |

---

## **üß≠¬†Week 6 Rhythm**

| **Day** | **Focus** | **Output** |
| --- | --- | --- |
| Mon | Mini-lecture + readings on SSL | Annotated notes + Ethics draft |
| Tue | SimCLR implementation | Training run + embeddings file |
| Wed | Visualization lab (UMAP / t-SNE) | embedding_visualization.png |
| Thu | Peer Review #1 exchange | Feedback forms submitted |
| Fri | Synthesis discussion | Cluster interpretation memo |
| Weekend | Reflection & prep for Week 7 | Final Ethics post |

---

### **‚úÖ¬†Outcome Alignment**

- **Primary Outcomes Mastered:** 3, 7, 9, 12, 13
- **Secondary Outcomes Advanced:** 4, 10, 14
- **Capstone Readiness Level:** ‚âà 40 % ‚Äî students can now produce and interpret embeddings; ready to extend representations to Transformer and Vision‚ÄìLanguage models in Week 7.

---

# **Week 7 ‚Äî Vision‚ÄìLanguage Fusion (ViT + CLIP)**

---

### **1. Theme / Title**

**‚ÄúWhen Vision Meets Language: Teaching AI to Describe the Earth‚Äù**

---

### **2. Guiding Questions**

- How does attention allow a model to ‚Äúsee‚Äù relationships across an entire image?
- In what ways can linking **language + vision** enrich our understanding of Earth processes?
- How can we query large satellite archives semantically ‚Äî e.g., ‚Äúshow me irrigated fields near shrinking lakes‚Äù?
- What are the interpretive and ethical implications of giving AI a ‚Äúvocabulary‚Äù for nature?

---

### **3. Learning Objectives**

By the end of Week 7, you will be able to:

1. Implement and fine-tune a **Vision Transformer (ViT)** for land-cover classification.
2. Apply **CLIP** (Contrastive Language‚ÄìImage Pre-training) for text-to-image and image-to-text retrieval in remote-sensing contexts.
3. Interpret transformer attention maps and token embeddings for spatial reasoning.
4. Integrate language queries to produce explainable, human-readable results.
5. Evaluate multimodal AI systems for fairness, interpretability, and applicability in environmental decision-making.

---

### **4. Lecture / Reading Topics**

- **Mini-Lecture:** *‚ÄúAttention Is All You See: Transformers for Earth Observation.‚Äù*
- **Core Concepts:**
    - Self-attention, positional encoding, tokenization of image patches
    - Multimodal contrastive learning (text ‚Üî image alignment)
    - Embedding similarity, cosine distance, and prompt engineering
    - Applications: image retrieval, semantic search, captioning, and explainable mapping
- **Readings & Resources:**
    - Dosovitskiy et al. (2020) *An Image is Worth 16√ó16 Words (ViT).*
    - Radford et al. (2021) *Learning Transferable Visual Models from Natural Language Supervision (CLIP).*
    - Hugging Face Tutorial: *ViT for Image Classification and CLIP for Image Search.*
    - TorchGeo example: *Applying Transformers to EO Data.*
    - Ethics Thread reading: *‚ÄúThe Language of Labels: Bias in Environmental Semantics.‚Äù*

---

### **5. Hands-On Lab / Technical Exercise**

**Lab: Vision‚ÄìLanguage Fusion for Semantic GeoSearch**

**Objective:** Use ViT and CLIP to connect natural-language prompts with remote-sensing imagery.

**Tasks:**

1. Fine-tune **ViT-Base** on EuroSAT or Los Lagos tiles (10‚Äì12 classes).
2. Load a **CLIP model** (openai/clip-vit-base-patch16) and create a text query list (e.g., ‚Äúdense forest,‚Äù ‚Äúnew construction,‚Äù ‚Äúflooded fields‚Äù).
3. Embed both images and queries ‚Üí compute cosine similarities.
4. Retrieve and visualize top-K matches with geolocation context (in QGIS or Plotly map).
5. Interpret attention maps to explain what visual features the model linked to each phrase.

**Outputs:**

- vit_finetune.ipynb
- clip_geosearch.ipynb
- Attention visualizations and retrieval maps
- Short summary (1 page): ‚ÄúUsing Language to See Landscapes‚Äù

---

### **6. Discussion / Reflection Prompt**

- **Prompt 1:** ‚ÄúHow does language shape what AI ‚Äòsees‚Äô in the Earth?‚Äù
- **Prompt 2:** ‚ÄúWhat biases can enter through text prompts and labels?‚Äù
- **Prompt 3:** ‚ÄúHow might vision‚Äìlanguage models help bridge scientific data and community narratives about land change?‚Äù
    
    *(Add #EthicsThread tag to responses in Notion and comment on two peers.)*
    

---

### **7. Weekly Deliverables**

1. vit_finetune.ipynb + classification metrics.
2. clip_geosearch.ipynb + retrieval maps.
3. Short summary document (‚âà1 page Markdown or PDF).
4. Ethics Thread reflection post.

---

### **8. Evaluation / Feedback**

- **Rubric Weights:**
    - Technical implementation (ViT + CLIP) ‚Äì 35 %
    - Interpretability & visualization ‚Äì 25 %
    - Semantic retrieval accuracy & clarity ‚Äì 20 %
    - Reflection & ethical analysis ‚Äì 20 %
- **Feedback Mode:** Peer code review swap + instructor rubric notes on GitHub PR.

---

### **9. Capstone Linkage**

| **Capstone Component** | **How Week 7 Advances It** |
| --- | --- |
| **1. Executive Narrative** | Introduces semantic bridge between quantitative outputs and qualitative language descriptions. |
| **2. Technical Pipeline** | Adds multimodal reasoning capability to existing workflow. |
| **3. Spatiotemporal Extension** | Lays groundwork for linking text-labeled features to time-series analysis (Week 11). |
| **4. Visualization & Communication** | Produces semantic maps and retrieval figures for final presentation. |
| **5. Open-Science Reflection** | Begins discussion on shared prompt libraries and transparent model cards. |

---

## **üß≠¬†Week 7 Rhythm**

| **Day** | **Focus** | **Output** |
| --- | --- | --- |
| Mon | Mini-lecture + readings | Annotated notes |
| Tue‚ÄìWed | ViT fine-tuning lab | vit_finetune.ipynb |
| Thu | CLIP retrieval lab | clip_geosearch.ipynb + maps |
| Fri | Reflection discussion + summary draft | 1-page summary + Ethics post |
| Weekend | Model card prep for foundation models | Pre-Week 8 setup complete |

---

### **‚úÖ¬†Outcome Alignment**

- **Primary Outcomes Mastered:** 2, 3, 6, 9, 10, 12
- **Secondary Outcomes Advanced:** 4, 13, 14, 15
- **Capstone Readiness Level:** ‚âà 50 % ‚Äî multimodal foundation established; students ready to work with planetary foundation models in Week 8.

---

# **Week 8 ‚Äî Geospatial Foundation Models I: Prithvi & Granite Geo**

---

### **1. Theme / Title**

**‚ÄúScaling Up ‚Äî From Regional Models to Planetary Foundations‚Äù**

---

### **2. Guiding Questions**

- What distinguishes a *foundation model* from traditional or pretrained vision networks?
- How do models like **Prithvi** and **Granite Geo** encode spectral, spatial, and temporal information across the globe?
- What does it mean to fine-tune or adapt a massive pretrained model responsibly?
- How can foundation models accelerate insight into land-use, hydrological, and ecological change in Chile?

---

### **3. Learning Objectives**

By the end of Week 8, you will be able to:

1. Access and use pretrained **Prithvi** and **Granite Geo** models from Hugging Face Hub.
2. Fine-tune a foundation model on a regional dataset (Los Lagos parcelization / flood mapping).
3. Evaluate model performance and visualize attention / embedding outputs.
4. Document model provenance and training metadata using open-science standards.
5. Reflect on scalability trade-offs (data coverage vs. local fidelity vs. computational cost).

---

### **4. Lecture / Reading Topics**

- **Mini-Lecture:** *‚ÄúWhat Makes a Foundation Model Foundational?‚Äù*
- **Core Concepts:**
    - Self-supervised pretraining on multisensor, multimodal global imagery
    - Transfer vs. adaptation: domain alignment for local fine-tuning
    - Foundation-model inference workflows in Hugging Face
    - Provenance, metadata, and reproducibility in large-scale AI
- **Readings & Resources:**
    - *Prithvi 2.0 Technical Report* (NASA FDL 2024)
    - *IBM Granite Geo Overview* (IBM Research 2024)
    - TorchGeo Docs: *Foundation Model Interfaces*
    - Hugging Face Blog: *Scaling Earth Observation Models Responsibly*
    - Ethics Thread Reading: *‚ÄúCompute Inequality and the Geographies of AI.‚Äù*

---

### **5. Hands-On Lab / Technical Exercise**

**Lab: Fine-Tuning Prithvi and Granite Geo for Regional Applications**

**Objective:**

Apply two foundation models to Chilean case studies and analyze their adaptability to local conditions.

**Tasks**

1. **Setup:** Install Hugging Face Transformers + TorchGeo; authenticate to model hub.
2. **Prithvi Experiment:** Fine-tune Prithvi 2.0 for parcelization detection (Los Lagos Region).
3. **Granite Experiment:** Run few-shot fine-tuning on hydrological change (megadrought area).
4. **Visualization:** Generate attention maps, segmentation outputs, and accuracy plots.
5. **Documentation:** Record hyperparameters, compute time, dataset links in a structured YAML log.

**Outputs**

- prithvi_finetune.ipynb
- granite_finetune.ipynb
- metrics_foundation.csv
- Attention / segmentation maps (.png / .gif)
- foundation_experiment.yml (metadata record)

---

### **6. Discussion / Reflection Prompt**

**Prompt 1:** ‚ÄúHow does global pretraining change what we can see ‚Äî and what we might miss ‚Äî in local data?‚Äù

**Prompt 2:** ‚ÄúWhat responsibilities arise when we reuse massive pretrained models built by major institutions?‚Äù

**Prompt 3:** ‚ÄúIn what ways could open-foundation models democratize climate and ecosystem research?‚Äù

(*Post in Notion Ethics Thread + peer comment on one response.*)

---

### **7. Weekly Deliverables**

1. Fine-tuned Prithvi and Granite notebooks with outputs.
2. metrics_foundation.csv + visualization figures.
3. Foundation Model Experiment Log (YAML + README summary).
4. Ethics Thread Reflection post (~200 words).

---

### **8. Evaluation / Feedback**

- **Rubric Weights:**
    - Technical implementation (fine-tuning, evaluation) ‚Äì 35 %
    - Analytical comparison (Prithvi vs Granite) ‚Äì 25 %
    - Documentation & metadata quality ‚Äì 20 %
    - Reflection & ethical considerations ‚Äì 20 %
- **Feedback Mode:** Instructor comment on GitHub PR + peer code review within team.
- **Outcome:** Working foundation-model outputs and documentation ready for Week 9 Alpha Earth embedding comparison.

---

### **9. Capstone Linkage**

| **Capstone Component** | **How Week 8 Advances It** |
| --- | --- |
| **1. Executive Narrative** | Provides technical evidence section for capstone report (model comparisons + rationale). |
| **2. Technical Pipeline** | Expands pipeline to include foundation models and performance metrics. |
| **3. Spatiotemporal Extension** | Prepares feature embeddings for temporal modeling (Week 11). |
| **4. Visualization & Communication** | Generates attention maps for final figures and dashboard. |
| **5. Open-Science Reflection** | Initiates model card documentation and metadata logging. |

---

## **üß≠¬†Week 8 Rhythm**

| **Day** | **Focus** | **Output** |
| --- | --- | --- |
| Mon | Mini-lecture + Readings | Annotated notes + Ethics Thread draft |
| Tue‚ÄìWed | Prithvi fine-tuning lab | prithvi_finetune.ipynb + metrics |
| Thu | Granite fine-tuning lab | granite_finetune.ipynb + attention maps |
| Fri | Model comparison + documentation | metrics_foundation.csv + experiment log |
| Weekend | Reflection + Alpha Earth prep | Finalize Ethics post + set up embedding workspace |

---

### **‚úÖ¬†Outcome Alignment**

- **Primary Outcomes Mastered:** 2, 5, 6, 9, 11, 16
- **Secondary Outcomes Advanced:** 4, 10, 13, 14
- **Capstone Readiness Level:** ‚âà 60 % ‚Äî first global-scale models implemented and documented; data pipeline ready for Alpha Earth embedding and ethics analysis in Week 9.

---

Would you like me to continue with **Week 7 ‚Äî Vision‚ÄìLanguage Fusion (ViT + CLIP)** next, which introduces the multimodal reasoning skills students need before they scale up to these foundation models?

# **Week 9 ‚Äî Geospatial Foundation Models II: Alpha Earth & Ethics Deep Dive**

---

### **1. Theme / Title**

**‚ÄúSeeing the Whole Planet: Embedding Global Insight Responsibly‚Äù**

---

### **2. Guiding Questions**

- How do global foundation models like **Alpha Earth** differ from earlier, domain-specific models (Prithvi, Granite)?
- What can Earth-scale embeddings tell us about processes such as water quality, vegetation stress, or urban sprawl?
- How can we evaluate environmental and social costs of training and deploying these models?
- What does it mean to make **foundation models for Earth** equitable and regenerative rather than extractive?

---

### **3. Learning Objectives**

By the end of Week 9, you will be able to:

1. Retrieve and apply **Alpha Earth embeddings** for geospatial analysis using Python and QGIS.
2. Compare outputs from multiple foundation models (Prithvi, Granite, Alpha Earth).
3. Interpret embedding spaces as representations of biophysical processes.
4. Critically evaluate environmental costs and bias in large AI model pipelines.
5. Document ethical considerations and open-science principles in model use and distribution.

---

### **4. Lecture / Reading Topics**

- **Mini-Lecture:** *‚ÄúFrom Pixels to Planets ‚Äî Understanding Alpha Earth and Global Embeddings.‚Äù*
- **Key Concepts:**
    - Alpha Earth architecture and training corpora
    - Embedding space interpretation and dimensionality reduction
    - Energy costs and carbon footprint of foundation models
    - Bias and representation of Global South landscapes
    - Principles of Responsible AI for Earth
- **Readings & Resources:**
    - Google Research (2024): *Alpha Earth Technical Overview & Embeddings Guide*
    - NASA Frontier Development Lab (2023): *Comparing Prithvi and Granite Geo Models*
    - Luccioni et al. (2023): *Estimating the Carbon Footprint of AI Models*
    - UNESCO (2023): *Ethical Guidelines for AI in Sustainability*
    - Case Article: *Equity in Earth Observation Data Access ‚Äî Perspectives from the Global South*

---

### **5. Hands-On Lab / Technical Exercise**

**Lab: Alpha Earth Embeddings & Responsible Model Evaluation**

**Objective:** Explore Alpha Earth‚Äôs global embedding representations and evaluate their value and limitations for regional applications.

**Tasks:**

1. Access Alpha Earth embeddings (via Hugging Face or Google Research API).
2. Extract embedding vectors for your region of interest (e.g., Los Lagos, Chile 2017‚Äì2024).
3. Perform dimensionality reduction (PCA / UMAP) and visualize clustering of land cover types or water quality patterns.
4. Compare embedding-based classification results to those from Prithvi or Granite.
5. Estimate relative compute resources and carbon cost (approximate calculation based on published energy data).
6. Create an ‚ÄúEthical Model Card‚Äù summarizing model scope, limitations, bias risks, and environmental footprint.

**Outputs:**

- alphaearth_embeddings.ipynb (notebook with visualizations)
- Comparison plots (PCA/UMAP, accuracy metrics)
- ModelCard_AlphaEarth.md with ethical and sustainability notes

---

### **6. Discussion / Reflection Prompt**

**Prompt 1:** ‚ÄúWhat is the responsibility of Earth scientists when using models trained on planet-scale data?‚Äù

**Prompt 2:** ‚ÄúHow might the energy cost of AI be weighed against its potential for climate and ecosystem insight?‚Äù

**Prompt 3:** ‚ÄúWhat would ‚ÄòRegenerative AI for Earth‚Äô look like in practice?‚Äù

*(Encourage responses as short essays or voice notes; tag #EthicsThread.)*

---

### **7. Weekly Deliverables**

1. **Alpha Earth Embedding Notebook** with visualizations and analysis.
2. **Model Comparison Summary** (Prithvi vs Granite vs Alpha Earth) ‚Äî 2-page memo or notebook markdown.
3. **Ethical Model Card PDF/Markdown** including carbon impact estimate and bias discussion.
4. **Ethics Thread Reflection** on global equity and responsible scaling of AI.

---

### **8. Evaluation / Feedback**

- **Feedback Mode:** Instructor + peer comment via GitHub pull request.
- **Rubric Weights:**
    - Technical accuracy & visualization ‚Äî 30 %
    - Comparative analysis depth ‚Äî 25 %
    - Ethical Model Card quality ‚Äî 25 %
    - Reflection depth ‚Äî 20 %
- **Outcome:** Students demonstrate ability to balance technical mastery with ethical awareness before integration sprint.

---

### **9. Capstone Linkage**

| **Capstone Component** | **How Week 9 Advances It** |
| --- | --- |
| **1. Executive Narrative** | Adds critical context on AI scale, equity, and impact. |
| **2. Technical Pipeline** | Incorporates Alpha Earth outputs as third foundation-model layer. |
| **3. Spatiotemporal Extension** | Prepares data and embeddings for temporal modeling next week. |
| **4. Visualization & Communication** | Creates comparative plots and model cards for presentation. |
| **5. Open-Science Reflection** | Documents environmental and ethical footprint in repository. |

---

## **üß≠¬†Week 9 Rhythm**

| **Day** | **Focus** | **Output** |
| --- | --- | --- |
| Mon | Lecture + Readings on Foundation Model Ethics | Reading notes + Ethics Thread reflection draft |
| Tue‚ÄìWed | Alpha Earth Lab | Embeddings notebook, visualizations |
| Thu | Model Comparison Memo | Performance and embedding interpretation |
| Fri | Ethical Model Card Workshop | Completed Model Card + reflection |
| Weekend | Preparation for Week 10 Integration | Consolidate all model outputs into one directory |

---

### **‚úÖ¬†Outcome Alignment**

- **Primary Outcomes Mastered:** 4, 6, 9, 14, 16
- **Secondary Outcomes Advanced:** 1, 2, 10, 13
- **Capstone Readiness Level:** ‚âà 70 % ‚Äî All foundation-model components developed and ethical framework established for integration sprint.

---

# **Week 10 ‚Äî Integration Sprint**

---

### **1. Theme / Title**

**‚ÄúBringing It All Together: Model Integration and Cross-Model Insight‚Äù**

---

### **2. Guiding Questions**

- How do the different GeoAI architectures (CNN, ViT, Prithvi, Granite, Alpha Earth) complement or contradict one another?
- What steps are required to make a multi-model pipeline fully reproducible and interpretable?
- How can peer feedback strengthen both the technical accuracy and narrative clarity of our evolving capstone projects?
- How do we evaluate success beyond metrics ‚Äî in terms of meaning, equity, and transparency?

---

### **3. Learning Objectives**

By the end of Week 10, you will be able to:

1. Integrate outputs from multiple models into a unified GeoAI workflow.
2. Conduct cross-model evaluation and visualize comparative performance.
3. Debug and document reproducibility issues in data, code, and environment.
4. Communicate intermediate results clearly to peers and solicit constructive feedback.
5. Reflect on how model comparisons illuminate strengths, limitations, and ethical trade-offs.

---

### **4. Lecture / Reading Topics**

- **Mini-Lecture:** *‚ÄúIntegration as Interpretation: Comparing Models for Earth Insight.‚Äù*
- **Concepts:**
    - Cross-model evaluation (CNN ‚Üî ViT ‚Üî Foundation Model).
    - Ensemble and consensus mapping.
    - Error propagation and uncertainty visualization.
    - Practical reproducibility (conda envs, Docker, Hugging Face Spaces).
- **Readings & Resources:**
    - Rahman & Li (2023) *Comparative Evaluation of Deep Learning Architectures for Land-Cover Classification.*
    - *Hugging Face Guide to Model Evaluation and Sharing.*
    - *The Turing Way ‚Äî Reproducible Research Chapter 2.*
    - Blog: *‚ÄúWhat Counts as Good Enough in AI for Earth?‚Äù*
    - Ethics Thread Reading: *‚ÄúBenchmark Fatigue and the Need for Context-Specific Metrics.‚Äù*

---

### **5. Hands-On Lab / Technical Exercise**

**Lab: Cross-Model Integration and Reproducibility Audit**

- **Objective:** Unify all previously trained models and conduct side-by-side comparison.
- **Tasks:**
    1. Assemble outputs from CNN, ViT, and at least one Foundation Model (Prithvi, Granite, or Alpha Earth).
    2. Compute comparative metrics (e.g., IoU, F1, Kappa, structural similarity).
    3. Generate ensemble or consensus map combining best model predictions.
    4. Visualize differences with attention maps and error heatmaps.
    5. Package the integrated workflow into a reproducible Jupyter notebook.
    6. Conduct a **Reproducibility Audit**: clean environment, document dependencies, validate notebook execution on a teammate‚Äôs machine.
- **Outputs:**
    - *integration_pipeline.ipynb* (complete multi-model workflow)
    - *metrics_summary.csv* + visual plots
    - *error_analysis_report.md*

---

### **6. Discussion / Reflection Prompt**

- **Prompt 1:** ‚ÄúWhat insights emerge only when models disagree?‚Äù
- **Prompt 2:** ‚ÄúHow does reproducibility relate to trust in GeoAI?‚Äù
- **Prompt 3:** ‚ÄúWhat responsibilities do we hold when sharing models that influence real-world resource decisions?‚Äù

Post 200‚Äì300 word reflections or short video notes; tag with #EthicsThread in Notion.

---

### **7. Weekly Deliverables**

1. **Integrated Model Pipeline Notebook** + metrics visualizations.
2. **Reproducibility Checklist & Environment File** (*environment.yml* or Dockerfile).
3. **Peer Review #2 Presentation** (10-minute live or recorded demo).
4. **Peer Feedback Form** for two colleagues (comment on clarity, interpretation, open-science quality).
5. **Ethics Thread Reflection** (submitted with notebook).

---

### **8. Evaluation / Feedback**

- **Peer Review:** structured rubric covering technical accuracy, interpretability, and communication.
- **Instructor Feedback:** qualitative memo on readiness for Capstone Design Review.
- **Rubric Weights:**
    - Integration pipeline completeness ‚Äì 30 %
    - Evaluation and visualization quality ‚Äì 25 %
    - Reproducibility audit ‚Äì 25 %
    - Peer presentation and feedback engagement ‚Äì 10 %
    - Reflection depth ‚Äì 10 %

---

### **9. Capstone Linkage**

| **Capstone Component** | **How Week 10 Advances It** |
| --- | --- |
| **1. Executive Narrative** | Generates quantitative comparison insights to inform final story. |
| **2. Technical Pipeline** | Produces integrated, reproducible workflow. |
| **3. Spatiotemporal Extension** | Establishes baseline ready for temporal/graph modeling in Week 11. |
| **4. Visualization & Communication** | Creates comparative visuals + peer-presentation practice. |
| **5. Open-Science Reflection** | Completes first full reproducibility audit; documents ethical lessons. |

---

## **üß≠¬†Week 10 Rhythm**

| **Day** | **Focus** | **Output** |
| --- | --- | --- |
| Mon | Mini-lecture + readings | Notes on integration & reproducibility |
| Tue‚ÄìWed | Integration Lab | Unified pipeline notebook, metrics summary |
| Thu | Peer Review #2 session | 10-min presentation + feedback forms |
| Fri | Reproducibility Audit + Ethics Thread | environment.yml + reflection |
| Weekend | Capstone planning | Adjust data + model plans for Week 11 temporal expansion |

---

### **‚úÖ¬†Outcome Alignment**

- **Primary Outcomes Mastered:** 2, 5, 6, 9, 12, 13
- **Secondary Outcomes Advanced:** 4, 10, 14, 16
- **Capstone Readiness Level:** ‚âà 80 % ‚Äî All model components integrated and validated; temporal/graph extensions to follow next week.

---

# **Week 11 ‚Äî Spatiotemporal & Graph-Based Modeling**

---

---

### **1. Theme / Title**

**‚ÄúLearning to See Change: Modeling Time and Connectivity in Earth Systems‚Äù**

---

### **2. Guiding Questions**

- How can GeoAI systems learn *processes* rather than static patterns?
- What can spatiotemporal and graph-based models tell us about evolving landscapes, water systems, and ecosystem health?
- How can we integrate multiple model outputs (CNNs, Transformers, Foundation Models) into a single coherent temporal story?
- What is our plan and structure for the final Capstone Synthesis Portfolio?

---

### **3. Learning Objectives**

By the end of Week 11, you will be able to:

1. Implement spatiotemporal models (ConvLSTM, Temporal Transformer) to analyze changes in vegetation, hydrology, or water quality.
2. Apply Graph Neural Networks (e.g., GAT) to represent spatial relationships such as watersheds, parcel networks, or riparian corridors.
3. Interpret temporal and graph-based results as evidence of real environmental processes (not just model outputs).
4. Design and present a coherent structure and timeline for the final Capstone Portfolio.
5. Reflect on how modeling temporal and spatial connectivity supports responsible, interpretable Earth Vision science.

---

### **4. Lecture / Reading Topics**

- **Mini-Lecture:** *‚ÄúFrom Snapshots to Streams: Temporal Reasoning in Earth Vision.‚Äù*
- **Key Concepts:**
    - Temporal deep learning (ConvLSTM, Temporal Transformer, TCN)
    - Graph-based reasoning (Graph Convolutional Networks, Graph Attention Networks)
    - Temporal explainability (attention over time, saliency sequences)
- **Readings & Resources:**
    - Shi et al. (2015). *Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting.*
    - Velickovic et al. (2018). *Graph Attention Networks.*
    - *Prithvi Temporal Modeling Notebook* (Hugging Face example).
    - Blog: *‚ÄúGraphs from Space: Understanding Connectivity in Remote Sensing.‚Äù*
    - Ethics Thread reading: *‚ÄúTemporal Bias and the Future of Prediction‚Äù (Nature AI Policy Forum).*

---

### **5. Hands-On Lab / Technical Exercise**

**Lab: Temporal and Graph Modeling for Change Detection**

- **Objective:** Add a dynamic dimension to your GeoAI pipeline by capturing how Earth systems evolve over time.
- **Tasks:**
    1. Prepare a **multi-date dataset** (2017‚Äì2024 Sentinel-2 NDVI or water indices).
    2. Train a **ConvLSTM** to forecast or classify vegetation/water quality change.
    3. Build a simple **Graph Attention Network (GAT)** using nodes = sample sites or parcels, edges = spatial proximity or shared watershed.
    4. Compare ConvLSTM vs. GAT results ‚Äî where do they agree or diverge?
    5. Visualize results in QGIS with time-enabled layers (TimeManager plugin).
- **Outputs:**
    - ConvLSTM model notebook
    - GAT notebook
    - Change visualization (animated GIF or QGIS temporal map)

---

### **6. Discussion / Reflection Prompt**

**Prompt 1:** ‚ÄúHow does seeing time through AI reshape our understanding of environmental change?‚Äù

**Prompt 2:** ‚ÄúWhere are the limits of prediction ‚Äî and how do we responsibly communicate uncertainty?‚Äù

**Prompt 3:** ‚ÄúHow can representing connectivity (via graphs) change how we design environmental policy or interventions?‚Äù

*(Post reflections on shared discussion board or Notion page.)*

---

### **7. Weekly Deliverables**

1. **Notebook #5: Spatiotemporal + Graph Modeling** (ConvLSTM and/or GAT).
2. **Visualization Artifact:** Temporal animation or QGIS time-series map.
3. **Capstone Design Review Presentation (5‚Äì7 slides):**
    - Case study focus
    - Models to be integrated (CNN ‚Üí Transformer ‚Üí Foundation ‚Üí Temporal/Graph)
    - Visualization plan
    - Repository structure
    - Anticipated challenges
4. **Ethics Thread Reflection:** One paragraph on ‚ÄúUncertainty, responsibility, and time.‚Äù

---

### **8. Evaluation / Feedback**

- **Feedback Method:**
    - Peer review of Capstone Design Slides (Week 11 mini-symposium).
    - Instructor summary feedback on pipeline feasibility and integration clarity.
- **Rubric Focus:**
    - Technical correctness (ConvLSTM/GAT implementation) ‚Äì 30%
    - Clarity of visualization and temporal reasoning ‚Äì 25%
    - Integration planning for Capstone ‚Äì 25%
    - Reflection depth (uncertainty and ethics) ‚Äì 20%
- **Integration Checkpoint:**
    
    Each team confirms that (a) data is cleaned, (b) baseline and advanced models are reproducible, (c) capstone story arc is defined.
    

---

### **9. Capstone Linkage**

| **Capstone Component** | **How Week 11 Advances It** |
| --- | --- |
| **1. Executive Narrative** | Builds storyline of temporal and connected processes (vegetation, water, parcelization). |
| **2. Technical Pipeline** | Adds temporal/graph modeling layer to existing architecture. |
| **3. Spatiotemporal Extension** | Fully implemented and evaluated; forms one of the capstone‚Äôs required components. |
| **4. Visualization & Communication** | Produces dynamic maps/animations for final presentation. |
| **5. Open-Science Reflection** | Encourages documentation of model assumptions and uncertainties in GitHub README. |

---

## **üß≠¬†Week 11 Rhythm**

| **Day** | **Focus** | **Output** |
| --- | --- | --- |
| Mon‚ÄìTue | Lecture & readings on temporal + graph models | Notes & ethics reflection |
| Wed‚ÄìThu | Hands-on Lab: ConvLSTM + GAT | Notebook #5, temporal map |
| Fri | Capstone Design Review Session (peer presentations) | 5‚Äì7 slide deck, feedback received |
| Weekend | Integration prep | GitHub structure updated, next week‚Äôs tasks planned |

---

### **‚úÖ¬†Outcome Alignment**

- **Primary Outcomes Mastered:** 3, 8, 9, 11, 12
- **Secondary Outcomes Advanced:** 10, 13, 14
- **Capstone Readiness Level:** 90% ‚Äî all components implemented, structure finalized for Week 12 integration.

---

# **Week 12 | Final Integration and Reflection**

---

### **1. Theme / Title**

**Week 12 ‚Äî ‚ÄúFinal Integration and Reflection Symposium: Responsible and Regenerative GeoAI‚Äù**

---

### **2. Guiding Questions**

- How can technical results from GeoAI be communicated as credible, ethical, and regenerative insights?
- What does it mean to conduct *responsible* AI for the Earth at global and community scales?
- How can our learning journey translate into a replicable graduate curriculum or institutional framework?

---

### **3. Learning Objectives**

By the end of Week 12, you will be able to:

1. Integrate CNN, Transformer, and Foundation Model outputs into a unified geospatial analysis pipeline.
2. Produce clear, interpretable visualizations linking patterns to ecological processes.
3. Document and release an open, reproducible repository compliant with FAIR standards.
4. Communicate results through a concise technical narrative and reflective essay.
5. Critically evaluate the ethical, societal, and pedagogical implications of your work.

---

### **4. Lecture / Reading Topics**

- **Mini-lecture:** ‚ÄúFrom Model Outputs to Earth Insights: Interpreting AI for Decision-Makers.‚Äù
- **Reading Set:**
    - *Prithvi 2.0 Model Card* (NASA Frontier Development Lab)
    - *IBM Granite Geo Technical Overview*
    - *Gebru et al. (2021). Datasheets for Datasets*
    - *UNESCO (2023). AI Ethics for Sustainable Development*
- **Optional Video:** ‚ÄúRegenerative AI for Earth Systems Science‚Äù (short keynote / YouTube link placeholder).

---

### **5. Hands-On Lab / Technical Exercise**

**Capstone Integration Lab:**

- Merge prior model outputs (e.g., CNN ‚Üí ViT ‚Üí Prithvi ‚Üí ConvLSTM) into one cohesive notebook.
- Generate *cross-model visual comparisons* (e.g., change-maps of Los Lagos 2017‚Äì2024).
- Finalize *interactive map dashboard* (QGIS + Plotly) illustrating key spatial-temporal trends.
- Run reproducibility test using a clean environment (confirm all notebooks execute end-to-end).

---

### **6. Discussion / Reflection Prompt**

- **Prompt 1:** ‚ÄúWhat responsibilities accompany the ability to model planetary change?‚Äù
- **Prompt 2:** ‚ÄúHow might regenerative principles reshape AI training and deployment practices?‚Äù
- **Prompt 3:** ‚ÄúIf you were to teach this course next year, what would you keep, adapt, or expand?‚Äù

---

### **7. Weekly Deliverables**

1. **Final Capstone Submission ‚Äî Earth Vision Synthesis Portfolio** (GitHub link + README).
2. **Responsible GeoAI Reflection Essay** (1‚Äì2 pages, PDF).
3. **Curriculum Translation Memo:** one-page summary of how this module sequence could become a graduate course.
4. **Peer Presentation:** 10-minute recorded or live demo during the *Reflection Symposium*.

---

### **8. Evaluation / Feedback**

- **Rubric Domains:** Conceptual framing (15 pts) ¬∑ Technical implementation (25) ¬∑ Scientific interpretation (20) ¬∑ Reproducibility (15) ¬∑ Reflection & ethics (15) ¬∑ Communication (10).
- **Feedback Method:**
    - Live peer + instructor commentary during symposium.
    - Written rubric + open-comment summary on GitHub pull request.
- **Revisions:** Optional one-week post-course revision window for publication-ready portfolios.

---

### **9. Capstone Linkage**

| **Capstone Component** | **How Week 12 Advances It** |
| --- | --- |
| **1. Executive Narrative** | Final synthesis of findings + 3‚Äì4 page report |
| **2. Technical Pipeline** | Integrated, reproducible notebook chain |
| **3. Spatiotemporal Extension** | Incorporation of temporal/graph modeling results |
| **4. Visualization & Communication** | Dashboard + final figure set + oral presentation |
| **5. Open-Science Reflection** | GitHub release + Responsible GeoAI essay + curriculum memo |

---

## **üß≠¬†Summary of Week 12 Rhythm**

| **Day** | **Activity Focus** | **Output** |
| --- | --- | --- |
| Mon‚ÄìTue | Integration Lab 1: Merge pipelines | Working composite notebook |
| Wed | Reflection Workshop 1 | Draft of Responsible GeoAI essay |
| Thu | Peer Symposium | 10-min live/recorded presentation |
| Fri | Integration Lab 2: Dashboard & Final Packaging | Final Capstone submission |
| Weekend | Reflection & Future Planning | Curriculum memo + course feedback survey |

---

### **‚úÖ¬†Outcome Alignment**

Week 12 achieves *mastery* of Learning Outcomes 10‚Äì17 and closes the loop on Outcomes 1‚Äì9 through applied synthesis and communication.

---

Would you like me to now **codify this weekly template** into a reusable Markdown or Notion format (a fill-in-the-blanks structure for Weeks 0‚Äì11) so that every weekly module follows the same architecture?

That will let us rapidly draft the remaining weeks with perfect consistency.

---

# **Capstone Assignment: ‚ÄúEarth Vision Synthesis Portfolio‚Äù**

### **Title:**

**GeoAI for Earth Systems: From Perception to Process Understanding**

### **Timing:**

Weeks **11‚Äì12**, building on artifacts and datasets accumulated throughout the course.

### **Duration:**

‚âà 16‚Äì20 hours over two weeks

---

## **üéØ¬†Purpose and Rationale**

The capstone represents the culmination of *GeoAI and Earth Vision: Foundations to Frontier Applications.*

Its goal is to synthesize theoretical insight, technical skill, ethical awareness, and design thinking into a coherent, reproducible, and visually compelling project that demonstrates how **AI-based Earth Vision systems can reveal and interpret real-world environmental processes**.

By completing this capstone, learners will demonstrate mastery of all **17 course learning outcomes**, including:

- Conceptual understanding of GeoAI‚Äôs evolution and architectures (Outcomes 1‚Äì4)
- Technical competence with CNNs, transformers, foundation, and graph-based models (Outcomes 5‚Äì9)
- Integration of real geospatial data and open workflows for environmental analysis (Outcomes 10‚Äì14)
- Higher-order synthesis through research framing and curriculum translation (Outcomes 15‚Äì17)

---

## **üß≠¬†Capstone Challenge Overview**

You will design and deliver an **integrated, multi-model GeoAI pipeline** that analyzes one or more of the following Chilean case studies:

1. **Land-Use Dynamics in the Los Lagos Region**
    
    *Detect and characterize parcelization trends converting agricultural land into peri-urban plots.*
    
2. **Hydrological Change under the Chilean Megadrought**
    
    *Quantify temporal changes in surface water and vegetation productivity.*
    
3. **Ecosystem Health in Llanquihue Lake**
    
    *Map eutrophication and turbidity dynamics as indicators of water quality.*
    

You may choose one primary case study (recommended) and optionally integrate secondary examples for comparison.

---

## **üß©¬†Capstone Deliverables**

The final submission will be an **‚ÄúEarth Vision Synthesis Portfolio‚Äù** consisting of **five integrated components**, each addressing a cluster of learning outcomes.

| **Component** | **Description** | **Key Outcomes** |
| --- | --- | --- |
| **1. Executive Narrative (3‚Äì4 pages)** | A research-style narrative explaining the environmental problem, rationale for AI approach, data sources, and expected impact. Must articulate ethical and accessibility considerations. | 1, 2, 4, 10, 14, 15 |
| **2. Technical Pipeline Notebook(s)** | Reproducible Jupyter or Colab notebooks integrating: ‚Ä¢ Data ingestion (QGIS + GEE API)‚Ä¢ Model training & inference using at least **two paradigms** (e.g., CNN baseline + Prithvi or ViT + Granite Geo)‚Ä¢ Evaluation and explainability visualizations (Grad-CAM, attention maps, embeddings) | 5‚Äì9, 11 |
| **3. Spatiotemporal Extension** | Include one temporal or graph-based analysis (ConvLSTM, Temporal Transformer, or GAT) to infer or predict change processes (e.g., vegetation loss, turbidity increase). | 8, 9, 11 |
| **4. Visualization & Communication Layer** | A polished visualization package (QGIS maps + interactive plots) narrating your results for scientific and public audiences. Use visual storytelling to show *patterns ‚Üí processes ‚Üí implications*. | 3, 9, 12 |
| **5. Open-Science Repository & Reflection** | ‚Ä¢ A public GitHub repository containing notebooks, data README, model cards, and documentation following FAIR principles.‚Ä¢ A 1‚Äì2 page reflective essay: ‚ÄúResponsible and Regenerative GeoAI in Practice.‚Äù | 13, 14, 15‚Äì17 |

---

## **üîç¬†Detailed Expectations**

### **A. Technical Criteria**

- Employ **at least two distinct model architectures** (e.g., CNN vs. Transformer, or Transformer vs. Foundation Model).
- Use **real remote sensing data** (Sentinel-2, MODIS, or HydroSHEDS).
- Include at least **one spatiotemporal dimension** (multi-year series or graph network).
- Demonstrate **explainability** through attention maps, embeddings, or interpretable indices.

### **B. Scientific Criteria**

- Provide clear evidence that model results correspond to known or plausible Earth-system processes.
- Integrate contextual data (land use, hydrology, vegetation indices) to support interpretation.
- Discuss **sources of uncertainty** and **limitations** transparently.

### **C. Ethical & Reflective Criteria**

- Address data bias, regional accessibility, and sustainability.
- Reflect on how open-source AI can empower communities and researchers in developing contexts.
- Propose guidelines or principles for ‚ÄúRegenerative GeoAI.‚Äù

### **D. Reproducibility Criteria**

- Code executes end-to-end with documented environment setup.
- Uses GitHub version control and environment.yml / requirements.txt files.
- Includes metadata and dataset links (no proprietary dependencies).

### **E. Communication Criteria**

- Maps and plots are annotated, color-accessible, and publication-quality.
- The narrative connects quantitative findings to societal and ecological meaning.
- The entire portfolio reads as a *scientific story* about the Earth seen through AI.

---

## **üßÆ¬†Evaluation Rubric (100 points total)**

| **Dimension** | **Learning Outcome Coverage** |
| --- | --- |
| Conceptual Framing & Ethics | 1‚Äì4, 14 |
| Technical Implementation & Accuracy | 5‚Äì9 |
| Spatiotemporal / Graph Extension | 8, 11 |
| Scientific Interpretation & Communication | 3, 9, 10, 12 |
| Reproducibility & Open-Science Quality | 13 |
| Reflection & Future Curriculum Integration | 15‚Äì17 |

---

## **üß≠¬†Suggested Submission Package**

```
/capstone/
 ‚îú‚îÄ‚îÄ notebooks/
 ‚îÇ     ‚îú‚îÄ‚îÄ 01_preprocessing.ipynb
 ‚îÇ     ‚îú‚îÄ‚îÄ 02_model_training_CNN.ipynb
 ‚îÇ     ‚îú‚îÄ‚îÄ 03_model_training_Prithvi.ipynb
 ‚îÇ     ‚îú‚îÄ‚îÄ 04_spatiotemporal_GAT.ipynb
 ‚îÇ     ‚îî‚îÄ‚îÄ 05_visualization.ipynb
 ‚îú‚îÄ‚îÄ data/ (linked or small samples)
 ‚îú‚îÄ‚îÄ results/ (maps, embeddings, plots)
 ‚îú‚îÄ‚îÄ README.md
 ‚îú‚îÄ‚îÄ environment.yml
 ‚îú‚îÄ‚îÄ Executive_Narrative.pdf
 ‚îî‚îÄ‚îÄ Reflection_Responsible_GeoAI.pdf
```

---

# **üåé¬†Backward-Mapping Table: From Capstone to Weekly Modules**

**Capstone Goal:**

Develop an open, reproducible **GeoAI pipeline and reflective portfolio** demonstrating mastery of computer vision, transformer, and foundation models for detecting and interpreting Earth system processes in Chilean contexts.

---

| **Capstone Component** | **Supporting Learning Outcomes** | **Scaffolded Skills / Knowledge** | **Weeks that Build It** | **Weekly Artifacts That Contribute to Final Capstone** |
| --- | --- | --- | --- | --- |
| **1. Executive Narrative***(Research framing, ethics, and interpretation)* | 1, 2, 4, 10, 14, 15 | - Understanding evolution of CV ‚Üí GeoAI- Articulating problem statements (parcelization, megadrought, eutrophication)- Connecting models to environmental meaning- Integrating ethical and responsible AI frameworks | Weeks 1‚Äì2, 10, 12 | ‚Ä¢ Week 2 reflection: *‚ÄúWhy GeoAI matters for equitable Earth observation‚Äù*‚Ä¢ Week 10 ethical reflection essay‚Ä¢ Week 12 synthesis narrative draft |
| **2. Technical Pipeline Notebook(s)***(CNN ‚Üí Transformer ‚Üí Foundation Model)* | 5, 6, 9, 11 | - Remote sensing data ingestion (QGIS, GEE)- Model setup, training, and fine-tuning- Evaluation metrics (precision, IoU, F1)- Comparative analysis across architectures | Weeks 3‚Äì4, 7‚Äì10 | ‚Ä¢ Week 3 ResNet baseline notebook‚Ä¢ Week 4 transfer-learning notebook‚Ä¢ Week 7 ViT experiment‚Ä¢ Week 9‚Äì10 Prithvi / Granite / Alpha Earth notebooks |
| **3. Spatiotemporal or Graph-Based Extension***(Dynamic or networked Earth process)* | 3, 8, 9, 11 | - Representing time and connectivity (ConvLSTM, GAT)- Encoding spatial-temporal relationships- Working with multi-date imagery (Sentinel-2 or MODIS) | Week 11 | ‚Ä¢ ConvLSTM NDVI trend analysis‚Ä¢ GAT network of water/turbidity nodes |
| **4. Visualization & Communication Layer***(Scientific storytelling through visual outputs)* | 3, 9, 12 | - QGIS map composition- Embedding & attention map visualization- Plotly / Holoviews dashboards- Narrative visualization for process understanding | Weeks 1, 6, 8, 10, 12 | ‚Ä¢ Week 1 spectral plots‚Ä¢ Week 6 embedding UMAP visualization‚Ä¢ Week 8 Geo-CLIP interactive demo‚Ä¢ Week 12 final figure set |
| **5. Open-Science Repository & Reflection***(FAIR-compliant, documented, and ethical project)* | 13, 14, 15‚Äì17 | - GitHub workflow, README, metadata, version control- Documentation and model cards- Reflective writing on responsible GeoAI and course translation | Weeks 0, 2, 9, 10, 12 | ‚Ä¢ Week 0 reproducibility setup repo‚Ä¢ Week 2 open-access reading summary‚Ä¢ Week 9 model card comparison (Prithvi/Granite)‚Ä¢ Week 12 reflection + syllabus design memo |

---

## **üß≠¬†Backward Design Flow (Conceptual Summary)**

1. **Capstone Need:** Interpret Earth processes through advanced GeoAI.
    
    ‚Üí **Weeks 1‚Äì2:** build conceptual understanding of how we ‚Äúsee‚Äù the Earth.
    
2. **Capstone Need:** Execute multi-model pipelines (CNN ‚Üí ViT ‚Üí GFM).
    
    ‚Üí **Weeks 3‚Äì10:** progressively train and compare architectures, learning explainability and evaluation.
    
3. **Capstone Need:** Incorporate time and spatial relationships.
    
    ‚Üí **Week 11:** apply ConvLSTM and GAT to add process-level inference.
    
4. **Capstone Need:** Communicate findings clearly and ethically.
    
    ‚Üí **Weeks 6‚Äì12:** develop interpretive visualizations, reflection, and communication practices.
    
5. **Capstone Need:** Deliver reproducible, open-science product and curricular synthesis.
    
    ‚Üí **Weeks 0, 2, 9, 12:** practice GitHub documentation, ethical reflection, and course translation.
    

---

## **üéì¬†Capstone Readiness Checklist (Skills Mastered Before Week 12)**

| **Skill Domain** | **Demonstrated By** | **Weeks Completed** |
| --- | --- | --- |
| Remote Sensing Data Acquisition & Preprocessing | Sentinel-2 visualization & NDVI analysis | 1‚Äì2 |
| CNN Modeling & Evaluation | Land-cover classification mini-project | 3‚Äì4 |
| Transfer Learning & Fusion | EfficientNet + MODIS/Sentinel experiment | 4 |
| Self-Supervised & Embedding Visualization | SimCLR embedding notebook | 5‚Äì6 |
| Vision Transformer & CLIP Querying | ViT and Geo-CLIP labs | 7‚Äì8 |
| Foundation Model Fine-Tuning | Prithvi, Granite, Alpha Earth notebooks | 9‚Äì10 |
| Spatiotemporal/Graph Modeling | ConvLSTM & GAT experiments | 11 |
| Communication & Visualization | QGIS dashboards & UMAP visualizations | 1, 6, 12 |
| Ethics, Accessibility, and Reflection | Week 2 & Week 10 reflections | 2, 10 |
| Open-Science Documentation | GitHub repo with README & model cards | 0, 9, 12 |
| Curriculum Translation & Synthesis | Graduate syllabus reflection | 12 |

## **üéØ¬†Learning Outcomes ‚Üî Assessment Map**

| **#** | **Learning Outcome** | **Primary Assessment(s)** | **Supporting Weeks / Modules** | **Evidence of Mastery** |
| --- | --- | --- | --- | --- |
| **1** | Explain how remote-sensing imagery encodes physical processes into digital bands. | **Lab 1 (Week 1)** ‚Äì *Earth as an Image* notebook and reflection. | Week 1 (Foundations of Seeing) | Clear interpretation of spectral composites and indices. |
| **2** | Differentiate architectures and learning paradigms in computer vision. | **Lecture Quiz + Discussion (Week 2)** ‚Äì ‚ÄúFrom Vision ‚Üí GeoVision.‚Äù | Weeks 2‚Äì3 | Accurate explanations contrasting CNNs, transformers, and SSL methods. |
| **3** | Describe the unique challenges of applying CV to geospatial and environmental contexts. | **Ethics Thread Launch (Week 2)** + reflections throughout course. | Weeks 2, 6, 10 (Ethics Thread) | Insightful posts connecting model design to environmental data realities. |
| **4** | Discuss the evolution from object recognition to Earth Vision. | **Week 2 Reading Response** + intro section of Capstone narrative. | Weeks 2‚Äì12 | Historical framing integrated into final project report. |
| **5** | Operate an integrated open-source GeoAI workflow (QGIS + EE API + PyTorch + HF). | **Lab 0 (Setup)** + repo audit during Week 3. | Weeks 0‚Äì3 | Verified, reproducible environment and functioning pipelines. |
| **6** | Implement and fine-tune CNNs / transfer-learning pipelines. | **Lab 3 (Week 3)** ‚Äì CNN Land-Cover Classification; **Lab 4 (Week 4)** ‚Äì Transfer + Fusion. | Weeks 3‚Äì4 | Working models with evaluation metrics and Grad-CAM visualizations. |
| **7** | Apply self-supervised / contrastive learning for embeddings. | **Lab 6 (Week 6)** ‚Äì Self-Supervised Representation Learning. | Weeks 5‚Äì6 | Successfully trained SSL model with embedding plots and interpretation. |
| **8** | Visualize and analyze latent feature spaces. | **Week 5 Representation Lab** + **Week 6 SSL Analysis**. | Weeks 5‚Äì6 | t-SNE/UMAP diagrams annotated with land-cover meaning. |
| **9** | Evaluate model performance and interpret attention maps. | **Evaluation Reports (Weeks 3‚Äì4)** + **Capstone Results Section (Week 12)**. | Weeks 3‚Äì12 | Correct use of IoU/F1 metrics + attention visualizations. |
| **10** | Deploy advanced architectures (Transformers, CLIP, Foundation Models). | **Labs 7‚Äì9 (Weeks 7‚Äì9)** ‚Äì Transformer and GeoFM implementation notebooks. | Weeks 7‚Äì9 | Fine-tuned ViT/CLIP/Prithvi models with comparative outputs. |
| **11** | Integrate multi-sensor / multi-modal data sources. | **Week 4 Fusion Lab** + **Week 8 Multimodal Pipeline.** | Weeks 4‚Äì8 | Combined Sentinel + MODIS inputs with performance gains. |
| **12** | Construct spatiotemporal / graph-based models for dynamic processes. | **Week 10 Temporal Model Lab** ‚Äì ConvLSTM or GNN. | Week 10 | Working sequence model predicting vegetation or water dynamics. |
| **13** | Interpret model outputs in relation to ecological and socio-environmental processes. | **Capstone Report Interpretation Section.** | Weeks 9‚Äì12 | Connection between model findings and real-world change clearly articulated. |
| **14** | Design and execute a reproducible end-to-end GeoAI project. | **Capstone Project (Weeks 11‚Äì12)** ‚Äì Earth Vision Synthesis Portfolio. | Weeks 0‚Äì12 | Complete, documented pipeline with results, figures, and metadata. |
| **15** | Communicate findings through annotated notebooks and visual dashboards. | **Capstone Presentation + Notebook Review.** | Week 12 | Well-structured visuals and explanatory text demonstrating interpretability. |
| **16** | Collaborate using open-science best practices (Git, FAIR, metadata). | **Repo Check-Ins (Weeks 0, 4, 8, 11)** + peer code reviews. | Continuous | Versioned repo, environment.yml, README, and licensing complete. |
| **17** | Critically reflect on ethical, environmental, and epistemic implications of GeoAI. | **Ethics Thread Series (Weeks 2‚Äì12)** + **Responsible AI Essay (Week 12).** | Weeks 2‚Äì12 | Thoughtful synthesis connecting technical work to Responsible AI principles. |

---

### **üìò Summary of Assessment Instruments**

| **Assessment Type** | **Description** | **Frequency / Weight** |
| --- | --- | --- |
| **Technical Labs (0‚Äì10)** | Structured, scaffolded notebooks building toward capstone. | 40 % |
| **Ethics & Reflection Threads** | Short written reflections and peer discussion posts. | 15 % |
| **Weekly Reports / Visual Summaries** | 1‚Äì2 page synthesis of results and interpretation. | 15 % |
| **Capstone: Earth Vision Synthesis Portfolio** | Final end-to-end project including code, results, Responsible AI essay, and curriculum reflection. | 30 % |

---

### **üìà Assessment Progression Across Learning Arcs**

| **Learning Arc** | **Weeks** | **Major Assessments** | **Outcome Clusters** |
| --- | --- | --- | --- |
| **Arc 1 ‚Äì Foundations of Seeing** | 0‚Äì3 | Labs 0‚Äì3, Reflection 1 | 1‚Äì4 |
| **Arc 2 ‚Äì Learning to Represent** | 4‚Äì6 | Labs 4‚Äì6, Representation Analysis | 5‚Äì9 |
| **Arc 3 ‚Äì Learning to Reason** | 7‚Äì10 | Labs 7‚Äì10, Ethics Thread III | 10‚Äì13 |
| **Arc 4 ‚Äì Integration & Reflection** | 11‚Äì12 | Capstone Portfolio, Responsible AI Essay | 14‚Äì17 |