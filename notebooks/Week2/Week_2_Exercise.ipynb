{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Week 2 Exercise — ImageNet ResNet‑50 on Sentinel‑2 (Guided Examples)\n",
        "\n",
        "- Kernel: \"Python (geoai)\"\n",
        "- AOI: `data/external/aoi.geojson`\n",
        "- Outputs: `figures/`, `reports/`\n",
        "\n",
        "Sections (run top-to-bottom):\n",
        "1) Imports, paths, reproducibility\n",
        "2) Earth Engine init and AOI load\n",
        "3) Sentinel‑2 median RGB composite\n",
        "4) Sample 5 land-cover points and fetch 224×224 patches\n",
        "5) Load ResNet‑50 (ImageNet) + labels\n",
        "6) Inference: per-sample top‑5 predictions\n",
        "7) Visualization: multi-panel patches + predictions (save)\n",
        "8) Grad‑CAM on one sample (save)\n",
        "9) Save prediction summary CSV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Imports, paths, reproducibility\n",
        "Objective:\n",
        "This code cell initializes the notebook environment — importing the necessary libraries, ensuring reproducibility, and establishing consistent file paths for data, figures, and reports.\n",
        "\n",
        "Key Components:\n",
        "\t•\tpathlib: Modern, object-oriented way to manage file paths (safer and cleaner than string paths).\n",
        "\t•\tgeopandas, earthengine-api (ee), geemap: Tools for spatial data handling and access to Google Earth Engine datasets.\n",
        "\t•\ttorch & torchvision: Deep learning framework and model zoo (ResNet will appear later).\n",
        "\t•\tReproducibility seeds: Fixing random states ensures results are repeatable across runs.\n",
        "\t•\tfind_repo_root(): A convenience function to automatically locate the repository root, supporting relative path resolution — a hallmark of reproducible research environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Imports, paths, reproducibility\n",
        "\n",
        "# Core utilities for file handling, randomness, and basic data types\n",
        "from pathlib import Path\n",
        "import os, io, json, random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "\n",
        "# Geospatial libraries\n",
        "import geopandas as gpd       # For handling vector spatial data (GeoJSON, shapefiles)\n",
        "import ee, geemap             # For accessing and visualizing Earth Engine datasets\n",
        "\n",
        "# Deep learning / model utilities\n",
        "import torch\n",
        "import torchvision as tv\n",
        "from torchvision.models import resnet50, ResNet50_Weights  # Pretrained CNN and associated weights\n",
        "\n",
        "# Reproducibility: set fixed seeds for Python, NumPy, and PyTorch\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# --- Repository Path Management ---\n",
        "\n",
        "# Function to detect repo root by looking upward for known folder names\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    for p in [start] + list(start.parents):\n",
        "        # We assume a repo structure containing 'data' and 'figures'\n",
        "        if (p / 'data').exists() and (p / 'figures').exists():\n",
        "            return p\n",
        "    # If nothing found, default to current working directory\n",
        "    return start\n",
        "\n",
        "# Resolve key paths\n",
        "CWD = Path.cwd()                # Current working directory (where notebook is running)\n",
        "REPO = find_repo_root(CWD)      # Root folder of the project/repo\n",
        "DATA = REPO / 'data'            # Data folder\n",
        "FIGS = REPO / 'figures'         # Folder for saving output images\n",
        "REPORTS = REPO / 'reports'      # Folder for CSVs and summaries\n",
        "\n",
        "# Define Area of Interest (AOI) path (used later for Sentinel-2 imagery)\n",
        "AOI_PATH = DATA / 'external' / 'aoi.geojson'\n",
        "\n",
        "# Ensure output directories exist (idempotent)\n",
        "FIGS.mkdir(exist_ok=True, parents=True)\n",
        "REPORTS.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Diagnostic prints for sanity check\n",
        "print('CWD:', CWD)\n",
        "print('Repo root:', REPO)\n",
        "print('Resolved AOI:', AOI_PATH)\n",
        "print('AOI exists:', AOI_PATH.exists())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outcome:**\n",
        "After running this cell, the environment is ready for geospatial and deep learning operations. The project structure (`data/`, `figures/`, `reports/`) exists and the AOI GeoJSON file is present.\n",
        "\n",
        "- If `AOI exists: True`, you can proceed to Earth Engine initialization.\n",
        "- If `False`, fix the AOI path before fetching Sentinel-2 data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🌟 The More You Know! — ResNet‑50 and Weights\n",
        "\n",
        "🧠 What is ResNet50?\n",
        "\n",
        "ResNet50 (short for Residual Network, 50 layers deep) is a convolutional neural network (CNN) architecture introduced by He et al., 2015 in the paper “Deep Residual Learning for Image Recognition.”\n",
        "\n",
        "It’s one of the first deep networks to successfully train very deep models (50+ layers) by using skip connections (or residuals) — shortcuts that allow the network to “skip” one or more layers.\n",
        "These connections solve the vanishing gradient problem that used to plague very deep CNNs, enabling better accuracy with efficient training.\n",
        "\n",
        "In short:\n",
        "\n",
        "ResNet50 is a 50-layer deep CNN that uses residual learning to improve performance and stability.\n",
        "\n",
        "🧩 How it’s used\n",
        "\n",
        "In your notebook:\n",
        "\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "You’ll later call it like this:\n",
        "\n",
        "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "\n",
        "That means:\n",
        "\t•\tYou’re instantiating the ResNet50 model architecture.\n",
        "\t•\tYou’re initializing it with pretrained weights (parameters learned from training on ImageNet — a dataset of 1.2 million images across 1,000 categories).\n",
        "\n",
        "The model then acts as a feature extractor or classifier that can recognize common objects (e.g., “dog,” “airplane,” “traffic light”) or be adapted to new domains (like satellite imagery).\n",
        "\n",
        "⸻\n",
        "\n",
        "🏋️ What is ResNet50_Weights?\n",
        "\n",
        "ResNet50_Weights is a convenient enumeration object in torchvision that keeps track of available pretrained weight configurations for ResNet50.\n",
        "\n",
        "It’s part of the new API in torchvision>=0.13, which replaced the old pretrained=True syntax with something clearer and more controlled.\n",
        "\n",
        "You can inspect it directly:\n",
        "\n",
        "from torchvision.models import ResNet50_Weights\n",
        "print(ResNet50_Weights.DEFAULT)\n",
        "\n",
        "Commonly used options include:\n",
        "\t•\tResNet50_Weights.IMAGENET1K_V1: Original weights trained on ImageNet-1k (2015 baseline).\n",
        "\t•\tResNet50_Weights.IMAGENET1K_V2: Updated weights with better performance (recommended).\n",
        "\t•\tResNet50_Weights.DEFAULT: Alias for the latest best-performing set.\n",
        "\n",
        "When you use one, you can also access its preprocessing transforms (normalization, resizing, etc.):\n",
        "\n",
        "weights = ResNet50_Weights.IMAGENET1K_V2\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "That gives you a pipeline for preparing your input images in exactly the way the model expects — ensuring consistent predictions.\n",
        "\n",
        "⸻\n",
        "\n",
        "🧩 TL;DR — Quick Reference\n",
        "\n",
        "Tool\tWhat it is\tHow you use it\tWhy it matters\n",
        "resnet50\tCNN architecture with 50 layers and residual (skip) connections\tmodel = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\tExtracts visual features or performs classification\n",
        "ResNet50_Weights\tPretrained weight enumerator + transform helper\tweights = ResNet50_Weights.DEFAULT\tProvides pretrained parameters and image preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Earth Engine Initialization\n",
        "\n",
        "Objective:\n",
        "This block connects your notebook to Google Earth Engine (EE) and loads the Area of Interest (AOI) GeoJSON file into both GeoPandas (for local Python-side manipulation) and Earth Engine (for server-side geospatial computation).\n",
        "\n",
        "Key Components:\n",
        "\t•\tee.Initialize() / ee.Authenticate() → Log in and establish a connection to Google Earth Engine.\n",
        "\t•\tgeopandas → Reads and checks your AOI GeoJSON locally; ensures it’s in WGS84 (EPSG:4326) coordinate reference system.\n",
        "\t•\tgeemap.geopandas_to_ee() → Converts the GeoDataFrame to an Earth Engine geometry object for use in cloud-based image queries.\n",
        "\t•\tgeemap.Map() → Creates an interactive map widget right inside your notebook, letting you visualize your AOI and, later, image layers.\n",
        "\n",
        "Why this matters:\n",
        "Think of this cell as connecting your lab notebook to the Earth Engine satellite archive. Once this works, you can query massive datasets (e.g., Sentinel-2) without downloading gigabytes of imagery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Earth Engine init and AOI load\n",
        "\n",
        "try:\n",
        "    # Attempt to initialize an existing Earth Engine session.\n",
        "    ee.Initialize()\n",
        "    print('EE initialized')\n",
        "except Exception:\n",
        "    # If initialization fails (e.g., first use or expired token), authenticate and re-initialize.\n",
        "    print('EE not initialized; authenticating...')\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize()\n",
        "\n",
        "# --- Load and validate Area of Interest (AOI) locally ---\n",
        "\n",
        "# Confirm that the AOI GeoJSON file exists before proceeding\n",
        "assert AOI_PATH.exists(), f\"Missing AOI at {AOI_PATH}\"\n",
        "\n",
        "# Read AOI into a GeoDataFrame (local vector representation)\n",
        "aoi_gdf = gpd.read_file(AOI_PATH)\n",
        "assert not aoi_gdf.empty, \"AOI GeoDataFrame is empty!\"\n",
        "\n",
        "# Ensure coordinate reference system (CRS) is defined; default to WGS84 if missing\n",
        "if aoi_gdf.crs is None:\n",
        "    aoi_gdf.set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "# Convert to standard geographic coordinates (latitude/longitude)\n",
        "aoi_gdf = aoi_gdf.to_crs('EPSG:4326')\n",
        "\n",
        "# Convert GeoDataFrame geometry to an Earth Engine feature collection\n",
        "AOI_EE = geemap.geopandas_to_ee(aoi_gdf[['geometry']])\n",
        "\n",
        "# --- Display and visualize AOI ---\n",
        "\n",
        "# Print bounding box for context (minx, miny, maxx, maxy)\n",
        "print('AOI bounds:', aoi_gdf.total_bounds)\n",
        "\n",
        "# Create an interactive map object using geemap\n",
        "Map = geemap.Map()\n",
        "\n",
        "# Add AOI layer to the map for visualization\n",
        "Map.add_gdf(aoi_gdf, layer_name='AOI')\n",
        "\n",
        "# Center the map on the AOI (zoom level 10 is a moderate regional scale)\n",
        "Map.centerObject(AOI_EE, 10)\n",
        "\n",
        "# Display the map widget\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outcome:**\n",
        "This cell verifies your Earth Engine connection and loads your AOI into both your local and cloud environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🌟 The More You Know! — Earth Engine & GeoPandas Integration\n",
        "\n",
        "🔭 Google Earth Engine (EE)\n",
        "A cloud-based planetary data platform that hosts petabytes of satellite imagery and geospatial data. Instead of downloading imagery, you send queries that Earth Engine executes server-side.\n",
        "You can authenticate once (ee.Authenticate()) and then access imagery by dataset name (e.g., \"COPERNICUS/S2_SR_HARMONIZED\" for Sentinel-2).\n",
        "\n",
        "🪶 GeoPandas ↔ Earth Engine Bridge (geemap)\n",
        "geemap is a Python package that acts as the bridge between GeoPandas (local vector data) and Earth Engine (cloud vector/raster data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Creating a composite RGB from Sentinel-2 data\n",
        "\n",
        "Objective:\n",
        "Retrieve and visualize a cloud-free median composite of Sentinel-2 surface reflectance imagery over your Area of Interest (AOI).\n",
        "\n",
        "Key Components:\n",
        "\t•\tDataset: COPERNICUS/S2_SR_HARMONIZED — harmonized Sentinel-2 Level-2A (surface reflectance) imagery from both Sentinel-2A and 2B.\n",
        "\t•\tFilters:\n",
        "\t•\tSpatial: restricted to your AOI (filterBounds).\n",
        "\t•\tTemporal: January 2020 – December 2023 (filterDate).\n",
        "\t•\tQuality: exclude images with > 20 % cloud cover (CLOUDY_PIXEL_PERCENTAGE).\n",
        "\t•\tCloud Mask: Uses the QA60 band, which encodes cloud and cirrus information as bit flags.\n",
        "\t•\tComposite: Takes the median value for each pixel across the filtered image collection, producing a representative, nearly cloud-free image.\n",
        "\t•\tVisualization: Generates a 512-pixel thumbnail URL for quick viewing and adds the RGB layer to your interactive geemap map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Sentinel-2 median RGB composite (harmonized)\n",
        "\n",
        "# Load Sentinel-2 Surface Reflectance (harmonized) collection from Earth Engine\n",
        "s2 = (\n",
        "    ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')  # Level-2A SR data from Sentinel-2A/B\n",
        "      .filterBounds(AOI_EE)                            # Spatial filter: only scenes intersecting AOI\n",
        "      .filterDate('2020-01-01', '2023-12-31')          # Temporal filter: Jan 2020 – Dec 2023\n",
        "      .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', 20))  # Metadata filter: ≤ 20 % cloud cover\n",
        ")\n",
        "\n",
        "# --- Define a simple cloud mask function using QA60 bit flags ---\n",
        "def mask_s2_sr(img):\n",
        "    qa60 = img.select('QA60')          # QA60 band contains cloud/cirrus bits\n",
        "    clouds = qa60.bitwiseAnd(1 << 10).eq(0)  # Bit 10 = clouds (1 = cloudy, 0 = clear)\n",
        "    cirrus = qa60.bitwiseAnd(1 << 11).eq(0)  # Bit 11 = cirrus (high thin clouds)\n",
        "    # Keep only pixels where both cloud and cirrus bits are clear\n",
        "    return img.updateMask(clouds.And(cirrus))\n",
        "\n",
        "# Apply the mask function to every image in the collection\n",
        "s2_clean = s2.map(mask_s2_sr)\n",
        "\n",
        "# Build a median composite of the cleaned images and select RGB bands\n",
        "rgb = (\n",
        "    s2_clean.median()                   # Pixel-wise median across time\n",
        "            .select(['B4','B3','B2'])   # True-color bands: Red, Green, Blue\n",
        "            .clip(AOI_EE)               # Clip result to AOI boundary\n",
        ")\n",
        "\n",
        "# --- Diagnostics and visualization ---\n",
        "\n",
        "# Print number of Sentinel-2 scenes included after filtering\n",
        "print('S2 count:', s2.size().getInfo())\n",
        "\n",
        "# Generate a thumbnail URL (512 px wide) for quick visual inspection\n",
        "thumb = rgb.getThumbURL({\n",
        "    'min': 0, 'max': 3000, 'dimensions': 512, 'region': AOI_EE.geometry()\n",
        "})\n",
        "print('Thumb URL:', thumb)\n",
        "\n",
        "# Add the RGB composite to the interactive map\n",
        "Map.addLayer(rgb, {'min':0,'max':3000}, 'S2 RGB (median)')\n",
        "\n",
        "# Optionally display the map widget in compatible environments\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outcome:**\n",
        "You’ve now created a median composite image representing typical surface conditions in your AOI from 2020–2023.\n",
        "\t•\tThe console output S2 count: tells you how many individual Sentinel-2 scenes contributed to this composite.\n",
        "\t•\tThe Thumb URL provides a quick-load image you can open in a browser to confirm the color balance and cloud removal.\n",
        "\t•\tThe interactive map now displays your AOI with a clean, cloud-free true-color background.\n",
        "\n",
        "Why median?\n",
        "Taking the median of each pixel stack removes outliers — mostly clouds and shadows — while preserving consistent surface features. It’s a statistical way to produce a “most typical” image over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🌟 The More You Know! — Sentinel‑2, QA60, and Harmonized Composites\n",
        "\n",
        "**🛰️ Sentinel-2 & the “Harmonized” Collection**\n",
        "\n",
        "- **Sentinel-2** is a twin-satellite system (S2A + S2B) providing 10 m–60 m resolution multispectral imagery.\n",
        "- **Harmonized SR (Surface Reflectance)**: ESA reprocessed both satellites’ data so that their spectral responses match — this is the COPERNICUS/S2_SR_HARMONIZED dataset.\n",
        "\n",
        "### ☁️ Cloud Masking with QA60\n",
        "\n",
        "The QA60 band encodes quality information using **bit flags**:\n",
        "\n",
        "| **Bit** | **Meaning** | **Mask logic** |\n",
        "| --- | --- | --- |\n",
        "| 10 | Cloud | keep if bit 10 = 0 |\n",
        "| 11 | Cirrus | keep if bit 11 = 0 |\n",
        "\n",
        "By combining both, img.updateMask(clouds.And(cirrus)) removes cloud-affected pixels from further analysis.\n",
        "\n",
        "### 🎨 Median Composites — “Typical Earth” View\n",
        "\n",
        "Each pixel in the composite is the **median reflectance** value across all cloud-free images.\n",
        "\n",
        "It’s not one real date — it’s a *synthetic, representative scene*.\n",
        "\n",
        "This technique is standard in remote sensing when producing annual or multi-year base layers.\n",
        "\n",
        "**Analogy:**\n",
        "\n",
        "Imagine stacking dozens of photos of the same landscape taken throughout the year.\n",
        "\n",
        "If you take the **median color** of each pixel stack, you get a single clean image with clouds statistically “erased.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Point Extraction\n",
        "\n",
        "Objective:\n",
        "Define and visualize a small set of representative land-cover points (Forest, Water, Urban, Agriculture) that you’ll use to extract Sentinel-2 image patches for classification and interpretability (via ResNet and Grad-CAM later on).\n",
        "\n",
        "Key Components:\n",
        "\t•\tPTS_COORDS: Dictionary of labeled longitude/latitude pairs representing known land-cover types.\n",
        "\t•\tpts: Reformatted dictionary for easier access in later loops.\n",
        "\t•\tVisualization: Each point is added to the interactive Earth Engine map for spatial verification — ensuring they fall within your AOI and represent diverse surface conditions.\n",
        "\n",
        "Why this matters:\n",
        "These labeled points are the anchors for your analysis — they link semantic meaning (“forest”, “water”) to actual imagery pixels. You’ll use them to probe how a pretrained image model (ResNet-50) interprets satellite textures across land-cover types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Use specified Week 1 points (Forest, Water, Urban, Agriculture)\n",
        "\n",
        "# Define dictionary of labeled coordinates (longitude, latitude)\n",
        "PTS_COORDS = {\n",
        "    'Forest': (-73.028383, -41.124933),\n",
        "    'Water': (-73.016458, -41.137219),\n",
        "    'Urban': (-73.054658, -41.124836),\n",
        "    'Agriculture': (-73.046583, -41.139317),\n",
        "}\n",
        "\n",
        "# Restructure dictionary to make later access easier (adds 'lon' and 'lat' keys explicitly)\n",
        "pts = {k: {'lon': lon, 'lat': lat} for k, (lon, lat) in PTS_COORDS.items()}\n",
        "\n",
        "# Display the points for verification\n",
        "print('Using fixed points (lon, lat):')\n",
        "for k, v in pts.items():\n",
        "    print(k, v['lon'], v['lat'])\n",
        "\n",
        "# --- Add points to interactive map for visual check ---\n",
        "\n",
        "for k, v in pts.items():\n",
        "    # Create a point geometry for each coordinate\n",
        "    point = ee.Geometry.Point([v['lon'], v['lat']])\n",
        "    # Add point as a red dot layer with the land-cover label\n",
        "    Map.addLayer(point, {'color':'red'}, f'{k} (fixed)')\n",
        "\n",
        "# Display the updated map\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outcome:**\n",
        "This cell defines four labeled geographic points representing distinct land-cover types.\n",
        "After running it:\n",
        "\t•\tYou’ll see a printed list of coordinates for each class.\n",
        "\t•\tThe interactive map will display red dots labeled Forest, Water, Urban, and Agriculture.\n",
        "\n",
        "These fixed reference points serve two purposes:\n",
        "\t1.\tThey ensure your extracted image chips are spatially and semantically distinct.\n",
        "\t2.\tThey form the foundation for comparing model predictions — observing how the pretrained ResNet interprets different landscapes.\n",
        "\n",
        "In essence: you’ve now created your “ground truth seeds” for visual AI interpretation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "🌟 The More You Know! — Ground Truth Points and Spatial Sampling\n",
        "\n",
        "🗺️ Why Fixed Points Matter\n",
        "\n",
        "When testing pretrained vision models on satellite imagery, it’s critical to use consistent, representative locations.\n",
        "By fixing coordinates:\n",
        "\t•\tYou remove randomness, ensuring reproducibility (each run uses the same locations).\n",
        "\t•\tYou can compare outputs across sessions, models, or preprocessing changes.\n",
        "\n",
        "🧭 How This Ties Into Machine Learning\n",
        "\n",
        "In your workflow, these points will act like mini labeled samples:\n",
        "\t•\tEach coordinate → one image patch (128×128 pixels).\n",
        "\t•\tEach patch → passed through ResNet-50 → predicted label(s).\n",
        "\n",
        "You’ll then compare what the model “thinks” (e.g., tennis court) to what you know (e.g., urban area).\n",
        "This contrast demonstrates domain shift — a central concept in Week 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Fetch Patches\n",
        "Objective:\n",
        "Extract small, square Sentinel-2 RGB image patches around your labeled points (Forest, Water, Urban, Agriculture) using Earth Engine’s thumbnail service and store them locally in memory as PIL images.\n",
        "\n",
        "Key Components:\n",
        "\t•\tPatch parameters:\n",
        "\t•\tPATCH_SIZE = 128 → pixel width and height of each extracted chip.\n",
        "\t•\tSCALE = 20 → spatial resolution in meters per pixel (Sentinel-2 RGB bands are at 10 m; 20 m gives manageable file sizes).\n",
        "\t•\tMAX_SAMPLES = 3 → limits how many labeled sites to fetch (useful for debugging).\n",
        "\t•\tget_patch() function:\n",
        "\t•\tBuilds a buffered bounding box around each point.\n",
        "\t•\tUses getThumbURL() to request a JPEG thumbnail (not full raster) from Earth Engine.\n",
        "\t•\tRetrieves the image using Python’s requests library and loads it into a PIL.Image object for later processing.\n",
        "\t•\tLoop over sample points: Fetches patches for up to three labeled locations (e.g., Forest, Water, Urban).\n",
        "\n",
        "Why this matters:\n",
        "This cell forms the bridge between remote sensing and machine learning — taking geospatial imagery (Earth Engine) and converting it into local tensors or arrays suitable for PyTorch inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Fetch patches \n",
        "\n",
        "# --- Patch extraction parameters ---\n",
        "PATCH_SIZE = 128        # Desired patch size (pixels per side)\n",
        "SCALE = 20              # Spatial scale (meters per pixel)\n",
        "MAX_SAMPLES = 3         # Max number of labeled samples to retrieve\n",
        "REQUEST_TIMEOUT = 20    # Timeout for HTTP requests (seconds)\n",
        "\n",
        "# --- Helper function: fetch one patch centered on (lon, lat) ---\n",
        "\n",
        "def get_patch(img, lon, lat, size_m=PATCH_SIZE*SCALE, scale=SCALE):\n",
        "    # Compute half-width of patch in meters (for buffer radius)\n",
        "    half = size_m / 2\n",
        "    \n",
        "    # Reproject image to a consistent coordinate system and resolution\n",
        "    proj = img.reproject(crs='EPSG:4326', scale=scale)\n",
        "    \n",
        "    # Define export/thumbnail parameters\n",
        "    params = {\n",
        "        'region': ee.Geometry.Point([lon, lat]).buffer(half).bounds(),  # square region\n",
        "        'dimensions': f'{PATCH_SIZE}x{PATCH_SIZE}',                     # output size in pixels\n",
        "        'format': 'jpg', 'min': 0, 'max': 3000                          # stretch values for RGB\n",
        "    }\n",
        "    \n",
        "    # Generate a temporary download URL from Earth Engine\n",
        "    url = proj.getThumbURL(params)\n",
        "    \n",
        "    # Retrieve the image bytes via HTTP request\n",
        "    r = requests.get(url, timeout=REQUEST_TIMEOUT)\n",
        "    r.raise_for_status()   # Raise error if download failed\n",
        "    \n",
        "    # Load bytes into a PIL Image and convert to RGB\n",
        "    return Image.open(io.BytesIO(r.content)).convert('RGB')\n",
        "\n",
        "# --- Loop through labeled points and fetch patches ---\n",
        "\n",
        "patches = {}\n",
        "for idx, (name, info) in enumerate(pts.items()):\n",
        "    if idx >= MAX_SAMPLES:  # Optional limit for testing\n",
        "        break\n",
        "    try:\n",
        "        patches[name] = get_patch(rgb, info['lon'], info['lat'])\n",
        "    except Exception as e:\n",
        "        print('Patch fetch failed for', name, e)\n",
        "\n",
        "# Confirm which patches were successfully retrieved\n",
        "print('Patch keys:', list(patches.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outcome:**\n",
        "\n",
        "This cell creates a dictionary `patches` containing small RGB image crops centered on up to three labeled points (e.g., `{'Forest': <PIL.Image>, 'Water': <PIL.Image>, 'Urban': <PIL.Image>}`).\n",
        "\n",
        "After running it, you should see a printout such as:\n",
        "\n",
        "```\n",
        "Patch keys: ['Forest', 'Water', 'Urban']\n",
        "```\n",
        "\n",
        "Each entry represents a 128×128 pixel Sentinel-2 thumbnail, roughly covering a 2.56 km × 2.56 km area (128 × 20 m per pixel).\n",
        "\n",
        "These image patches are now ready for:\n",
        "\n",
        "- Visualization (sanity checks)\n",
        "- Input to your ResNet-50 model to generate top-5 ImageNet predictions and Grad-CAM explanations\n",
        "\n",
        "If a request fails (e.g., network hiccup or invalid coordinates), the try/except block reports it without stopping execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **🌟 The More You Know! — Thumbnails, Scale, and Sampling**\n",
        "\n",
        "### **🛰️ Earth Engine Thumbnails**\n",
        "\n",
        "getThumbURL() is Earth Engine’s efficient way to export small visualization-ready images.\n",
        "\n",
        "Rather than downloading raw geotiffs, it creates lightweight RGB JPEGs using your specified visualization parameters (min, max, dimensions, region).\n",
        "\n",
        "Perfect for quick model prototyping or visualization tasks.\n",
        "\n",
        "### **⚖️ Scale and Patch Size**\n",
        "\n",
        "- **Scale (m/pixel)** controls the ground resolution.\n",
        "    - At 20 m/pixel, a 128×128 patch covers about **2.56 km × 2.56 km**.\n",
        "    - Smaller scale (e.g., 10 m) increases resolution but also doubles file size and load time.\n",
        "- **Patch size (pixels)** controls how much context the model sees — a balance between local detail and landscape context.\n",
        "\n",
        "### **💡 Why Use JPEG Thumbnails?**\n",
        "\n",
        "For exploratory ML and explainability work, speed matters more than spectral precision.\n",
        "\n",
        "JPEGs load fast, use less memory, and are fully compatible with vision models like ResNet-50, which expect 3-band RGB input tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Load ResNet-50\n",
        "**Objective:**\n",
        "Load a pretrained ResNet-50 model from torchvision — one of the most widely used architectures for image classification — along with its associated preprocessing pipeline and label set from ImageNet-1K (1,000 everyday object categories).\n",
        "\n",
        "**Key Components:**\n",
        "\t•\tResNet50_Weights.IMAGENET1K_V2 — provides pretrained model weights and the correct transforms for image normalization.\n",
        "\t•\tDevice selection — automatically uses a GPU (cuda) if available, else defaults to CPU.\n",
        "\t•\tmodel.eval() — switches the model to inference (evaluation) mode, disabling dropout and gradient updates.\n",
        "\t•\tweights.transforms() — returns the preprocessing pipeline that matches the ImageNet training setup (resize, crop, normalize).\n",
        "\t•\tweights.meta['categories'] — contains the 1,000 human-readable ImageNet class names used for model predictions.\n",
        "\n",
        "**Why this matters:**\n",
        "This step equips your notebook with a general-purpose visual recognition model trained on everyday imagery — perfect for exploring domain shift between photographic and satellite data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Load ResNet-50 (ImageNet) + labels\n",
        "\n",
        "# Import pretrained weight enumerator (provides weights + metadata)\n",
        "from torchvision.models import ResNet50_Weights\n",
        "\n",
        "# --- Hardware configuration ---\n",
        "# Use GPU (cuda) if available, else fallback to CPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Torch device:', device)\n",
        "\n",
        "# --- Model weights and initialization ---\n",
        "# Select the improved ImageNet pretrained weights (Version 2)\n",
        "weights = ResNet50_Weights.IMAGENET1K_V2\n",
        "\n",
        "# Instantiate the ResNet-50 architecture with pretrained weights\n",
        "# Move model to the selected device and set to evaluation mode\n",
        "model = resnet50(weights=weights).to(device).eval()\n",
        "\n",
        "# --- Preprocessing pipeline ---\n",
        "# The weight object includes the exact preprocessing transforms used during training\n",
        "# (resize → center-crop → normalize to ImageNet mean/std)\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "# --- Class labels ---\n",
        "# Retrieve human-readable class names from ImageNet metadata (1,000 categories)\n",
        "labels = weights.meta.get('categories', None)\n",
        "\n",
        "# Diagnostic printout\n",
        "print('Model loaded with', len(labels) if labels else 'unknown', 'classes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outcome:**\n",
        "\n",
        "This cell successfully loads a pretrained **ResNet-50 model** with **1,000 ImageNet categories**, ready to classify your Sentinel-2 patches.\n",
        "\n",
        "You’ll see output similar to:\n",
        "\n",
        "```\n",
        "Torch device: cpu (or cuda)\n",
        "Model loaded with 1000 classes\n",
        "```\n",
        "\n",
        "At this point:\n",
        "\n",
        "- The variable **model** holds a fully initialized neural network on your chosen device.\n",
        "- **preprocess** is a transformation function that standardizes your patches for inference.\n",
        "- **labels** is a list of the 1,000 possible prediction classes (e.g., “airplane,” “broccoli,” “volcano”).\n",
        "\n",
        "In the next step, you’ll apply preprocess to each patch, feed them through model, and interpret the predicted top-k classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **🌟 The More You Know! — Transfer Learning, ImageNet, and Preprocessing**\n",
        "\n",
        "### **🧠 What is Transfer Learning?**\n",
        "\n",
        "Instead of training from scratch, you “transfer” knowledge from a model trained on a large, diverse dataset (ImageNet) to your own task.\n",
        "\n",
        "ResNet-50 has already learned to detect **edges, textures, shapes, and patterns** that are general to many kinds of imagery — these features transfer surprisingly well to new visual domains like remote sensing.\n",
        "\n",
        "> In short: we borrow a model’s\n",
        "> \n",
        "> \n",
        "> *visual literacy*\n",
        "> \n",
        "\n",
        "### **🏋️ ImageNet and**\n",
        "\n",
        "### **IMAGENET1K_V2**\n",
        "\n",
        "- **ImageNet-1K** contains **1.2 million** labeled photos in **1,000 categories** (cats, planes, churches, etc.).\n",
        "- **IMAGENET1K_V2** are refined weights released in PyTorch’s new API (better calibration, accuracy).\n",
        "- Using ResNet50_Weights.IMAGENET1K_V2 ensures consistency between preprocessing, model weights, and label mapping.\n",
        "\n",
        "### **🧩 Why weights.transforms()**\n",
        "\n",
        "### **Matters**\n",
        "\n",
        "Every pretrained model expects inputs scaled and normalized exactly as during training.\n",
        "\n",
        "Calling weights.transforms() automatically returns the correct preprocessing steps:\n",
        "\n",
        "1. Resize → 256 px shorter side\n",
        "2. Center-crop → 224×224\n",
        "3. Convert to tensor\n",
        "4. Normalize with ImageNet mean and std\n",
        "\n",
        "This alignment prevents subtle prediction errors caused by mismatched input distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Inference: top‑5 predictions per sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Objective:**\n",
        "\n",
        "Run **inference** with your pretrained **ResNet-50 (ImageNet)** model on each of your Sentinel-2 image patches and display the **top-5 predicted labels** (and their probabilities).\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "- **top5() helper function**: Converts raw model outputs (logits) into probabilities using softmax and extracts the 5 most likely classes.\n",
        "- **Inference loop**:\n",
        "    - Applies the correct preprocessing pipeline to each patch.\n",
        "    - Performs a forward pass through the network (model(inp)).\n",
        "    - Retrieves top-5 predictions and human-readable labels.\n",
        "- **results list**: Stores structured prediction data (class name, labels, probabilities) for later use — like generating Grad-CAMs or saving reports.\n",
        "\n",
        "**Why this matters:**\n",
        "\n",
        "This is your first hands-on view of how **general-purpose image recognition** models behave when faced with **remote-sensing data** — usually revealing mismatches between what the model “sees” (e.g., “sports field”) and the real surface type (“agriculture”)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Inference: top-5 predictions per sample\n",
        "\n",
        "# --- Helper function to extract top-5 predictions from logits ---\n",
        "\n",
        "def top5(logits):\n",
        "    # Apply softmax to convert logits → probabilities\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    # Extract top-5 probabilities and their indices\n",
        "    p, i = probs.topk(5, dim=1)\n",
        "    # Remove batch dimension for convenience\n",
        "    return p.squeeze(0).tolist(), i.squeeze(0).tolist()\n",
        "\n",
        "# --- Run inference on each patch ---\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, img in patches.items():\n",
        "    # Apply ImageNet preprocessing: resize, crop, normalize\n",
        "    inp = preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # Disable gradient tracking for efficiency (inference mode)\n",
        "    with torch.no_grad():\n",
        "        logits = model(inp)\n",
        "\n",
        "    # Get top-5 probabilities and class indices\n",
        "    p, idx = top5(logits)\n",
        "\n",
        "    # Convert indices to human-readable labels\n",
        "    labs = [labels[j] if labels else str(j) for j in idx]\n",
        "\n",
        "    # Print the sample name and its top-5 predictions (label + probability)\n",
        "    print(name, list(zip(labs, [round(x,4) for x in p])))\n",
        "\n",
        "    # Store structured results for later analysis\n",
        "    results.append({\n",
        "        'class': name,\n",
        "        'top5_labels': labs,\n",
        "        'top5_probs': [float(x) for x in p]\n",
        "    })\n",
        "\n",
        "# Display aggregated results for all patches\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outcome:**\n",
        "\n",
        "You now have a list of top-5 model predictions for each Sentinel-2 patch.\n",
        "\n",
        "Each patch produces:\n",
        "\n",
        "- `top5_labels`: the most likely ImageNet classes\n",
        "- `top5_probs`: corresponding probabilities (sum ≤ 1 because only top-5 shown)\n",
        "\n",
        "This demonstrates domain shift — how pretrained models interpret out-of-distribution data using their existing visual vocabulary. The `results` list feeds forward into CSV summaries, visualization panels, and Grad-CAM analyses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **🌟 The More You Know! — Logits, Softmax, and Top-k Predictions**\n",
        "\n",
        "### **🔢 Logits → Probabilities**\n",
        "\n",
        "A model’s raw output (logits) are **unnormalized scores** — higher means more confident, but they don’t sum to 1.\n",
        "\n",
        "torch.softmax(logits, dim=1) transforms them into **probabilities** that sum to 1 across all 1,000 classes.\n",
        "\n",
        "P(y_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
        "\n",
        "### **🥇 Top-k Logic**\n",
        "\n",
        "topk(5) returns the indices and probabilities of the 5 most likely predictions.\n",
        "\n",
        "The “top-5” metric is standard in ImageNet evaluation because it allows for near-misses (the true class being among the top-5 guesses).\n",
        "\n",
        "### **🧠 Why Use**\n",
        "\n",
        "### **torch.no_grad()**\n",
        "\n",
        "When you’re not training, you disable gradient computation to save memory and speed up inference.\n",
        "\n",
        "Without it, PyTorch tracks all operations for backpropagation, which is unnecessary here.\n",
        "\n",
        "### **🌍 Domain Shift in Action**\n",
        "\n",
        "- **ResNet-50** learned from human-eye-level photos (ImageNet).\n",
        "- **Sentinel-2** captures 10–20 m ground pixels from orbit.\n",
        " \n",
        "    The visual features overlap (edges, shapes, colors) but the *context* differs.\n",
        "    \n",
        "    This mismatch is what your analysis explores — how transferable general visual features are to environmental imagery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Visualization: multi-panel patches + predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Objective:**\n",
        "\n",
        "Create a clean, multi-panel **visual summary** showing each Sentinel-2 patch alongside its top-3 ImageNet predictions and probabilities.\n",
        "\n",
        "This is both a diagnostic tool and a storytelling artifact — allowing you to visually compare what the model perceives with what you know each scene represents.\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "- **Matplotlib grid layout** dynamically sized to the number of patches.\n",
        "- **Titles** summarize the model’s top-3 predictions (labels + confidence values).\n",
        "- **Saved figure** stored in your repository’s /figures directory for reproducibility and report inclusion.\n",
        "\n",
        "**Why this matters:**\n",
        "\n",
        "Visualizing results is crucial for interpretability. You’ll quickly spot patterns — for example, the model confusing “agriculture” with “golf course” or “runway.” These qualitative insights complement the quantitative outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7) Visualization: multi-panel patches + predictions\n",
        "import math\n",
        "\n",
        "# --- Layout setup ---\n",
        "n = len(patches)           # Number of image patches to display\n",
        "cols = 3                   # Number of columns in the grid\n",
        "rows = math.ceil(n / cols) if n else 1  # Rows needed to display all patches\n",
        "\n",
        "# Create a Matplotlib figure with dynamic sizing\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(12, 4 * rows))\n",
        "\n",
        "# Ensure axes are iterable even if only one subplot is created\n",
        "axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]\n",
        "\n",
        "# Hide empty subplot axes (for unused grid cells)\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "\n",
        "# --- Populate grid with image patches and predictions ---\n",
        "for i, (name, img) in enumerate(patches.items()):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(img)  # Show the Sentinel-2 RGB patch\n",
        "\n",
        "    # Retrieve model results corresponding to this patch\n",
        "    row = next(r for r in results if r['class'] == name)\n",
        "\n",
        "    # Compose title string with top-3 predicted labels and probabilities\n",
        "    title = (\n",
        "        f\"{name}: \" +\n",
        "        \", \".join([f\"{l} ({p:.2f})\" for l, p in zip(row['top5_labels'][:3], row['top5_probs'][:3])])\n",
        "    )\n",
        "\n",
        "    # Apply title to subplot\n",
        "    ax.set_title(title, fontsize=9)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# --- Save figure to /figures directory ---\n",
        "out_fig = FIGS / 'week2_imagenet_predictions.png'\n",
        "plt.savefig(out_fig, dpi=200)\n",
        "print('Saved:', out_fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outcome:**\n",
        "\n",
        "This cell produces a multi-panel figure — one image per sample patch — each annotated with its top-3 predicted ImageNet classes and corresponding confidence scores. The figure is saved to `figures/week2_imagenet_predictions.png`.\n",
        "\n",
        "You can now qualitatively assess how the pretrained ResNet model “reads” Earth’s surface textures. This provides a visual foundation for your Grad-CAM exploration — where you examine which parts of the image the model attends to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **🌟 The More You Know! — Visualizing AI Predictions**\n",
        "\n",
        "### **🎨 Why Multi-Panel Layouts Matter**\n",
        "\n",
        "Humans excel at **side-by-side comparison**.\n",
        "\n",
        "By juxtaposing images with their predictions, you can quickly detect patterns, misclassifications, or biases — for example, consistent misinterpretation of agricultural textures as sports fields.\n",
        "\n",
        "### **🧠 Reading the Titles**\n",
        "\n",
        "Each subplot title includes:\n",
        "\n",
        "- The **ground truth label** (Forest, Urban, etc.)\n",
        "- The **model’s top-3 predictions** (ImageNet categories)\n",
        "- The **confidence values** (softmax probabilities)\n",
        "\n",
        "Confidence values can be misleadingly high even for wrong classes — a reminder that model confidence ≠ correctness.\n",
        "\n",
        "### **💾 Reproducibility & Reporting**\n",
        "\n",
        "Saving the figure to /figures/week2_imagenet_predictions.png ensures your visual outputs are version-controlled and easy to embed in future reports, hackathon summaries, or reflective logs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Grad‑CAM Heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Objective:**\n",
        "Implement and apply Grad-CAM (Gradient-weighted Class Activation Mapping) to visualize where ResNet-50 is “looking” when it makes a classification decision on one Sentinel-2 patch.\n",
        "\n",
        "**Key Components:**\n",
        "\t•\tGradCAM class: Captures forward activations and backward gradients from a chosen layer (here, layer4, the final convolutional block in ResNet-50).\n",
        "\t•\tHook functions: Attach to the model so you can extract intermediate data during the forward and backward passes.\n",
        "\t•\tWeighted activation map computation: Combines gradients and activations to form a heatmap highlighting the most influential spatial regions.\n",
        "\t•\tVisualization: Overlays the heatmap (red = high importance) on the original image to interpret the model’s spatial attention.\n",
        "\n",
        "**Why this matters:**\n",
        "Grad-CAM translates numerical deep-learning processes into human-interpretable visuals, letting you see which features or textures influenced a model’s decision — crucial for building trust, identifying biases, and diagnosing domain shift."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8) Grad-CAM on one sample (ResNet-50 layer4)\n",
        "\n",
        "# --- Define a lightweight Grad-CAM utility class ---\n",
        "class GradCAM:\n",
        "    def __init__(self, model: torch.nn.Module, target_layer: torch.nn.Module):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None     # To store gradients from backward pass\n",
        "        self.activations = None   # To store activations from forward pass\n",
        "\n",
        "        # Register hooks on the target layer to capture activations and gradients\n",
        "        self.h1 = target_layer.register_forward_hook(self._fwd)\n",
        "        # Use full backward hook (register_backward_hook is deprecated)\n",
        "        self.h2 = target_layer.register_full_backward_hook(self._bwd)\n",
        "\n",
        "    def _fwd(self, m, i, o):\n",
        "        # Save forward activations (feature maps)\n",
        "        self.activations = o.detach()\n",
        "\n",
        "    def _bwd(self, m, gin, gout):\n",
        "        # Save gradients flowing back from the output\n",
        "        self.gradients = gout[0].detach()\n",
        "\n",
        "    def __call__(self, inp: torch.Tensor, class_idx: int | None = None):\n",
        "        # Zero existing gradients to avoid accumulation\n",
        "        self.model.zero_grad(set_to_none=True)\n",
        "        logits = self.model(inp)\n",
        "\n",
        "        # If no class index provided, use the model's top-predicted class\n",
        "        if class_idx is None:\n",
        "            class_idx = int(logits.argmax(dim=1).item())\n",
        "\n",
        "        # Compute gradient of the target class score w.r.t. activations\n",
        "        score = logits[:, class_idx]\n",
        "        score.backward(retain_graph=True)\n",
        "\n",
        "        # Retrieve stored gradients and activations\n",
        "        grads = self.gradients\n",
        "        acts = self.activations\n",
        "\n",
        "        # Compute per-channel weights: mean gradient over spatial dimensions\n",
        "        w = grads.mean(dim=(2,3), keepdim=True)\n",
        "\n",
        "        # Weighted sum of activations → class activation map (CAM)\n",
        "        cam = torch.relu((w * acts).sum(dim=1))[0].cpu().numpy()\n",
        "\n",
        "        # Normalize CAM to [0,1] for visualization\n",
        "        cam -= cam.min() if cam.max() != cam.min() else 0\n",
        "        cam /= cam.max() if cam.max() > 0 else 1\n",
        "        return cam\n",
        "\n",
        "# --- Select first sample for Grad-CAM visualization ---\n",
        "first_name, first_img = next(iter(patches.items()))\n",
        "first_inp = preprocess(first_img).unsqueeze(0).to(device)\n",
        "\n",
        "# Run forward pass to obtain predicted class index\n",
        "with torch.no_grad():\n",
        "    logits = model(first_inp)\n",
        "cls_idx = int(logits.argmax(dim=1).item())\n",
        "\n",
        "# --- Generate Grad-CAM heatmap for layer4 ---\n",
        "cam = GradCAM(model, model.layer4)(first_inp, cls_idx)\n",
        "\n",
        "# --- Plot original image and Grad-CAM overlay ---\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "ax[0].imshow(first_img)\n",
        "ax[0].set_title(f'{first_name} input')\n",
        "ax[0].axis('off')\n",
        "\n",
        "# Overlay CAM on image with transparency (jet colormap)\n",
        "ax[1].imshow(first_img)\n",
        "ax[1].imshow(cam, cmap='jet', alpha=0.4)\n",
        "ax[1].set_title('Grad-CAM layer4')\n",
        "ax[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# --- Save figure for reproducibility ---\n",
        "cam_path = FIGS / 'week2_gradcam_layer4.png'\n",
        "plt.savefig(cam_path, dpi=200)\n",
        "print('Saved:', cam_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outcome:**\n",
        "\n",
        "This cell generates and saves a two-panel figure showing:\n",
        "\n",
        "1. The original Sentinel-2 patch (left)\n",
        "2. The Grad-CAM overlay highlighting the areas that most influenced ResNet-50’s top prediction (right)\n",
        "\n",
        "Saved to `figures/week2_gradcam_layer4.png`.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "- Red/yellow regions = high activation importance\n",
        "- Blue/transparent areas = low influence\n",
        "\n",
        "This indicates which visual cues drove the model’s decision — useful for evaluating whether it’s reasoning sensibly or relying on artifacts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **🌟 The More You Know! — How Grad-CAM Sees Inside CNNs**\n",
        "\n",
        "### **🔍 The Core Idea**\n",
        "\n",
        "Grad-CAM computes a weighted sum of a convolutional layer’s activations, where each channel’s weight reflects **how important that feature map was** for the chosen class.\n",
        "\n",
        "Formally:\n",
        "\n",
        "\\text{Grad-CAM}(x,y) = \\text{ReLU}\\left( \\sum_k \\alpha_k A^k_{x,y} \\right), \\quad\n",
        "\\alpha_k = \\frac{1}{Z}\\sum_i\\sum_j \\frac{\\partial y^c}{\\partial A^k_{ij}}\n",
        "\n",
        "This produces a coarse spatial heatmap showing where the model “looked.”\n",
        "\n",
        "### **🧠 Why Layer 4?**\n",
        "\n",
        "- Early ResNet layers capture low-level features (edges, textures).\n",
        "- Deeper layers like **layer4** encode high-level semantic patterns (objects, shapes).\n",
        "    \n",
        "    Using layer4 gives you a meaningful, object-level attention map rather than noisy textures.\n",
        "    \n",
        "\n",
        "### **🎨 Reading the Heatmap**\n",
        "\n",
        "- **Bright areas** = strong influence on classification.\n",
        "- **Dark areas** = minimal effect.\n",
        "- The overlay’s transparency (alpha=0.4) allows you to see both the image and attention simultaneously.\n",
        "\n",
        "### **🪄 Conceptual Analogy**\n",
        "\n",
        "Imagine highlighting the parts of a photograph that most convinced an ecologist that an image shows “forest.”\n",
        "\n",
        "Grad-CAM does that automatically for your CNN — it’s the model’s *eye-tracking heatmap*.\n",
        "\n",
        "### **⚖️ Limitations**\n",
        "\n",
        "- Grad-CAM provides coarse spatial insights, not pixel-level accuracy.\n",
        "- Interpretations depend on the chosen layer — earlier layers give finer but less semantically clear results.\n",
        "- Still, it’s one of the most intuitive and widely used **explainability tools** in deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Export predictions to CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Objective:**\n",
        "Export all inference results (predicted labels and probabilities for each image patch) into a CSV summary file for transparent documentation and later analysis.\n",
        "\n",
        "**Key Components:**\n",
        "\t•\tCSV output file: /reports/week2_prediction_summary.csv\n",
        "\t•\tFields included:\n",
        "\t•\tclass — your true label (Forest, Water, etc.)\n",
        "\t•\ttop1_label / top1_prob — model’s most confident prediction and probability\n",
        "\t•\ttop5_labels / top5_probs — semicolon-separated lists of all top-5 predictions and probabilities\n",
        "\t•\tcsv.writer() — Python’s standard library writer for creating structured tabular files without extra dependencies.\n",
        "\n",
        "**Why this matters:**\n",
        "Saving structured outputs like this makes your workflow auditable, shareable, and reproducible — core principles of applied machine learning and scientific computing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9) Save prediction summary CSV\n",
        "import csv\n",
        "\n",
        "# Define output file path inside the reports directory\n",
        "csv_path = REPORTS / 'week2_prediction_summary.csv'\n",
        "\n",
        "# Open file for writing (overwrite if exists)\n",
        "with open(csv_path, 'w', newline='') as f:\n",
        "    w = csv.writer(f)\n",
        "\n",
        "    # Write header row\n",
        "    w.writerow(['class', 'top1_label', 'top1_prob', 'top5_labels', 'top5_probs'])\n",
        "\n",
        "    # Write one row per sample\n",
        "    for r in results:\n",
        "        top1_label = r['top5_labels'][0]  # Most likely predicted class\n",
        "        top1_prob = r['top5_probs'][0]    # Corresponding probability\n",
        "\n",
        "        # Join lists with semicolons for compact storage\n",
        "        w.writerow([\n",
        "            r['class'],                                   # Ground truth label\n",
        "            top1_label, f'{top1_prob:.4f}',               # Top-1 label and probability\n",
        "            ';'.join(r['top5_labels']),                   # All top-5 labels\n",
        "            ';'.join([f'{p:.4f}' for p in r['top5_probs']])  # All top-5 probabilities\n",
        "        ])\n",
        "\n",
        "print('Saved CSV:', csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outcome:**\n",
        "\n",
        "This cell generates a CSV summarizing model predictions for each sampled patch and confirms the path: `reports/week2_prediction_summary.csv`.\n",
        "\n",
        "This structured summary enables:\n",
        "\n",
        "- Quantitative review of model predictions\n",
        "- Integration with other analytical tools (e.g., pandas, Excel, Tableau)\n",
        "- Version-controlled record of inference results — useful for tracking model behavior over time\n",
        "\n",
        "You now have numerical (CSV), visual (multi-panel figure), and interpretability (Grad-CAM) artifacts, forming a complete analytical narrative for Week 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **🌟 The More You Know! — Why Saving Results Matters**\n",
        "\n",
        "### **📁 Reproducibility & Transparency**\n",
        "\n",
        "Saving results to CSV formalizes your analysis — anyone (including future-you) can re-open the file and reproduce or extend your work without rerunning the notebook.\n",
        "\n",
        "It’s the digital equivalent of **archiving lab notes**.\n",
        "\n",
        "### **🧩 Structured Data Pipelines**\n",
        "\n",
        "In real-world ML workflows, this step transitions you from experimentation to production-like structure.\n",
        "\n",
        "Downstream tools (e.g., dashboards, metrics scripts, model comparisons) all expect tidy, structured outputs like this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **🏁 Week 2 Notebook Summary — What You’ve Built**\n",
        "\n",
        "| **Step** | **Focus** | **Key Concept** |\n",
        "| --- | --- | --- |\n",
        "| 1–2 | Setup & AOI Load | Geospatial data handling, reproducibility |\n",
        "| 3 | Sentinel-2 Composite | Cloud masking & median compositing |\n",
        "| 4–5 | Sampling & Patching | Bridging Earth Engine and PyTorch |\n",
        "| 6 | Inference | Transfer learning, domain shift |\n",
        "| 7 | Visualization | Qualitative interpretability |\n",
        "| 8 | Grad-CAM | Model explainability |\n",
        "| 9 | CSV Export | Reproducible results & reporting |\n",
        "\n",
        "You’ve essentially built a complete mini-pipeline for geospatial AI model evaluation — end-to-end, explainable, and research-grade.\n",
        "\n",
        "**Next steps:**\n",
        "- Fine-tune ResNet on a small labeled Sentinel-2 set; compare pre/post Grad-CAM.\n",
        "- Swap in a remote-sensing model (e.g., ResNet pretrained on BigEarthNet) and re-run."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (geoai)",
      "language": "python",
      "name": "geoai"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
