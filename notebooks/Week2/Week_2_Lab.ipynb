{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Lab: From ImageNet to GeoVision\n",
    "\n",
    "**Objective:** Explore how pre-trained computer vision models perform on Earth observation imagery to understand domain shift.\n",
    "\n",
    "**Learning Goals:**\n",
    "- Load and use pre-trained ImageNet models\n",
    "- Apply models to satellite imagery from your study region\n",
    "- Visualize model activations and predictions\n",
    "- Analyze domain shift and model failures\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch and TorchVision\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "# Earth Engine and geemap\n",
    "import ee\n",
    "import geemap\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Initialize Earth Engine\n",
    "try:\n",
    "    ee.Initialize()\n",
    "    print('EE initialized')\n",
    "except Exception:\n",
    "    print('EE not initialized; authenticating...')\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  Earth Engine initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load Pre-Trained ImageNet Model\n",
    "\n",
    "We'll use ResNet-50, a popular CNN architecture trained on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet-50 (ImageNet)\n",
    "print(\"Loading ResNet-50 (this may take a moment on first run)...\")\n",
    "\n",
    "# Device selection\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Use modern weights API\n",
    "weights = ResNet50_Weights.IMAGENET1K_V2\n",
    "model = resnet50(weights=weights).to(device).eval()\n",
    "\n",
    "print(\"✓ Model loaded successfully\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Model has {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet labels and preprocessing from weights\n",
    "imagenet_labels = weights.meta.get('categories', None)\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "print(f\"✓ Loaded {len(imagenet_labels) if imagenet_labels else 'unknown'} ImageNet class labels from weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Select Your Area of Interest (Interactive)\n",
    "\n",
    "Draw your AOI on the map. This ensures you precisely define the study region before fetching imagery.\n",
    "\n",
    "Instructions:\n",
    "- Use the draw tool to create a Polygon or Rectangle around your AOI.\n",
    "- Click \"Set AOI\" to save it to the variable `aoi`.\n",
    "- Then run the next cell to fetch Sentinel‑2 imagery for your AOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive AOI selection\n",
    "# How to use:\n",
    "# 1) Use the map's draw tool (polygon or rectangle) to delineate your AOI.\n",
    "# 2) Click \"Set AOI\" to store it in the variable `aoi`.\n",
    "# 3) Run the next cell to fetch Sentinel-2 imagery.\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipyleaflet import DrawControl\n",
    "from IPython.display import display\n",
    "\n",
    "# Create/clear a working map\n",
    "AOI_Map = geemap.Map()\n",
    "\n",
    "# Optional: show a basemap users can orient with\n",
    "AOI_Map.addLayer(ee.Image().visualize(), {}, 'Basemap')\n",
    "\n",
    "# Drawing control for polygon/rectangle only\n",
    "if not any(isinstance(c, DrawControl) for c in AOI_Map.controls):\n",
    "    dc_aoi = DrawControl(marker={}, polyline={}, circle={}, circlemarker={},\n",
    "                         polygon={\"shapeOptions\": {}}, rectangle={\"shapeOptions\": {}})\n",
    "    AOI_Map.add_control(dc_aoi)\n",
    "else:\n",
    "    dc_aoi = [c for c in AOI_Map.controls if isinstance(c, DrawControl)][0]\n",
    "\n",
    "_aoi_geom = {'type': None, 'coordinates': None}\n",
    "status_aoi = widgets.HTML('Draw a Polygon/Rectangle, then click \"Set AOI\".')\n",
    "set_btn = widgets.Button(description='Set AOI', button_style='primary')\n",
    "\n",
    "# Capture last drawn polygon/rectangle\n",
    "\n",
    "def _on_draw_aoi(target, action, geo_json):\n",
    "    if action != 'created':\n",
    "        return\n",
    "    g = geo_json.get('geometry', {})\n",
    "    if g.get('type') in ('Polygon', 'Rectangle') or g.get('type') == 'MultiPolygon':\n",
    "        _aoi_geom['type'] = g['type']\n",
    "        _aoi_geom['coordinates'] = g['coordinates']\n",
    "        status_aoi.value = 'AOI geometry captured. Click \"Set AOI\".'\n",
    "    else:\n",
    "        status_aoi.value = '<b>Use Polygon or Rectangle for AOI.</b>'\n",
    "\n",
    "dc_aoi.on_draw(_on_draw_aoi)\n",
    "\n",
    "aoi = None\n",
    "\n",
    "def _set_aoi(_):\n",
    "    global aoi\n",
    "    if not _aoi_geom['coordinates']:\n",
    "        status_aoi.value = '<b>No geometry found.</b> Draw Polygon/Rectangle first.'\n",
    "        return\n",
    "    # Build EE geometry (handle both Polygon/Rectangle)\n",
    "    try:\n",
    "        if _aoi_geom['type'] == 'Polygon':\n",
    "            aoi = ee.Geometry.Polygon(_aoi_geom['coordinates'])\n",
    "        elif _aoi_geom['type'] == 'Rectangle':\n",
    "            # Rectangle coordinates given as polygon; wrap directly\n",
    "            aoi = ee.Geometry.Polygon(_aoi_geom['coordinates'])\n",
    "        elif _aoi_geom['type'] == 'MultiPolygon':\n",
    "            aoi = ee.Geometry.MultiPolygon(_aoi_geom['coordinates'])\n",
    "        else:\n",
    "            raise ValueError('Unsupported geometry type for AOI.')\n",
    "        status_aoi.value = '✓ AOI set. You can proceed to the next cell.'\n",
    "        # Outline AOI for feedback\n",
    "        outline = ee.Image().paint(aoi, 0, 2)\n",
    "        AOI_Map.addLayer(outline, {'palette': ['cyan']}, 'AOI (selected)')\n",
    "        AOI_Map.centerObject(aoi, 10)\n",
    "    except Exception as e:\n",
    "        status_aoi.value = f'<b>Error setting AOI:</b> {e}'\n",
    "\n",
    "set_btn.on_click(_set_aoi)\n",
    "\n",
    "ui = widgets.VBox([AOI_Map, widgets.HBox([set_btn]), status_aoi])\n",
    "display(ui)\n",
    "print(\"When AOI is set, proceed to load Sentinel-2 imagery below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentinel-2 imagery for the selected AOI\n",
    "assert aoi is not None, \"AOI is not set. Draw and 'Set AOI' above first.\"\n",
    "\n",
    "s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "\n",
    "# Filter for your region and time period\n",
    "image = (s2\n",
    "    .filterBounds(aoi)\n",
    "    .filterDate('2023-06-01', '2023-09-01')  # Adjust dates as needed\n",
    "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n",
    "    .median())  # Create median composite\n",
    "\n",
    "# Select RGB bands\n",
    "rgb_image = image.select(['B4', 'B3', 'B2'])\n",
    "\n",
    "print(\"✓ Sentinel-2 imagery loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Define Sample Locations (Interactive)\n",
    "\n",
    "You will identify three land-cover categories inside your AOI using the map below. This visual check ensures your samples are within bounds and representative.\n",
    "\n",
    "Instructions:\n",
    "- Use the dropdown to choose a class (Forest, Water, Urban, Agriculture).\n",
    "- Click inside the AOI to add a point. The point is colored by class and validated against the AOI.\n",
    "- Add at least three points covering at least three distinct classes.\n",
    "- Click Export when done; this populates `land_cover_samples` for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive section below defines `land_cover_samples`.\n",
    "# Use the map and Export button; this placeholder ensures the name exists.\n",
    "land_cover_samples = {}\n",
    "print(\"Awaiting interactive sampling...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive sampling: add labeled points within the AOI\n",
    "# How to use:\n",
    "# 1) Pick a class in the Class dropdown (Forest/Water/Urban/Agriculture).\n",
    "# 2) Click on the map INSIDE the AOI outline to drop a point for that class.\n",
    "# 3) Repeat for at least three distinct classes.\n",
    "# 4) Click \"Export samples\" when done. Then run the next cell to confirm.\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipyleaflet import DrawControl\n",
    "from IPython.display import display\n",
    "\n",
    "# Ensure a sampling map exists (create if missing)\n",
    "try:\n",
    "    Map\n",
    "except NameError:\n",
    "    Map = geemap.Map()\n",
    "    try:\n",
    "        aoi_geom = aoi.geometry()\n",
    "    except Exception:\n",
    "        aoi_geom = aoi\n",
    "    Map.add_basemap('SATELLITE')\n",
    "    Map.addLayer(rgb_image, {'min': 0, 'max': 3000}, 'S2 RGB (median)')\n",
    "    outline = ee.Image().paint(aoi_geom, 0, 2)\n",
    "    Map.addLayer(outline, {'palette': ['00FFFF']}, 'AOI (outline)')\n",
    "    Map.centerObject(aoi_geom, 12)\n",
    "    display(Map)\n",
    "\n",
    "label_dd = widgets.Dropdown(\n",
    "    options=['Forest', 'Water', 'Urban', 'Agriculture'],\n",
    "    value='Forest',\n",
    "    description='Class:'\n",
    ")\n",
    "save_btn = widgets.Button(description='Export samples', button_style='primary')\n",
    "status = widgets.HTML('Pick a Class, click on map to add a point. Then click Export.')\n",
    "\n",
    "# Add a point-only drawing control (reuse if already added)\n",
    "if not any(isinstance(c, DrawControl) for c in Map.controls):\n",
    "    dc = DrawControl(marker={\"shapeOptions\": {}}, polyline={}, polygon={}, circle={}, rectangle={}, circlemarker={})\n",
    "    Map.add_control(dc)\n",
    "else:\n",
    "    dc = [c for c in Map.controls if isinstance(c, DrawControl)][0]\n",
    "\n",
    "collected = []\n",
    "\n",
    "# AOI geometry for validation\n",
    "try:\n",
    "    aoi_geom = aoi.geometry()\n",
    "except Exception:\n",
    "    aoi_geom = aoi\n",
    "\n",
    "# Color by class for quick visual feedback\n",
    "cls_color = {'Forest': 'green', 'Water': 'blue', 'Urban': 'red', 'Agriculture': 'orange'}\n",
    "\n",
    "def _on_draw(target, action, geo_json):\n",
    "    if action != 'created' or geo_json.get('geometry', {}).get('type') != 'Point':\n",
    "        return\n",
    "    lon, lat = geo_json['geometry']['coordinates']\n",
    "    label = label_dd.value\n",
    "    inside = ee.Geometry(aoi_geom).contains(ee.Geometry.Point([lon, lat])).getInfo()\n",
    "    collected.append({'label': label, 'lon': lon, 'lat': lat, 'inside': inside})\n",
    "    # Visual marker via EE layer color\n",
    "    Map.addLayer(ee.Geometry.Point([lon, lat]), {'color': cls_color.get(label, 'yellow')}, f'{label} ({len(collected)})')\n",
    "    if not inside:\n",
    "        status.value = f\"<b>Warning:</b> Outside AOI: {label} @ ({lon:.5f}, {lat:.5f}). Place inside outline.\"\n",
    "    else:\n",
    "        status.value = f\"Added {label} @ ({lon:.5f}, {lat:.5f})\"\n",
    "\n",
    "dc.on_draw(_on_draw)\n",
    "\n",
    "# Export to a Python dict of ee.Geometry.Point for downstream cells\n",
    "land_cover_samples = {}\n",
    "\n",
    "def _export_samples(_):\n",
    "    global land_cover_samples\n",
    "    land_cover_samples = {f\"{d['label']}_{i}\": ee.Geometry.Point([d['lon'], d['lat']]) for i, d in enumerate(collected, 1)}\n",
    "    outside = [d for d in collected if not d['inside']]\n",
    "    if outside:\n",
    "        print(f\"⚠ {len(outside)} point(s) outside AOI. Remove or re-add before proceeding.\")\n",
    "    print(f\"✓ Collected {len(collected)} point(s). 'land_cover_samples' set.\")\n",
    "    print(\"Next: run the 'Finalize your samples' cell below to confirm and preview.\")\n",
    "\n",
    "save_btn.on_click(_export_samples)\n",
    "\n",
    "widgets.VBox([\n",
    "    widgets.HBox([label_dd, save_btn]),\n",
    "    status\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling map: centered on AOI with true-color context\n",
    "Map = geemap.Map()\n",
    "\n",
    "# Ensure we have a geometry for centering and outline\n",
    "try:\n",
    "    aoi_geom = aoi.geometry()\n",
    "except Exception:\n",
    "    aoi_geom = aoi\n",
    "\n",
    "# Basemap imagery for context\n",
    "Map.add_basemap('SATELLITE')\n",
    "\n",
    "# Convert sample dict to an EE FeatureCollection with labels (may be empty)\n",
    "features = [ee.Feature(geom, {'label': name}) for name, geom in land_cover_samples.items()]\n",
    "fc_points = ee.FeatureCollection(features)\n",
    "\n",
    "# Add layers: Sentinel‑2 RGB + AOI outline + any existing points\n",
    "Map.addLayer(rgb_image, {'min': 0, 'max': 3000}, 'S2 RGB (median)')\n",
    "outline = ee.Image().paint(aoi_geom, 0, 2)  # 2 px outline\n",
    "Map.addLayer(outline, {'palette': ['00FFFF']}, 'AOI (outline)')\n",
    "if len(features):\n",
    "    Map.addLayer(fc_points, {'color': 'red'}, 'Sample points')\n",
    "\n",
    "# Center map tighter for sampling\n",
    "Map.centerObject(aoi_geom, 12)\n",
    "Map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize your samples\n",
    "\n",
    "- If you have added points, click Export.\n",
    "- Then run the cell below to confirm and proceed. You need at least three distinct classes (e.g., Forest, Water, Urban).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate and display collected samples\n",
    "assert land_cover_samples, \"No samples found. Use the map to add points, then click Export.\"\n",
    "\n",
    "# Count distinct classes\n",
    "classes = [k.split('_')[0] for k in land_cover_samples.keys()]\n",
    "num_classes = len(set(classes))\n",
    "print(f\"Collected {len(land_cover_samples)} point(s) across {num_classes} class(es):\")\n",
    "for name, geom in land_cover_samples.items():\n",
    "    print(\"  \", name, geom.coordinates().getInfo())\n",
    "\n",
    "assert num_classes >= 3, \"Collect at least three distinct classes before continuing.\"\n",
    "\n",
    "# Build a map preview of the exported samples\n",
    "Map = geemap.Map()\n",
    "try:\n",
    "    aoi_geom = aoi.geometry()\n",
    "except Exception:\n",
    "    aoi_geom = aoi\n",
    "\n",
    "Map.addLayer(rgb_image, {'min': 0, 'max': 3000}, 'S2 RGB (median)')\n",
    "Map.addLayer(aoi_geom, {'color': '#00FFFF'}, 'AOI')\n",
    "fc = ee.FeatureCollection([ee.Feature(g, {'label': k}) for k, g in land_cover_samples.items()])\n",
    "Map.addLayer(fc, {'color': 'yellow'}, 'Exported samples')\n",
    "Map.centerObject(aoi_geom, 10)\n",
    "Map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Helper Functions\n",
    "\n",
    "Functions to download imagery and run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ee_to_numpy(image, region, scale=10):\n",
    "    \"\"\"\n",
    "    Download Earth Engine image as numpy array.\n",
    "    \n",
    "    Args:\n",
    "        image: Earth Engine image (ee.Image)\n",
    "        region: Earth Engine geometry (ee.Geometry)\n",
    "        scale: Spatial resolution in meters (e.g., 10 or 20)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (H, W, C)\n",
    "    \"\"\"\n",
    "    # Reproject for consistent sampling resolution\n",
    "    img = image.reproject(crs='EPSG:4326', scale=scale)\n",
    "\n",
    "    # Sample rectangle\n",
    "    image_data = img.sampleRectangle(region=region, defaultValue=0)\n",
    "    \n",
    "    # Convert to numpy in RGB band order\n",
    "    bands = ['B4', 'B3', 'B2']  # RGB\n",
    "    arrays = [np.array(image_data.get(band).getInfo()) for band in bands]\n",
    "    \n",
    "    # Stack to (H, W, C)\n",
    "    rgb_array = np.stack(arrays, axis=-1)\n",
    "    \n",
    "    return rgb_array\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline is provided by weights (see above)\n",
    "\n",
    "def predict_patch(rgb_array, model, preprocess, imagenet_labels, top_k=5):\n",
    "    \"\"\"\n",
    "    Run ImageNet model inference on a satellite image patch.\n",
    "    \n",
    "    Args:\n",
    "        rgb_array: numpy array of shape (H, W, 3)\n",
    "        model: PyTorch model\n",
    "        preprocess: torchvision transforms (from weights)\n",
    "        imagenet_labels: list of class labels (from weights.meta)\n",
    "        top_k: number of top predictions to return\n",
    "    \n",
    "    Returns:\n",
    "        list of (label, confidence) tuples, and the normalized RGB array\n",
    "    \"\"\"\n",
    "    # Normalize to 0-255 range\n",
    "    rgb_norm = (rgb_array - rgb_array.min()) / (rgb_array.max() - rgb_array.min()) * 255\n",
    "    rgb_norm = rgb_norm.astype(np.uint8)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    pil_image = Image.fromarray(rgb_norm)\n",
    "    \n",
    "    # Preprocess and move to device\n",
    "    input_tensor = preprocess(pil_image)\n",
    "    input_batch = input_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "    \n",
    "    # Get top-k predictions (move to CPU for numpy-friendly ops)\n",
    "    probabilities = torch.nn.functional.softmax(output[0].to('cpu'), dim=0)\n",
    "    top_prob, top_idx = torch.topk(probabilities, top_k)\n",
    "    \n",
    "    # Format results\n",
    "    predictions = []\n",
    "    for prob, idx in zip(top_prob, top_idx):\n",
    "        label = imagenet_labels[idx.item()] if imagenet_labels else str(idx.item())\n",
    "        predictions.append({\n",
    "            'label': label,\n",
    "            'confidence': prob.item() * 100\n",
    "        })\n",
    "    \n",
    "    return predictions, rgb_norm\n",
    "\n",
    "print(\"✓ Prediction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Run Experiments\n",
    "\n",
    "Test the ImageNet model on each land cover type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "results = {}\n",
    "patches = {}\n",
    "\n",
    "# Test each land cover type\n",
    "for lc_type, point in land_cover_samples.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {lc_type}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Define region (500m buffer)\n",
    "        region = point.buffer(500)\n",
    "        \n",
    "        # Download patch\n",
    "        print(\"  Downloading patch...\")\n",
    "        rgb_array = ee_to_numpy(rgb_image, region)\n",
    "        print(f\"  Patch shape: {rgb_array.shape}\")\n",
    "        \n",
    "        # Run prediction\n",
    "        print(\"  Running inference...\")\n",
    "        predictions, rgb_norm = predict_patch(\n",
    "            rgb_array, model, preprocess, imagenet_labels\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results[lc_type] = predictions\n",
    "        patches[lc_type] = rgb_norm\n",
    "        \n",
    "        # Display predictions\n",
    "        print(\"\\n  Top 5 Predictions:\")\n",
    "        for i, pred in enumerate(predictions, 1):\n",
    "            print(f\"    {i}. {pred['label']:30s} {pred['confidence']:5.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "        results[lc_type] = None\n",
    "        patches[lc_type] = None\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Experiments complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualize Results\n",
    "\n",
    "Create a multi-panel figure showing patches and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "n_samples = len([p for p in patches.values() if p is not None])\n",
    "fig, axes = plt.subplots(2, n_samples, figsize=(4*n_samples, 8))\n",
    "\n",
    "col = 0\n",
    "for lc_type in land_cover_samples.keys():\n",
    "    if patches[lc_type] is None:\n",
    "        continue\n",
    "    \n",
    "    # Top row: satellite image\n",
    "    axes[0, col].imshow(patches[lc_type])\n",
    "    axes[0, col].set_title(f'Actual: {lc_type}', fontweight='bold', fontsize=12)\n",
    "    axes[0, col].axis('off')\n",
    "    \n",
    "    # Bottom row: predictions\n",
    "    axes[1, col].axis('off')\n",
    "    \n",
    "    # Format predictions as text\n",
    "    pred_text = \"Top 3 Predictions:\\n\\n\"\n",
    "    for i, pred in enumerate(results[lc_type][:3], 1):\n",
    "        pred_text += f\"{i}. {pred['label']}\\n\"\n",
    "        pred_text += f\"   ({pred['confidence']:.1f}%)\\n\\n\"\n",
    "    \n",
    "    axes[1, col].text(0.1, 0.5, pred_text, \n",
    "                     fontsize=10, verticalalignment='center',\n",
    "                     family='monospace')\n",
    "    \n",
    "    col += 1\n",
    "\n",
    "plt.suptitle('ImageNet Model Predictions on Satellite Imagery', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/week2_imagenet_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualization saved to ../figures/week2_imagenet_predictions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Activation Visualization\n",
    "\n",
    "Visualize what the model is \"looking at\" using feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one example for detailed activation analysis (auto-pick)\n",
    "# Prefer any key starting with \"Forest\" (e.g., \"Forest_1\"); otherwise use the first available patch.\n",
    "preferred = 'Forest'\n",
    "available = [k for k, v in patches.items() if v is not None]\n",
    "if not available:\n",
    "    print(\"✗ No patches available. Run Part 5 first.\")\n",
    "else:\n",
    "    candidates = [k for k in available if k.lower().startswith(preferred.lower())]\n",
    "    example_key = candidates[0] if candidates else available[0]\n",
    "\n",
    "    # Get the patch\n",
    "    rgb_array = patches[example_key]\n",
    "    \n",
    "    # Preprocess\n",
    "    rgb_norm = (rgb_array - rgb_array.min()) / (rgb_array.max() - rgb_array.min()) * 255\n",
    "    rgb_norm = rgb_norm.astype(np.uint8)\n",
    "    pil_image = Image.fromarray(rgb_norm)\n",
    "    input_tensor = preprocess(pil_image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Create feature extractor\n",
    "    feature_extractor = create_feature_extractor(\n",
    "        model, \n",
    "        return_nodes={'layer4': 'features'}\n",
    "    )\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor(input_tensor)['features']\n",
    "    \n",
    "    # Average across channels (move to CPU)\n",
    "    activation_map = features[0].to('cpu').mean(dim=0).numpy()\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(rgb_array)\n",
    "    axes[0].set_title(f'Sentinel-2 RGB: {example_key}', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Activation map\n",
    "    im = axes[1].imshow(activation_map, cmap='hot')\n",
    "    top_pred = results[example_key][0]\n",
    "    axes[1].set_title(f'Activation Map\\nPredicted: {top_pred[\"label\"]} ({top_pred[\"confidence\"]:.1f}%)',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[1], fraction=0.046)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../figures/week2_activation_{example_key.lower()}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Activation map saved for {example_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Analysis and Reflection\n",
    "\n",
    "Answer these questions based on your experiments:\n",
    "\n",
    "### Observations\n",
    "\n",
    "1. **Which land cover types were most misclassified?**\n",
    "\n",
    "   *Your answer here*\n",
    "\n",
    "2. **What patterns do you notice in the misclassifications?**\n",
    "\n",
    "   *Your answer here*\n",
    "\n",
    "3. **How do the activation maps relate to the predictions?**\n",
    "\n",
    "   *Your answer here*\n",
    "\n",
    "### Explanations\n",
    "\n",
    "4. **Why do you think the model made these specific errors?**\n",
    "\n",
    "   *Your answer here (consider scale, spectral information, training data)*\n",
    "\n",
    "5. **How does this demonstrate the domain shift problem?**\n",
    "\n",
    "   *Your answer here*\n",
    "\n",
    "### Implications\n",
    "\n",
    "6. **What would you need to do to make this model work better for Earth observation?**\n",
    "\n",
    "   *Your answer here*\n",
    "\n",
    "7. **What does this tell you about the importance of domain-specific training?**\n",
    "\n",
    "   *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Summary Statistics\n",
    "\n",
    "Create a summary table of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for lc_type, preds in results.items():\n",
    "    if preds is not None:\n",
    "        top_pred = preds[0]\n",
    "        summary_data.append({\n",
    "            'Actual Land Cover': lc_type,\n",
    "            'Top Prediction': top_pred['label'],\n",
    "            'Confidence (%)': f\"{top_pred['confidence']:.1f}\",\n",
    "            'Correct?': 'No' if lc_type.lower() not in top_pred['label'].lower() else 'Maybe'\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nSummary of ImageNet Predictions on Satellite Imagery:\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save to file\n",
    "summary_df.to_csv('../reports/week2_prediction_summary.csv', index=False)\n",
    "print(\"\\n✓ Summary saved to ../reports/week2_prediction_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This experiment demonstrates the **domain shift problem**: models trained on natural images (ImageNet) often fail when applied to satellite imagery because:\n",
    "\n",
    "1. **Scale mismatch**: Satellite features are at different scales than everyday objects\n",
    "2. **Spectral vs. visual**: Models learn visual appearance, not spectral signatures\n",
    "3. **Context matters**: Geographic and environmental context is crucial for Earth observation\n",
    "4. **Training distribution**: ImageNet contains photos, not satellite imagery\n",
    "\n",
    "**Next steps:**\n",
    "- Week 3: Train CNNs specifically for Earth observation data\n",
    "- Weeks 4-6: Explore transfer learning and domain adaptation\n",
    "- Weeks 7-10: Use foundation models trained on satellite imagery\n",
    "\n",
    "---\n",
    "\n",
    "**Deliverables for this lab:**\n",
    "- ✅ This completed notebook\n",
    "- ✅ Prediction visualization: `figures/week2_imagenet_predictions.png`\n",
    "- ✅ Activation maps: `figures/week2_activation_*.png`\n",
    "- ✅ Summary table: `reports/week2_prediction_summary.csv`\n",
    "- ✅ Domain shift analysis: `reports/Week2_Domain_Shift_Analysis.md` (separate document)\n",
    "- ✅ Ethics Thread post: `reports/Week2_Ethics_Thread.md` (separate document)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geoai)",
   "language": "python",
   "name": "geoai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
