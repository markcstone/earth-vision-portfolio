{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Lab: CNN Land-Cover Classification\n",
    "\n",
    "**Objective**: Build, train, and interpret a convolutional neural network for land-cover classification using Sentinel-2 imagery.\n",
    "\n",
    "**Learning Goals**:\n",
    "- Implement a CNN architecture in PyTorch\n",
    "- Prepare geospatial data for deep learning\n",
    "- Train a model with proper monitoring\n",
    "- Evaluate performance with appropriate metrics\n",
    "- Interpret learned features through visualization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "import geopandas as gpd\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Google Earth Engine\n",
    "import ee\n",
    "import geemap\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Earth Engine\n",
    "try:\n",
    "    ee.Initialize()\n",
    "    print(\"Earth Engine initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Earth Engine: {e}\")\n",
    "    print(\"Run: earthengine authenticate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "dirs = [\n",
    "    'data/raw',\n",
    "    'data/processed',\n",
    "    'data/labels',\n",
    "    'models',\n",
    "    'figures',\n",
    "    'reports',\n",
    "    'config'\n",
    "]\n",
    "\n",
    "for dir_path in dirs:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Directory structure created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Configuration\n",
    "\n",
    "Define all hyperparameters and settings in one place for easy modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration dictionary\n",
    "config = {\n",
    "    # Data parameters\n",
    "    'aoi': {  # Replace with your Area of Interest\n",
    "        'name': 'Los_Lagos',  # or 'Central_Chile' or 'Lake_Llanquihue'\n",
    "        'bounds': [-73.5, -42.0, -72.5, -41.0],  # [west, south, east, north]\n",
    "    },\n",
    "    'date_range': ['2023-06-01', '2023-08-31'],  # Summer months\n",
    "    'cloud_threshold': 20,  # Maximum cloud cover percentage\n",
    "    \n",
    "    # Class definitions\n",
    "    'classes': {\n",
    "        0: 'Forest',\n",
    "        1: 'Agriculture',\n",
    "        2: 'Parcels',\n",
    "        3: 'Water'\n",
    "    },\n",
    "    'num_classes': 4,\n",
    "    \n",
    "    # Sentinel-2 bands to use\n",
    "    'bands': ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'],\n",
    "    'num_bands': 10,\n",
    "    \n",
    "    # Patch extraction\n",
    "    'patch_size': 128,  # Pixels\n",
    "    'stride': 64,  # Overlap for more training examples\n",
    "    \n",
    "    # Data splitting\n",
    "    'train_split': 0.7,\n",
    "    'val_split': 0.3,\n",
    "    \n",
    "    # Model architecture\n",
    "    'model': {\n",
    "        'name': 'SimpleCNN',\n",
    "        'channels': [32, 64, 128],  # Channels in each conv block\n",
    "        'dropout': 0.5\n",
    "    },\n",
    "    \n",
    "    # Training parameters\n",
    "    'training': {\n",
    "        'batch_size': 32,\n",
    "        'num_epochs': 50,\n",
    "        'learning_rate': 0.001,\n",
    "        'optimizer': 'Adam',\n",
    "        'weight_decay': 0.01,\n",
    "        'scheduler': 'ReduceLROnPlateau',\n",
    "        'scheduler_patience': 5,\n",
    "        'early_stopping_patience': 10\n",
    "    },\n",
    "    \n",
    "    # Device\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Paths\n",
    "    'paths': {\n",
    "        'labels': 'data/labels/training_polygons.geojson',\n",
    "        'processed_data': 'data/processed',\n",
    "        'model_save': 'models/week3_best_model.pth',\n",
    "        'config_save': 'config/week3_config.json'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "with open(config['paths']['config_save'], 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Collection and Preparation\n",
    "\n",
    "### Step 3.1: Define Area of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AOI as Earth Engine geometry\n",
    "aoi_bounds = config['aoi']['bounds']\n",
    "aoi = ee.Geometry.Rectangle(aoi_bounds)\n",
    "\n",
    "# Visualize AOI\n",
    "Map = geemap.Map()\n",
    "Map.centerObject(aoi, zoom=10)\n",
    "Map.addLayer(aoi, {'color': 'red'}, 'AOI')\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Load Training Labels\n",
    "\n",
    "**Note**: You need to create training labels first using QGIS. See the Detailed Study Guide for instructions.\n",
    "\n",
    "Your GeoJSON should have a 'class' field with values 0-3 corresponding to your land-cover classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training polygons\n",
    "labels_path = config['paths']['labels']\n",
    "\n",
    "if os.path.exists(labels_path):\n",
    "    training_labels = gpd.read_file(labels_path)\n",
    "    print(f\"Loaded {len(training_labels)} training polygons\")\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(training_labels['class'].value_counts().sort_index())\n",
    "    \n",
    "    # Visualize on map\n",
    "    Map.addLayer(ee.FeatureCollection(labels_path), {'color': 'blue'}, 'Training Labels')\n",
    "else:\n",
    "    print(f\"Training labels not found at {labels_path}\")\n",
    "    print(\"Please create training labels in QGIS first.\")\n",
    "    print(\"See Week 3 Detailed Study Guide, Section 'Collecting Training Data'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Collect Sentinel-2 Imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentinel2_composite(aoi, date_range, cloud_threshold, bands):\n",
    "    \"\"\"\n",
    "    Get cloud-free Sentinel-2 composite for AOI.\n",
    "    \n",
    "    Args:\n",
    "        aoi: Earth Engine geometry\n",
    "        date_range: [start_date, end_date] as strings\n",
    "        cloud_threshold: Maximum cloud cover percentage\n",
    "        bands: List of band names to include\n",
    "    \n",
    "    Returns:\n",
    "        Earth Engine image\n",
    "    \"\"\"\n",
    "    # Load Sentinel-2 Surface Reflectance collection\n",
    "    s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "        .filterBounds(aoi) \\\n",
    "        .filterDate(date_range[0], date_range[1]) \\\n",
    "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', cloud_threshold))\n",
    "    \n",
    "    # Cloud masking function\n",
    "    def mask_clouds(image):\n",
    "        qa = image.select('QA60')\n",
    "        cloud_mask = qa.bitwiseAnd(1 << 10).eq(0).And(\n",
    "                     qa.bitwiseAnd(1 << 11).eq(0))\n",
    "        return image.updateMask(cloud_mask).select(bands)\n",
    "    \n",
    "    # Apply cloud masking and create median composite\n",
    "    composite = s2.map(mask_clouds).median().clip(aoi)\n",
    "    \n",
    "    return composite\n",
    "\n",
    "# Get composite\n",
    "s2_composite = get_sentinel2_composite(\n",
    "    aoi,\n",
    "    config['date_range'],\n",
    "    config['cloud_threshold'],\n",
    "    config['bands']\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "vis_params = {\n",
    "    'bands': ['B4', 'B3', 'B2'],\n",
    "    'min': 0,\n",
    "    'max': 3000,\n",
    "    'gamma': 1.4\n",
    "}\n",
    "Map.addLayer(s2_composite, vis_params, 'Sentinel-2 Composite')\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4: Extract Patches and Labels\n",
    "\n",
    "This is a simplified version. For production use, consider using TorchGeo or custom data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches_from_labels(image, labels_gdf, patch_size, class_field='class'):\n",
    "    \"\"\"\n",
    "    Extract image patches centered on training label polygons.\n",
    "    \n",
    "    This is a simplified approach. For each polygon:\n",
    "    1. Get centroid\n",
    "    2. Extract patch_size x patch_size region\n",
    "    3. Assign label based on polygon class\n",
    "    \n",
    "    Args:\n",
    "        image: Earth Engine image\n",
    "        labels_gdf: GeoDataFrame with training polygons\n",
    "        patch_size: Size of patches in pixels\n",
    "        class_field: Field name containing class labels\n",
    "    \n",
    "    Returns:\n",
    "        patches: List of numpy arrays (num_bands, patch_size, patch_size)\n",
    "        labels: List of class labels\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    labels = []\n",
    "    \n",
    "    # Get image projection and scale\n",
    "    projection = image.select(0).projection()\n",
    "    scale = projection.nominalScale().getInfo()\n",
    "    \n",
    "    print(f\"Extracting patches at {scale}m resolution...\")\n",
    "    \n",
    "    for idx, row in labels_gdf.iterrows():\n",
    "        # Get polygon centroid\n",
    "        centroid = row.geometry.centroid\n",
    "        lon, lat = centroid.x, centroid.y\n",
    "        \n",
    "        # Define patch region (patch_size pixels around centroid)\n",
    "        half_size = (patch_size * scale) / 2\n",
    "        region = ee.Geometry.Rectangle(\n",
    "            [lon - half_size, lat - half_size, lon + half_size, lat + half_size]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Extract patch\n",
    "            patch_array = geemap.ee_to_numpy(\n",
    "                image,\n",
    "                region=region,\n",
    "                scale=scale,\n",
    "                bands=config['bands']\n",
    "            )\n",
    "            \n",
    "            # Check if patch has correct size\n",
    "            if patch_array.shape[1:] == (patch_size, patch_size):\n",
    "                patches.append(patch_array)\n",
    "                labels.append(row[class_field])\n",
    "            \n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"Extracted {idx + 1}/{len(labels_gdf)} patches\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting patch {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nSuccessfully extracted {len(patches)} patches\")\n",
    "    return np.array(patches), np.array(labels)\n",
    "\n",
    "# Extract patches\n",
    "if os.path.exists(labels_path):\n",
    "    patches, patch_labels = extract_patches_from_labels(\n",
    "        s2_composite,\n",
    "        training_labels,\n",
    "        config['patch_size']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nPatches shape: {patches.shape}\")\n",
    "    print(f\"Labels shape: {patch_labels.shape}\")\n",
    "    print(f\"\\nClass distribution in patches:\")\n",
    "    unique, counts = np.unique(patch_labels, return_counts=True)\n",
    "    for cls, count in zip(unique, counts):\n",
    "        print(f\"  {config['classes'][cls]}: {count}\")\n",
    "else:\n",
    "    print(\"Skipping patch extraction - no training labels found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.5: Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_patches(patches):\n",
    "    \"\"\"\n",
    "    Normalize patches using per-band standardization.\n",
    "    \n",
    "    Args:\n",
    "        patches: (N, C, H, W) array\n",
    "    \n",
    "    Returns:\n",
    "        normalized: (N, C, H, W) array\n",
    "        stats: Dictionary with mean and std for each band\n",
    "    \"\"\"\n",
    "    # Compute per-band statistics\n",
    "    mean = patches.mean(axis=(0, 2, 3))  # Average over samples and spatial dims\n",
    "    std = patches.std(axis=(0, 2, 3))\n",
    "    \n",
    "    # Normalize\n",
    "    normalized = (patches - mean[None, :, None, None]) / (std[None, :, None, None] + 1e-8)\n",
    "    \n",
    "    stats = {\n",
    "        'mean': mean.tolist(),\n",
    "        'std': std.tolist()\n",
    "    }\n",
    "    \n",
    "    return normalized, stats\n",
    "\n",
    "if 'patches' in locals():\n",
    "    patches_normalized, normalization_stats = normalize_patches(patches)\n",
    "    \n",
    "    print(\"Normalization statistics:\")\n",
    "    for i, band in enumerate(config['bands']):\n",
    "        print(f\"  {band}: mean={normalization_stats['mean'][i]:.2f}, std={normalization_stats['std'][i]:.2f}\")\n",
    "    \n",
    "    # Save normalization stats\n",
    "    with open('config/normalization_stats.json', 'w') as f:\n",
    "        json.dump(normalization_stats, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.6: Spatial Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_train_test_split(patches, labels, train_ratio=0.7, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data spatially using random assignment.\n",
    "    \n",
    "    For a more sophisticated spatial split, use spatial clustering.\n",
    "    \n",
    "    Args:\n",
    "        patches: (N, C, H, W) array\n",
    "        labels: (N,) array\n",
    "        train_ratio: Fraction for training\n",
    "        random_state: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        train_patches, train_labels, val_patches, val_labels\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Random permutation\n",
    "    indices = np.random.permutation(len(patches))\n",
    "    split_idx = int(len(patches) * train_ratio)\n",
    "    \n",
    "    train_idx = indices[:split_idx]\n",
    "    val_idx = indices[split_idx:]\n",
    "    \n",
    "    return (\n",
    "        patches[train_idx], labels[train_idx],\n",
    "        patches[val_idx], labels[val_idx]\n",
    "    )\n",
    "\n",
    "if 'patches_normalized' in locals():\n",
    "    train_patches, train_labels, val_patches, val_labels = spatial_train_test_split(\n",
    "        patches_normalized,\n",
    "        patch_labels,\n",
    "        train_ratio=config['train_split'],\n",
    "        random_state=SEED\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {len(train_patches)} patches\")\n",
    "    print(f\"Validation set: {len(val_patches)} patches\")\n",
    "    \n",
    "    print(f\"\\nTraining class distribution:\")\n",
    "    unique, counts = np.unique(train_labels, return_counts=True)\n",
    "    for cls, count in zip(unique, counts):\n",
    "        print(f\"  {config['classes'][cls]}: {count} ({100*count/len(train_labels):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nValidation class distribution:\")\n",
    "    unique, counts = np.unique(val_labels, return_counts=True)\n",
    "    for cls, count in zip(unique, counts):\n",
    "        print(f\"  {config['classes'][cls]}: {count} ({100*count/len(val_labels):.1f}%)\")\n",
    "    \n",
    "    # Save processed data\n",
    "    np.save('data/processed/train_patches.npy', train_patches)\n",
    "    np.save('data/processed/train_labels.npy', train_labels)\n",
    "    np.save('data/processed/val_patches.npy', val_patches)\n",
    "    np.save('data/processed/val_labels.npy', val_labels)\n",
    "    \n",
    "    print(\"\\nProcessed data saved to data/processed/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.7: Visualize Example Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_patches(patches, labels, class_names, num_examples=4):\n",
    "    \"\"\"\n",
    "    Visualize example patches for each class.\n",
    "    \"\"\"\n",
    "    num_classes = len(class_names)\n",
    "    fig, axes = plt.subplots(num_classes, num_examples, figsize=(12, 3*num_classes))\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        # Get indices for this class\n",
    "        class_indices = np.where(labels == class_id)[0]\n",
    "        \n",
    "        # Select random examples\n",
    "        if len(class_indices) >= num_examples:\n",
    "            selected = np.random.choice(class_indices, num_examples, replace=False)\n",
    "        else:\n",
    "            selected = class_indices\n",
    "        \n",
    "        for i, idx in enumerate(selected):\n",
    "            patch = patches[idx]\n",
    "            \n",
    "            # Create RGB visualization (bands 2, 1, 0 = R, G, B)\n",
    "            rgb = np.stack([patch[2], patch[1], patch[0]], axis=-1)\n",
    "            \n",
    "            # Normalize to [0, 1] for display\n",
    "            rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-8)\n",
    "            \n",
    "            axes[class_id, i].imshow(rgb)\n",
    "            axes[class_id, i].set_title(f\"{class_names[class_id]}\")\n",
    "            axes[class_id, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/week3_example_patches.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "if 'train_patches' in locals():\n",
    "    visualize_patches(train_patches, train_labels, config['classes'], num_examples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model Architecture\n",
    "\n",
    "### Step 4.1: Define SimpleCNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN for land-cover classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - 3 convolutional blocks (conv → batch norm → ReLU → max pool)\n",
    "    - Global average pooling\n",
    "    - Dropout\n",
    "    - Final classification layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_bands=10, num_classes=4, channels=[32, 64, 128], dropout=0.5):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(num_bands, channels[0], kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels[0])\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Block 2\n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels[1])\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Block 3\n",
    "        self.conv3 = nn.Conv2d(channels[1], channels[2], kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(channels[2])\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Classifier\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(channels[2], num_classes)\n",
    "        \n",
    "        # For storing activations (used in Grad-CAM)\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Block 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        # Store activations for Grad-CAM\n",
    "        if x.requires_grad:\n",
    "            x.register_hook(self.save_gradient)\n",
    "        self.activations = x\n",
    "        \n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def save_gradient(self, grad):\n",
    "        \"\"\"Hook to save gradients for Grad-CAM.\"\"\"\n",
    "        self.gradients = grad\n",
    "    \n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_activations(self):\n",
    "        return self.activations\n",
    "\n",
    "# Instantiate model\n",
    "model = SimpleCNN(\n",
    "    num_bands=config['num_bands'],\n",
    "    num_classes=config['num_classes'],\n",
    "    channels=config['model']['channels'],\n",
    "    dropout=config['model']['dropout']\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "device = torch.device(config['device'])\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nModel moved to: {device}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Test Model with Dummy Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "dummy_input = torch.randn(2, config['num_bands'], config['patch_size'], config['patch_size']).to(device)\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {dummy_output.shape}\")\n",
    "print(f\"Output (logits): {dummy_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Training Setup\n",
    "\n",
    "### Step 5.1: Create PyTorch Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandCoverDataset(Dataset):\n",
    "    \"\"\"PyTorch dataset for land-cover patches.\"\"\"\n",
    "    \n",
    "    def __init__(self, patches, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patches: (N, C, H, W) numpy array\n",
    "            labels: (N,) numpy array\n",
    "        \"\"\"\n",
    "        self.patches = torch.FloatTensor(patches)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.patches[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets\n",
    "if 'train_patches' in locals():\n",
    "    train_dataset = LandCoverDataset(train_patches, train_labels)\n",
    "    val_dataset = LandCoverDataset(val_patches, val_labels)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Test data loader\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"\\nSample batch shapes:\")\n",
    "    print(f\"  Images: {sample_batch[0].shape}\")\n",
    "    print(f\"  Labels: {sample_batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.2: Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "if config['training']['optimizer'] == 'Adam':\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config['training']['learning_rate'],\n",
    "        weight_decay=config['training']['weight_decay']\n",
    "    )\n",
    "elif config['training']['optimizer'] == 'AdamW':\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['training']['learning_rate'],\n",
    "        weight_decay=config['training']['weight_decay']\n",
    "    )\n",
    "else:\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config['training']['learning_rate'],\n",
    "        momentum=0.9,\n",
    "        weight_decay=config['training']['weight_decay']\n",
    "    )\n",
    "\n",
    "# Learning rate scheduler\n",
    "if config['training']['scheduler'] == 'ReduceLROnPlateau':\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=config['training']['scheduler_patience'],\n",
    "        verbose=True\n",
    "    )\n",
    "else:\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=10,\n",
    "        gamma=0.1\n",
    "    )\n",
    "\n",
    "print(f\"Loss function: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "print(f\"Scheduler: {scheduler}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Training Loop\n",
    "\n",
    "### Step 6.1: Define Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc, np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "print(\"Training and validation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.2: Run Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0.0\n",
    "epochs_without_improvement = 0\n",
    "early_stopping_patience = config['training']['early_stopping_patience']\n",
    "\n",
    "# Training loop\n",
    "num_epochs = config['training']['num_epochs']\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'train_loader' in locals():\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, val_preds, val_true = validate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # Update scheduler\n",
    "        if config['training']['scheduler'] == 'ReduceLROnPlateau':\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_acc = val_acc\n",
    "            epochs_without_improvement = 0\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'config': config\n",
    "            }, config['paths']['model_save'])\n",
    "            \n",
    "            print(f\"  → New best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            print(f\"No improvement for {early_stopping_patience} epochs\")\n",
    "            break\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nTraining complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    # Save training history\n",
    "    with open('reports/week3_training_history.json', 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "else:\n",
    "    print(\"Skipping training - no data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.3: Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'history' in locals() and len(history['train_loss']) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "    ax1.plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "    ax2.plot(history['val_acc'], label='Val Accuracy', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/week3_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Training curves saved to figures/week3_training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Model Evaluation\n",
    "\n",
    "### Step 7.1: Load Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model checkpoint\n",
    "if os.path.exists(config['paths']['model_save']):\n",
    "    checkpoint = torch.load(config['paths']['model_save'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "    print(f\"Validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "    print(f\"Validation accuracy: {checkpoint['val_acc']:.2f}%\")\n",
    "else:\n",
    "    print(\"No saved model found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.2: Compute Detailed Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'val_loader' in locals():\n",
    "    # Get predictions on validation set\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=None\n",
    "    )\n",
    "    \n",
    "    # Create metrics dataframe\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Class': [config['classes'][i] for i in range(config['num_classes'])],\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Support': support\n",
    "    })\n",
    "    \n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    print(metrics_df.to_string(index=False))\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_df.to_csv('reports/week3_metrics.csv', index=False)\n",
    "    print(\"\\nMetrics saved to reports/week3_metrics.csv\")\n",
    "    \n",
    "    # Overall metrics\n",
    "    print(f\"\\nOverall Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"Macro-averaged F1: {f1.mean():.3f}\")\n",
    "    print(f\"Weighted-averaged F1: {np.average(f1, weights=support):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.3: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'all_preds' in locals():\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Normalize by row (true labels)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Absolute counts\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "                xticklabels=[config['classes'][i] for i in range(config['num_classes'])],\n",
    "                yticklabels=[config['classes'][i] for i in range(config['num_classes'])])\n",
    "    ax1.set_xlabel('Predicted', fontsize=12)\n",
    "    ax1.set_ylabel('Actual', fontsize=12)\n",
    "    ax1.set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Normalized\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', ax=ax2,\n",
    "                xticklabels=[config['classes'][i] for i in range(config['num_classes'])],\n",
    "                yticklabels=[config['classes'][i] for i in range(config['num_classes'])])\n",
    "    ax2.set_xlabel('Predicted', fontsize=12)\n",
    "    ax2.set_ylabel('Actual', fontsize=12)\n",
    "    ax2.set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/week3_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Confusion matrix saved to figures/week3_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.4: Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, val_dataset, device, num_examples=12):\n",
    "    \"\"\"\n",
    "    Visualize predictions on validation examples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Select random examples\n",
    "    indices = np.random.choice(len(val_dataset), num_examples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            image, true_label = val_dataset[idx]\n",
    "            \n",
    "            # Predict\n",
    "            output = model(image.unsqueeze(0).to(device))\n",
    "            prob = torch.softmax(output, dim=1)\n",
    "            pred_label = output.argmax(1).item()\n",
    "            confidence = prob[0, pred_label].item()\n",
    "            \n",
    "            # Create RGB visualization\n",
    "            rgb = np.stack([image[2], image[1], image[0]], axis=-1).numpy()\n",
    "            rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-8)\n",
    "            \n",
    "            # Plot\n",
    "            axes[i].imshow(rgb)\n",
    "            \n",
    "            # Title with true and predicted labels\n",
    "            true_class = config['classes'][true_label]\n",
    "            pred_class = config['classes'][pred_label]\n",
    "            \n",
    "            if true_label == pred_label:\n",
    "                color = 'green'\n",
    "                title = f\"✓ {pred_class}\\n({confidence:.2f})\"\n",
    "            else:\n",
    "                color = 'red'\n",
    "                title = f\"✗ True: {true_class}\\nPred: {pred_class} ({confidence:.2f})\"\n",
    "            \n",
    "            axes[i].set_title(title, fontsize=10, color=color, fontweight='bold')\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/week3_predictions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "if 'val_dataset' in locals():\n",
    "    visualize_predictions(model, val_dataset, device, num_examples=12)\n",
    "    print(\"Prediction visualizations saved to figures/week3_predictions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Model Interpretation\n",
    "\n",
    "### Step 8.1: Visualize Learned Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conv_filters(model, layer_name='conv1', num_filters=16):\n",
    "    \"\"\"\n",
    "    Visualize convolutional filters from a layer.\n",
    "    \"\"\"\n",
    "    # Get layer\n",
    "    layer = getattr(model, layer_name)\n",
    "    \n",
    "    # Get filter weights\n",
    "    filters = layer.weight.data.cpu().numpy()\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(min(num_filters, filters.shape[0])):\n",
    "        # Average across input channels\n",
    "        filter_img = filters[i].mean(axis=0)\n",
    "        \n",
    "        axes[i].imshow(filter_img, cmap='gray')\n",
    "        axes[i].set_title(f'Filter {i}', fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Learned Filters from {layer_name}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/week3_filters_{layer_name}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize filters from first layer\n",
    "visualize_conv_filters(model, 'conv1', num_filters=16)\n",
    "print(\"Filter visualizations saved to figures/week3_filters_conv1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8.2: Grad-CAM Activation Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gradcam(model, image, target_class, device):\n",
    "    \"\"\"\n",
    "    Generate Grad-CAM activation map.\n",
    "    \n",
    "    Args:\n",
    "        model: CNN model with hooks\n",
    "        image: Input image tensor (1, C, H, W)\n",
    "        target_class: Class to visualize\n",
    "        device: Device\n",
    "    \n",
    "    Returns:\n",
    "        cam: Activation map (H, W)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    image.requires_grad = True\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(image)\n",
    "    \n",
    "    # Zero gradients\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass for target class\n",
    "    output[0, target_class].backward()\n",
    "    \n",
    "    # Get gradients and activations\n",
    "    gradients = model.get_activations_gradient()\n",
    "    activations = model.get_activations()\n",
    "    \n",
    "    # Weight activations by gradients\n",
    "    weights = gradients.mean(dim=(2, 3), keepdim=True)\n",
    "    cam = (weights * activations).sum(dim=1, keepdim=True)\n",
    "    \n",
    "    # Apply ReLU and normalize\n",
    "    cam = torch.relu(cam)\n",
    "    cam = cam / (cam.max() + 1e-8)\n",
    "    \n",
    "    # Resize to input size\n",
    "    cam = torch.nn.functional.interpolate(\n",
    "        cam,\n",
    "        size=(image.shape[2], image.shape[3]),\n",
    "        mode='bilinear',\n",
    "        align_corners=False\n",
    "    )\n",
    "    \n",
    "    return cam[0, 0].detach().cpu().numpy()\n",
    "\n",
    "def visualize_gradcam_examples(model, val_dataset, device, num_examples=12):\n",
    "    \"\"\"\n",
    "    Visualize Grad-CAM for multiple examples.\n",
    "    \"\"\"\n",
    "    # Select examples (3 per class)\n",
    "    indices_per_class = {}\n",
    "    for cls in range(config['num_classes']):\n",
    "        class_indices = np.where(val_dataset.labels.numpy() == cls)[0]\n",
    "        if len(class_indices) >= 3:\n",
    "            indices_per_class[cls] = np.random.choice(class_indices, 3, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 6, figsize=(18, 12))\n",
    "    \n",
    "    for cls in range(config['num_classes']):\n",
    "        if cls not in indices_per_class:\n",
    "            continue\n",
    "        \n",
    "        for i, idx in enumerate(indices_per_class[cls]):\n",
    "            image, label = val_dataset[idx]\n",
    "            \n",
    "            # Generate Grad-CAM\n",
    "            cam = generate_gradcam(model, image, label, device)\n",
    "            \n",
    "            # Create RGB visualization\n",
    "            rgb = np.stack([image[2], image[1], image[0]], axis=-1).numpy()\n",
    "            rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-8)\n",
    "            \n",
    "            # Plot original\n",
    "            axes[cls, i*2].imshow(rgb)\n",
    "            axes[cls, i*2].set_title(f\"{config['classes'][cls]}\", fontsize=10)\n",
    "            axes[cls, i*2].axis('off')\n",
    "            \n",
    "            # Plot with Grad-CAM overlay\n",
    "            axes[cls, i*2+1].imshow(rgb)\n",
    "            axes[cls, i*2+1].imshow(cam, cmap='jet', alpha=0.5)\n",
    "            axes[cls, i*2+1].set_title('Grad-CAM', fontsize=10)\n",
    "            axes[cls, i*2+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/week3_gradcam_examples.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "if 'val_dataset' in locals():\n",
    "    visualize_gradcam_examples(model, val_dataset, device, num_examples=12)\n",
    "    print(\"Grad-CAM visualizations saved to figures/week3_gradcam_examples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Summary and Next Steps\n",
    "\n",
    "### Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"WEEK 3 LAB SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'best_val_acc' in locals():\n",
    "    print(f\"\\n✓ Model Training Complete\")\n",
    "    print(f\"  - Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"  - Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"  - Total Parameters: {total_params:,}\")\n",
    "\n",
    "if 'metrics_df' in locals():\n",
    "    print(f\"\\n✓ Evaluation Metrics Computed\")\n",
    "    print(f\"  - Overall Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"  - Macro F1-Score: {f1.mean():.3f}\")\n",
    "    print(f\"  - Per-class metrics saved to reports/week3_metrics.csv\")\n",
    "\n",
    "print(f\"\\n✓ Visualizations Generated\")\n",
    "print(f\"  - Training curves: figures/week3_training_curves.png\")\n",
    "print(f\"  - Confusion matrix: figures/week3_confusion_matrix.png\")\n",
    "print(f\"  - Predictions: figures/week3_predictions.png\")\n",
    "print(f\"  - Learned filters: figures/week3_filters_conv1.png\")\n",
    "print(f\"  - Grad-CAM: figures/week3_gradcam_examples.png\")\n",
    "\n",
    "print(f\"\\n✓ Model and Configuration Saved\")\n",
    "print(f\"  - Model checkpoint: {config['paths']['model_save']}\")\n",
    "print(f\"  - Configuration: {config['paths']['config_save']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Next Steps:\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. Write model interpretation memo (reports/Week3_Model_Interpretation.md)\")\n",
    "print(\"2. Complete Ethics Thread reflection (reports/Week3_Ethics_Thread.md)\")\n",
    "print(\"3. Write weekly reflection (reports/Week3_Reflection.md)\")\n",
    "print(\"4. Update GitHub repository README\")\n",
    "print(\"5. Commit all materials to GitHub\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've successfully built, trained, and interpreted your first CNN for land-cover classification. This forms the foundation for Week 4's transfer learning and data fusion experiments.\n",
    "\n",
    "**Key Achievements:**\n",
    "- ✅ Prepared geospatial training data with spatial splitting\n",
    "- ✅ Implemented a CNN architecture in PyTorch\n",
    "- ✅ Trained a model with proper monitoring and early stopping\n",
    "- ✅ Evaluated performance with multiple metrics\n",
    "- ✅ Interpreted learned features through visualization\n",
    "- ✅ Documented a reproducible training pipeline\n",
    "\n",
    "**Remember to complete:**\n",
    "1. Model interpretation memo (500-750 words)\n",
    "2. Ethics Thread reflection (450-600 words)\n",
    "3. Weekly reflection (300-400 words)\n",
    "4. GitHub repository update\n",
    "\n",
    "See you in Week 4 for transfer learning and multi-source fusion!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

