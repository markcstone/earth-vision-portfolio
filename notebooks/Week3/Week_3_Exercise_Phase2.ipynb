{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Activity 1: Phase 2 - Batch Dataset Creation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **How to extract patches at scale** - Moving from single test to batch processing\n",
    "2. **Spatial augmentation with jitter** - Creating diversity without synthetic transformations\n",
    "3. **Quality control workflows** - Ensuring dataset integrity through validation\n",
    "4. **Train/validation splitting** - Creating stratified splits that maintain class balance\n",
    "\n",
    "## What is Phase 2?\n",
    "\n",
    "Phase 2 transforms validated extraction parameters into a production-ready dataset. We will:\n",
    "\n",
    "- Load Phase 1 configuration (patch size, parameters)\n",
    "- Extract **375 patches** from 126 polygons (3 patches per polygon with spatial jitter)\n",
    "- Perform comprehensive quality control\n",
    "- Create stratified train/validation split (80/20)\n",
    "- Validate dataset integrity\n",
    "- Generate summary statistics and visualizations\n",
    "\n",
    "**Flow**: Phase 0 (Polygons) â†’ Phase 1 (Validate) â†’ **Phase 2 (Extract)** â†’ Phase 3 (Train CNN)\n",
    "\n",
    "**Expected Time**: 60-90 minutes (including Earth Engine extraction)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Load Configuration\n",
    "\n",
    "We'll start by loading the configuration determined in Phase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import ee\n",
    "import geemap\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "print(\"âœ“ Packages imported\")\n",
    "print(\"âœ“ Random seed set:\", RANDOM_SEED)\n",
    "print(\"âœ“ Earth Engine initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "REPO = Path.cwd().parent\n",
    "DATA = REPO / 'data'\n",
    "POLYGONS_FILE = DATA / 'labels' / 'larger_polygons.geojson'\n",
    "\n",
    "# Load Phase 1 configuration\n",
    "PHASE1_CONFIG_PATH = Path('phase1_config.json')\n",
    "\n",
    "if not PHASE1_CONFIG_PATH.exists():\n",
    "    print(\"âš ï¸  Phase 1 config not found! Using fallback values.\")\n",
    "    # Fallback values if Phase 1 wasn't run\n",
    "    PATCH_SIZE = 8\n",
    "    PATCHES_PER_POLYGON = 3\n",
    "    BANDS = ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']\n",
    "    ASSET_ID = 'users/markstonegobigred/Parcela/s2_2019_median_6b'\n",
    "else:\n",
    "    with open(PHASE1_CONFIG_PATH, 'r') as f:\n",
    "        phase1_config = json.load(f)\n",
    "    PATCH_SIZE = phase1_config['PATCH_SIZE']\n",
    "    PATCHES_PER_POLYGON = phase1_config.get('PATCHES_PER_POLYGON', 3)\n",
    "    BANDS = phase1_config['BANDS']\n",
    "    ASSET_ID = phase1_config['COMPOSITE_ASSET']\n",
    "    print(\"âœ“ Phase 1 configuration loaded\")\n",
    "\n",
    "# Create output directories\n",
    "PHASE2_DIR = Path('phase2_outputs')\n",
    "PATCHES_DIR = PHASE2_DIR / 'patches'\n",
    "METADATA_DIR = PHASE2_DIR / 'metadata'\n",
    "PATCHES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METADATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Patch size: {PATCH_SIZE}Ã—{PATCH_SIZE} pixels ({PATCH_SIZE*10}m)\")\n",
    "print(f\"  Patches per polygon: {PATCHES_PER_POLYGON}\")\n",
    "print(f\"  Bands: {', '.join(BANDS)}\")\n",
    "print(f\"  Output directory: {PHASE2_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Load Polygons and Composite\n",
    "\n",
    "Load our training polygons and Sentinel-2 composite from Phase 0/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training polygons\n",
    "polygons = gpd.read_file(POLYGONS_FILE)\n",
    "if polygons.crs.to_string() != 'EPSG:4326':\n",
    "    polygons = polygons.to_crs('EPSG:4326')\n",
    "\n",
    "print(f\"âœ“ Loaded {len(polygons)} training polygons\")\n",
    "print(f\"  Classes: {sorted(polygons['class_name'].unique())}\")\n",
    "\n",
    "# Class distribution\n",
    "class_counts = polygons['class_name'].value_counts().sort_index()\n",
    "print(f\"\\n  Class distribution:\")\n",
    "for cls, count in class_counts.items():\n",
    "    print(f\"    {cls:12s}: {count:3d} polygons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentinel-2 composite\n",
    "composite = ee.Image(ASSET_ID).select(BANDS)\n",
    "\n",
    "print(f\"âœ“ Composite loaded\")\n",
    "print(f\"  Asset ID: {ASSET_ID}\")\n",
    "print(f\"  Bands: {composite.bandNames().getInfo()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Extraction Strategy with Spatial Jitter\n",
    "\n",
    "### What is spatial jitter?\n",
    "\n",
    "**Jitter** is a small random offset applied when extracting patches. Instead of always extracting from the exact center of a polygon, we shift the extraction location slightly (Â±10m).\n",
    "\n",
    "**Why use jitter?**\n",
    "- **Data augmentation**: Creates 3Ã— more patches from the same polygons\n",
    "- **Reduces overfitting**: Model doesn't learn to expect features at exact pixel locations\n",
    "- **Authentic variation**: Uses real imagery, not synthetic transformations\n",
    "\n",
    "**How it works:**\n",
    "```\n",
    "For each polygon:\n",
    "  Patch 1: Extract from center (jitter = 0, 0)\n",
    "  Patch 2: Extract with +10m offset in random direction\n",
    "  Patch 3: Extract with -8m offset in different direction\n",
    "```\n",
    "\n",
    "Let's calculate extraction locations for all patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate extraction manifest\n",
    "extraction_manifest = []\n",
    "\n",
    "for idx, poly in polygons.iterrows():\n",
    "    centroid = poly.geometry.centroid\n",
    "    lat, lon = centroid.y, centroid.x\n",
    "    \n",
    "    # Calculate meters per degree at this latitude\n",
    "    meters_per_deg_lat = 111320\n",
    "    meters_per_deg_lon = 111320 * np.cos(np.radians(lat))\n",
    "    \n",
    "    for patch_idx in range(PATCHES_PER_POLYGON):\n",
    "        # Generate jitter\n",
    "        if patch_idx == 0:\n",
    "            # First patch: no jitter (center)\n",
    "            offset_m = (0, 0)\n",
    "        else:\n",
    "            # Subsequent patches: random jitter Â±1 pixel (Â±10m)\n",
    "            offset_m = (np.random.uniform(-10, 10), np.random.uniform(-10, 10))\n",
    "        \n",
    "        # Convert offset to degrees\n",
    "        offset_lon = offset_m[0] / meters_per_deg_lon\n",
    "        offset_lat = offset_m[1] / meters_per_deg_lat\n",
    "        \n",
    "        # Calculate patch center with jitter\n",
    "        patch_lon = lon + offset_lon\n",
    "        patch_lat = lat + offset_lat\n",
    "        \n",
    "        # Store in manifest\n",
    "        extraction_manifest.append({\n",
    "            'patch_id': f\"patch_{idx:03d}_{patch_idx}\",\n",
    "            'polygon_id': idx,\n",
    "            'class_name': poly['class_name'],\n",
    "            'class_id': poly['class_id'],\n",
    "            'center_lon': patch_lon,\n",
    "            'center_lat': patch_lat,\n",
    "            'offset_lon_m': offset_m[0],\n",
    "            'offset_lat_m': offset_m[1],\n",
    "            'jitter_idx': patch_idx\n",
    "        })\n",
    "\n",
    "manifest_df = pd.DataFrame(extraction_manifest)\n",
    "\n",
    "print(f\"âœ“ Extraction manifest created\")\n",
    "print(f\"  Total patches planned: {len(manifest_df)}\")\n",
    "print(f\"  Patches per polygon: {PATCHES_PER_POLYGON}\")\n",
    "print(f\"\\n  Patches by class:\")\n",
    "for cls, count in manifest_df['class_name'].value_counts().sort_index().items():\n",
    "    print(f\"    {cls:12s}: {count:3d} patches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize jitter pattern (sample)\n",
    "\n",
    "Let's look at jitter offsets for a few polygons to verify the pattern looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 6 polygons for visualization\n",
    "sample_poly_ids = np.random.choice(polygons.index, size=min(6, len(polygons)), replace=False)\n",
    "sample_patches = manifest_df[manifest_df['polygon_id'].isin(sample_poly_ids)]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, poly_id in enumerate(sample_poly_ids):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Get patches for this polygon\n",
    "    poly_patches = sample_patches[sample_patches['polygon_id'] == poly_id]\n",
    "    poly_row = polygons.loc[poly_id]\n",
    "    \n",
    "    # Plot polygon\n",
    "    poly_geom = poly_row.geometry\n",
    "    if poly_geom.geom_type == 'Polygon':\n",
    "        x, y = poly_geom.exterior.xy\n",
    "        ax.plot(x, y, 'k-', linewidth=2, label='Polygon boundary')\n",
    "    \n",
    "    # Plot patch centers\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    for _, patch in poly_patches.iterrows():\n",
    "        ax.plot(patch['center_lon'], patch['center_lat'], 'o', \n",
    "               color=colors[patch['jitter_idx']], markersize=10,\n",
    "               label=f\"Patch {patch['jitter_idx']}\" if i == 0 else \"\")\n",
    "    \n",
    "    ax.set_title(f\"{poly_row['class_name']} (ID {poly_id})\", fontweight='bold')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    if i == 0:\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Interpretation:\")\n",
    "print(\"   Red = center patch (no jitter)\")\n",
    "print(\"   Blue/Green = jittered patches (Â±10m offset)\")\n",
    "print(\"   All patches should fall well within polygon boundaries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Batch Patch Extraction\n",
    "\n",
    "### The main event!\n",
    "\n",
    "Now we'll extract all patches from Earth Engine. This will take **30-60 minutes** depending on:\n",
    "- Network speed\n",
    "- Earth Engine server load\n",
    "- Number of patches\n",
    "\n",
    "**What's happening:**\n",
    "1. For each patch in the manifest:\n",
    "2. Calculate bounding box from center coordinates\n",
    "3. Extract from Earth Engine composite\n",
    "4. Save as NumPy array (.npy file)\n",
    "5. Log success/failure and quality metrics\n",
    "\n",
    "**Progress tracking:** The progress bar shows real-time status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch extraction function\n",
    "def extract_patch(patch_info, composite, patch_size, bands):\n",
    "    \"\"\"\n",
    "    Extract a single patch from Earth Engine.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success, patch_array, metadata_dict)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lat, lon = patch_info['center_lat'], patch_info['center_lon']\n",
    "        \n",
    "        # Calculate bounding box\n",
    "        patch_half_m = (patch_size * 10) / 2\n",
    "        meters_per_deg_lat = 111320\n",
    "        meters_per_deg_lon = 111320 * np.cos(np.radians(lat))\n",
    "        \n",
    "        half_deg_lon = patch_half_m / meters_per_deg_lon\n",
    "        half_deg_lat = patch_half_m / meters_per_deg_lat\n",
    "        \n",
    "        patch_geom = ee.Geometry.Rectangle([\n",
    "            lon - half_deg_lon,\n",
    "            lat - half_deg_lat,\n",
    "            lon + half_deg_lon,\n",
    "            lat + half_deg_lat\n",
    "        ])\n",
    "        \n",
    "        # Extract from Earth Engine\n",
    "        start_time = datetime.now()\n",
    "        patch = geemap.ee_to_numpy(\n",
    "            composite,\n",
    "            region=patch_geom,\n",
    "            scale=10,\n",
    "            bands=bands\n",
    "        )\n",
    "        extraction_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Handle size mismatch\n",
    "        if patch.shape[:2] != (patch_size, patch_size):\n",
    "            h, w, c = patch.shape\n",
    "            resized = np.full((patch_size, patch_size, len(bands)), np.nan)\n",
    "            h_copy = min(h, patch_size)\n",
    "            w_copy = min(w, patch_size)\n",
    "            resized[:h_copy, :w_copy, :] = patch[:h_copy, :w_copy, :]\n",
    "            patch = resized\n",
    "        \n",
    "        # Calculate quality metrics\n",
    "        nan_pct = np.isnan(patch).sum() / patch.size * 100\n",
    "        valid = patch[~np.isnan(patch)]\n",
    "        \n",
    "        metadata = {\n",
    "            'success': True,\n",
    "            'nan_pct': nan_pct,\n",
    "            'value_min': float(valid.min()) if len(valid) > 0 else np.nan,\n",
    "            'value_max': float(valid.max()) if len(valid) > 0 else np.nan,\n",
    "            'value_mean': float(valid.mean()) if len(valid) > 0 else np.nan,\n",
    "            'extraction_time': extraction_time\n",
    "        }\n",
    "        \n",
    "        return True, patch, metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        metadata = {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'nan_pct': np.nan,\n",
    "            'value_min': np.nan,\n",
    "            'value_max': np.nan,\n",
    "            'value_mean': np.nan,\n",
    "            'extraction_time': np.nan\n",
    "        }\n",
    "        return False, None, metadata\n",
    "\n",
    "print(\"âœ“ Extraction function defined\")\n",
    "print(\"\\nReady to begin batch extraction...\")\n",
    "print(f\"  Total patches: {len(manifest_df)}\")\n",
    "print(f\"  Estimated time: {len(manifest_df) * 0.12 / 60:.0f}-{len(manifest_df) * 0.2 / 60:.0f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch extraction\n",
    "print(\"Starting batch extraction...\\n\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "extraction_log = []\n",
    "successful = 0\n",
    "failed = 0\n",
    "\n",
    "for _, patch_info in tqdm(manifest_df.iterrows(), total=len(manifest_df), desc=\"Extracting patches\"):\n",
    "    patch_id = patch_info['patch_id']\n",
    "    \n",
    "    # Extract patch\n",
    "    success, patch, metadata = extract_patch(patch_info, composite, PATCH_SIZE, BANDS)\n",
    "    \n",
    "    if success:\n",
    "        # Save patch as .npy file\n",
    "        patch_path = PATCHES_DIR / f\"{patch_id}.npy\"\n",
    "        np.save(patch_path, patch.astype(np.float32))\n",
    "        successful += 1\n",
    "    else:\n",
    "        failed += 1\n",
    "    \n",
    "    # Log extraction\n",
    "    log_entry = {\n",
    "        'patch_id': patch_id,\n",
    "        'polygon_id': patch_info['polygon_id'],\n",
    "        'class_name': patch_info['class_name'],\n",
    "        **metadata\n",
    "    }\n",
    "    extraction_log.append(log_entry)\n",
    "\n",
    "total_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "# Create extraction log DataFrame\n",
    "extraction_df = pd.DataFrame(extraction_log)\n",
    "\n",
    "print(f\"\\nâœ“ Batch extraction complete!\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Successful: {successful}/{len(manifest_df)} ({successful/len(manifest_df)*100:.1f}%)\")\n",
    "print(f\"  Failed: {failed}/{len(manifest_df)} ({failed/len(manifest_df)*100:.1f}%)\")\n",
    "print(f\"  Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"  Mean time per patch: {total_time/len(manifest_df):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Quality Control Analysis\n",
    "\n",
    "### Assessing dataset quality\n",
    "\n",
    "Before using these patches for training, we need to verify:\n",
    "1. **NaN percentage** - How much missing data?\n",
    "2. **Value ranges** - Are values reasonable for Sentinel-2?\n",
    "3. **Class distribution** - Did all classes extract successfully?\n",
    "\n",
    "**Quality tiers:**\n",
    "- **Excellent**: 0% NaN\n",
    "- **Good**: 0-10% NaN\n",
    "- **Acceptable**: 10-20% NaN\n",
    "- **Poor**: >20% NaN (should exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality analysis on successful patches\n",
    "successful_patches = extraction_df[extraction_df['success']].copy()\n",
    "\n",
    "# Classify quality tiers\n",
    "def classify_quality(nan_pct):\n",
    "    if nan_pct == 0:\n",
    "        return 'excellent'\n",
    "    elif nan_pct < 10:\n",
    "        return 'good'\n",
    "    elif nan_pct < 20:\n",
    "        return 'acceptable'\n",
    "    else:\n",
    "        return 'poor'\n",
    "\n",
    "successful_patches['quality_tier'] = successful_patches['nan_pct'].apply(classify_quality)\n",
    "\n",
    "# Calculate statistics\n",
    "print(\"Quality Control Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNaN Percentage:\")\n",
    "print(f\"  Mean:   {successful_patches['nan_pct'].mean():.2f}%\")\n",
    "print(f\"  Median: {successful_patches['nan_pct'].median():.2f}%\")\n",
    "print(f\"  Range:  {successful_patches['nan_pct'].min():.2f}% - {successful_patches['nan_pct'].max():.2f}%\")\n",
    "\n",
    "print(f\"\\nQuality Tier Distribution:\")\n",
    "for tier in ['excellent', 'good', 'acceptable', 'poor']:\n",
    "    count = (successful_patches['quality_tier'] == tier).sum()\n",
    "    pct = count / len(successful_patches) * 100\n",
    "    print(f\"  {tier.capitalize():12s}: {count:3d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nValue Ranges:\")\n",
    "print(f\"  Min value: {successful_patches['value_min'].min():.0f}\")\n",
    "print(f\"  Max value: {successful_patches['value_max'].max():.0f}\")\n",
    "print(f\"  (Expected range for Sentinel-2: 0-10000)\")\n",
    "\n",
    "# Class distribution\n",
    "print(f\"\\nSuccessful Patches by Class:\")\n",
    "for cls, count in successful_patches['class_name'].value_counts().sort_index().items():\n",
    "    pct = count / len(successful_patches) * 100\n",
    "    print(f\"  {cls:12s}: {count:3d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create quality visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# NaN distribution histogram\n",
    "ax = axes[0]\n",
    "ax.hist(successful_patches['nan_pct'], bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax.axvline(20, color='red', linestyle='--', linewidth=2, label='Acceptable threshold (20%)')\n",
    "ax.set_xlabel('NaN Percentage (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Number of Patches', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Distribution of Missing Data (NaN)', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Quality tier pie chart\n",
    "ax = axes[1]\n",
    "tier_counts = successful_patches['quality_tier'].value_counts()\n",
    "colors_map = {'excellent': 'green', 'good': 'lightgreen', 'acceptable': 'yellow', 'poor': 'red'}\n",
    "colors = [colors_map[tier] for tier in tier_counts.index]\n",
    "ax.pie(tier_counts.values, labels=tier_counts.index, autopct='%1.1f%%', \n",
    "       colors=colors, startangle=90)\n",
    "ax.set_title('Quality Tier Distribution', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Interpretation:\")\n",
    "print(\"   Left: Most patches should have 0% NaN (excellent quality)\")\n",
    "print(\"   Right: Green (excellent/good) should dominate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Train/Validation Split\n",
    "\n",
    "### Why stratified splitting?\n",
    "\n",
    "A **stratified split** maintains the same class proportions in both training and validation sets.\n",
    "\n",
    "**Example:**\n",
    "- If Agriculture is 28% of total dataset\n",
    "- Then Agriculture will be 28% of training set AND 28% of validation set\n",
    "\n",
    "**Why this matters:**\n",
    "- Fair evaluation (validation set represents all classes)\n",
    "- Prevents bias from imbalanced validation\n",
    "- Standard practice in machine learning\n",
    "\n",
    "**Split ratio:** 80% training, 20% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to high-quality patches (excellent or good)\n",
    "high_quality = successful_patches[successful_patches['quality_tier'].isin(['excellent', 'good'])].copy()\n",
    "\n",
    "print(f\"Using high-quality patches for splitting:\")\n",
    "print(f\"  Total: {len(high_quality)} ({len(high_quality)/len(successful_patches)*100:.1f}% of successful)\")\n",
    "print(f\"  Excluding: {len(successful_patches) - len(high_quality)} low-quality patches\")\n",
    "\n",
    "# Perform stratified split\n",
    "train_df, val_df = train_test_split(\n",
    "    high_quality,\n",
    "    test_size=0.2,\n",
    "    stratify=high_quality['class_name'],\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Stratified split complete:\")\n",
    "print(f\"  Training: {len(train_df)} patches (80%)\")\n",
    "print(f\"  Validation: {len(val_df)} patches (20%)\")\n",
    "\n",
    "# Verify stratification\n",
    "print(f\"\\nClass Distribution Verification:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Class':<12s} {'Total':>8s} {'Train':>8s} {'Val':>8s} {'Train%':>8s} {'Val%':>8s}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for cls in sorted(high_quality['class_name'].unique()):\n",
    "    total = (high_quality['class_name'] == cls).sum()\n",
    "    train = (train_df['class_name'] == cls).sum()\n",
    "    val = (val_df['class_name'] == cls).sum()\n",
    "    train_pct = train / len(train_df) * 100\n",
    "    val_pct = val / len(val_df) * 100\n",
    "    print(f\"{cls:<12s} {total:8d} {train:8d} {val:8d} {train_pct:7.1f}% {val_pct:7.1f}%\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'TOTAL':<12s} {len(high_quality):8d} {len(train_df):8d} {len(val_df):8d} {100.0:7.1f}% {100.0:7.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save splits to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train/val splits\n",
    "train_df.to_csv(METADATA_DIR / 'train_split.csv', index=False)\n",
    "val_df.to_csv(METADATA_DIR / 'val_split.csv', index=False)\n",
    "\n",
    "# Save split metadata\n",
    "split_metadata = {\n",
    "    'split_method': 'stratified',\n",
    "    'train_ratio': 0.8,\n",
    "    'val_ratio': 0.2,\n",
    "    'random_seed': RANDOM_SEED,\n",
    "    'train_count': len(train_df),\n",
    "    'val_count': len(val_df),\n",
    "    'quality_filter': ['excellent', 'good'],\n",
    "    'class_distribution': {\n",
    "        cls: {\n",
    "            'train': int((train_df['class_name'] == cls).sum()),\n",
    "            'val': int((val_df['class_name'] == cls).sum())\n",
    "        }\n",
    "        for cls in sorted(high_quality['class_name'].unique())\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(METADATA_DIR / 'split_metadata.json', 'w') as f:\n",
    "    json.dump(split_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Split files saved:\")\n",
    "print(f\"  {METADATA_DIR / 'train_split.csv'}\")\n",
    "print(f\"  {METADATA_DIR / 'val_split.csv'}\")\n",
    "print(f\"  {METADATA_DIR / 'split_metadata.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Dataset Validation\n",
    "\n",
    "### Final integrity checks\n",
    "\n",
    "Before declaring the dataset ready, let's verify:\n",
    "1. All patch files exist and are readable\n",
    "2. Shapes are consistent (8Ã—8Ã—6)\n",
    "3. No overlap between train and validation\n",
    "4. Minimum samples per class met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validation checks\n",
    "print(\"Running Dataset Validation Checks...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "checks_passed = 0\n",
    "checks_total = 0\n",
    "\n",
    "# Check 1: All files exist\n",
    "checks_total += 1\n",
    "missing_files = []\n",
    "for patch_id in high_quality['patch_id']:\n",
    "    if not (PATCHES_DIR / f\"{patch_id}.npy\").exists():\n",
    "        missing_files.append(patch_id)\n",
    "\n",
    "if len(missing_files) == 0:\n",
    "    print(\"âœ… Check 1: All patch files exist\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"âŒ Check 1: {len(missing_files)} patch files missing\")\n",
    "\n",
    "# Check 2: All files readable and correct shape\n",
    "checks_total += 1\n",
    "shape_errors = []\n",
    "for patch_id in high_quality['patch_id'][:10]:  # Sample check\n",
    "    try:\n",
    "        patch = np.load(PATCHES_DIR / f\"{patch_id}.npy\")\n",
    "        if patch.shape != (PATCH_SIZE, PATCH_SIZE, len(BANDS)):\n",
    "            shape_errors.append(patch_id)\n",
    "    except:\n",
    "        shape_errors.append(patch_id)\n",
    "\n",
    "if len(shape_errors) == 0:\n",
    "    print(f\"âœ… Check 2: All patches have correct shape ({PATCH_SIZE}Ã—{PATCH_SIZE}Ã—{len(BANDS)})\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"âŒ Check 2: {len(shape_errors)} patches have incorrect shape\")\n",
    "\n",
    "# Check 3: No overlap between train and validation\n",
    "checks_total += 1\n",
    "overlap = set(train_df['patch_id']) & set(val_df['patch_id'])\n",
    "if len(overlap) == 0:\n",
    "    print(\"âœ… Check 3: No overlap between train and validation sets\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"âŒ Check 3: {len(overlap)} patches appear in both splits\")\n",
    "\n",
    "# Check 4: Minimum samples per class (train)\n",
    "checks_total += 1\n",
    "min_train_samples = 5\n",
    "train_class_counts = train_df['class_name'].value_counts()\n",
    "insufficient_train = train_class_counts[train_class_counts < min_train_samples]\n",
    "if len(insufficient_train) == 0:\n",
    "    print(f\"âœ… Check 4: All classes have â‰¥ {min_train_samples} training samples\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"âŒ Check 4: {len(insufficient_train)} classes have < {min_train_samples} training samples\")\n",
    "\n",
    "# Check 5: Minimum samples per class (val)\n",
    "checks_total += 1\n",
    "min_val_samples = 2\n",
    "val_class_counts = val_df['class_name'].value_counts()\n",
    "insufficient_val = val_class_counts[val_class_counts < min_val_samples]\n",
    "if len(insufficient_val) == 0:\n",
    "    print(f\"âœ… Check 5: All classes have â‰¥ {min_val_samples} validation samples\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"âŒ Check 5: {len(insufficient_val)} classes have < {min_val_samples} validation samples\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nValidation Summary: {checks_passed}/{checks_total} checks passed\")\n",
    "\n",
    "if checks_passed == checks_total:\n",
    "    print(\"\\nâœ… DATASET VALIDATED - Ready for CNN training!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Some validation checks failed - review above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Summary & Next Steps\n",
    "\n",
    "### What we accomplished in Phase 2\n",
    "\n",
    "âœ… **Extracted patches at scale** - Moved from single test to batch processing\n",
    "\n",
    "âœ… **Applied spatial augmentation** - 3 patches per polygon with jitter\n",
    "\n",
    "âœ… **Performed quality control** - Classified all patches by quality tier\n",
    "\n",
    "âœ… **Created stratified split** - 80/20 train/val with balanced classes\n",
    "\n",
    "âœ… **Validated dataset integrity** - All checks passed\n",
    "\n",
    "### Final Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile final statistics\n",
    "final_stats = pd.DataFrame([\n",
    "    ['Total polygons', len(polygons)],\n",
    "    ['Patches attempted', len(manifest_df)],\n",
    "    ['Successful extractions', successful],\n",
    "    ['Success rate', f\"{successful/len(manifest_df)*100:.1f}%\"],\n",
    "    ['High-quality patches', len(high_quality)],\n",
    "    ['Training patches', len(train_df)],\n",
    "    ['Validation patches', len(val_df)],\n",
    "    ['Patch size', f\"{PATCH_SIZE}Ã—{PATCH_SIZE} pixels ({PATCH_SIZE*10}m)\"],\n",
    "    ['Number of bands', len(BANDS)],\n",
    "    ['Number of classes', len(high_quality['class_name'].unique())],\n",
    "    ['Mean NaN percentage', f\"{successful_patches['nan_pct'].mean():.2f}%\"],\n",
    "    ['Excellent quality', f\"{(successful_patches['quality_tier']=='excellent').sum()} patches\"],\n",
    "    ['Total extraction time', f\"{total_time/60:.1f} minutes\"],\n",
    "    ['Random seed', RANDOM_SEED]\n",
    "], columns=['Metric', 'Value'])\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2 COMPLETE: Final Dataset Statistics\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(final_stats.to_string(index=False))\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "**1. Spatial jitter creates authentic diversity**\n",
    "- 3Ã— more data from the same polygons\n",
    "- No synthetic transformations needed\n",
    "- Model learns spatial invariance naturally\n",
    "\n",
    "**2. Quality control ensures clean training**\n",
    "- Filtering by NaN percentage removes problematic patches\n",
    "- Excellent/good quality patches have reliable spectral signatures\n",
    "- Bad data = bad model performance\n",
    "\n",
    "**3. Stratified splitting prevents bias**\n",
    "- Both splits represent all classes proportionally\n",
    "- Validation results will be representative\n",
    "- Standard ML best practice\n",
    "\n",
    "**4. Validation gives confidence**\n",
    "- Systematic checks catch issues early\n",
    "- Know your data is ready before training\n",
    "- Saves debugging time later\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps: CNN Training (Phase 3)\n",
    "\n",
    "Now that you have a clean, validated dataset, you're ready for:\n",
    "\n",
    "**Week 3 Lab: CNN Training**\n",
    "```python\n",
    "# Loading your data is now simple:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_split = pd.read_csv('phase2_outputs/metadata/train_split.csv')\n",
    "val_split = pd.read_csv('phase2_outputs/metadata/val_split.csv')\n",
    "\n",
    "X_train = np.array([np.load(f\"phase2_outputs/patches/{pid}.npy\") \n",
    "                     for pid in train_split['patch_id']])\n",
    "y_train = train_split['class_id'].values\n",
    "\n",
    "# Ready for model.fit()!\n",
    "```\n",
    "\n",
    "**Phase 3 Workflow:**\n",
    "1. Build CNN architecture (Conv2D, MaxPooling, Dense layers)\n",
    "2. Train model on training set\n",
    "3. Evaluate on validation set\n",
    "4. Analyze results (confusion matrix, per-class accuracy)\n",
    "5. Visualize learned features\n",
    "6. Apply to full study area\n",
    "\n",
    "---\n",
    "\n",
    "### Congratulations! ðŸŽ‰\n",
    "\n",
    "You've successfully completed Phase 2 and created a production-ready dataset for CNN training.\n",
    "\n",
    "**The key insight from Phase 2:**\n",
    "\n",
    "**Quality over quantity - A small, clean dataset beats a large, messy one!**\n",
    "\n",
    "This approach:\n",
    "- âœ… Ensures high-quality training data\n",
    "- âœ… Prevents common pitfalls (data leakage, class imbalance)\n",
    "- âœ… Provides complete documentation and traceability\n",
    "- âœ… Sets you up for CNN training success\n",
    "\n",
    "**Ready for Phase 3: CNN Training!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
