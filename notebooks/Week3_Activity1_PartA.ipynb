{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 3 Activity 1 â€” Data Preparation and Exploration for CNN Training\n",
        "\n",
        "**Case Study:** Los Lagos ParcelizaciÃ³n  \n",
        "**Objective:** Prepare a high-quality training dataset for land-cover classification  \n",
        "**Duration:** 90 minutes  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“‹ Activity Overview\n",
        "\n",
        "This notebook guides you through the critical process of preparing geospatial training data for deep learning. You'll learn that **data quality matters more than model sophistication** â€” a well-prepared dataset with thoughtful labels and proper spatial splitting will outperform a fancy architecture trained on poor data.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "By the end of this activity, you will:\n",
        "1. Define spectrally and spatially distinct land-cover classes\n",
        "2. Create training labels in QGIS with proper attribute structure\n",
        "3. Extract and store multispectral patches from Sentinel-2 imagery\n",
        "4. Analyze spectral signatures to understand class separability\n",
        "5. Implement spatial train/validation splitting (critical!)\n",
        "6. Understand tradeoffs between storage formats and band selection\n",
        "\n",
        "### Workflow Sections\n",
        "\n",
        "**Part A**\n",
        "\n",
        "1. **Setup and Initialization** â€” Environment, paths, reproducibility\n",
        "2. **Class Definition** â€” Document your 5 land-cover classes\n",
        "3. **QGIS Digitization Guide** â€” Attribute table setup and workflow\n",
        "4. **Load Training Polygons** â€” Import and validate labels\n",
        "5. **Sentinel-2 Data Access** â€” Connect to Earth Engine, select optimal bands\n",
        "\n",
        "**Part B**\n",
        "\n",
        "6. **Patch Extraction** â€” Extract training patches with multiple approaches\n",
        "7. **Spectral Analysis** â€” Visualize and compare class signatures\n",
        "8. **Spatial Train/Val Split** â€” Implement proper spatial separation\n",
        "9. **Storage Format Comparison** â€” NumPy vs GeoTIFF tradeoffs\n",
        "10. **Experiment Logging** â€” Document your data preparation decisions\n",
        "11. **Self-Assessment** â€” Evaluate data quality and readiness\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ Critical Concept: Spatial Autocorrelation\n",
        "\n",
        "**The most common mistake in geospatial ML**: Random train/test splitting.\n",
        "\n",
        "Nearby pixels are similar (Tobler's First Law). If you randomly split patches, nearby patches end up in both train and test sets. Your model achieves high test accuracy by memorizing locations, not learning generalizable features.\n",
        "\n",
        "**Solution**: Spatially separate train and test data. We'll explore this thoroughly in Section 8.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Setup and Initialization\n",
        "\n",
        "**Objective:**  \n",
        "Initialize the notebook environment with all necessary libraries, establish reproducible random seeds, configure file paths, and create the project directory structure.\n",
        "\n",
        "**Key Components:**\n",
        "- **Geospatial libraries**: `geopandas`, `earthengine-api`, `geemap`, `rasterio`\n",
        "- **ML/Data libraries**: `numpy`, `torch`, `scikit-learn`\n",
        "- **Visualization**: `matplotlib`, `plotly` (interactive)\n",
        "- **Reproducibility**: Fixed random seeds for Python, NumPy, and PyTorch\n",
        "- **Path management**: Automatic repository root detection\n",
        "- **Experiment logging**: Basic logging system initialization\n",
        "\n",
        "**Why this matters:**  \n",
        "Proper setup ensures reproducible results and clean project organization. The experiment log will track all your data preparation decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Setup and Initialization\n",
        "\n",
        "# === Core Python utilities ===\n",
        "from pathlib import Path\n",
        "import os, json, random, warnings\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# === Geospatial libraries ===\n",
        "import geopandas as gpd              # Vector data handling\n",
        "import rasterio                       # Raster I/O for GeoTIFFs\n",
        "from rasterio.features import rasterize\n",
        "from shapely.geometry import box, Point\n",
        "import ee                             # Earth Engine Python API\n",
        "import geemap                         # Earth Engine visualization\n",
        "\n",
        "# === Machine Learning libraries ===\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# === Visualization libraries ===\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# === Reproducibility: Set all random seeds ===\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "print(f'âœ“ Random seed set to {RANDOM_SEED} for reproducibility')\n",
        "\n",
        "# === Repository Path Management ===\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    \"\"\"\n",
        "    Locate repository root by searching upward for 'data' and 'figures' directories.\n",
        "    This allows notebooks to run from any subdirectory.\n",
        "    \"\"\"\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / 'data').exists() and (p / 'figures').exists():\n",
        "            return p\n",
        "    return start  # Fallback to current directory\n",
        "\n",
        "# Resolve key paths\n",
        "CWD = Path.cwd()\n",
        "REPO = find_repo_root(CWD)\n",
        "DATA = REPO / 'data'\n",
        "FIGS = REPO / 'figures'\n",
        "REPORTS = REPO / 'reports'\n",
        "MODELS = REPO / 'models'\n",
        "NOTEBOOKS = REPO / 'notebooks'\n",
        "\n",
        "# Data subdirectories\n",
        "DATA_EXTERNAL = DATA / 'external'      # External data (AOI, reference data)\n",
        "DATA_RAW = DATA / 'raw'                # Raw downloaded imagery\n",
        "DATA_PROCESSED = DATA / 'processed'    # Processed patches and arrays\n",
        "DATA_LABELS = DATA / 'labels'          # Training labels (GeoJSON)\n",
        "\n",
        "# Create all necessary directories\n",
        "for directory in [FIGS, REPORTS, MODELS, NOTEBOOKS, \n",
        "                  DATA_EXTERNAL, DATA_RAW, DATA_PROCESSED, DATA_LABELS]:\n",
        "    directory.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Define key file paths\n",
        "AOI_PATH = DATA_EXTERNAL / 'aoi.geojson'\n",
        "TRAINING_POLYGONS_PATH = DATA_LABELS / 'training_polygons.geojson'\n",
        "\n",
        "# === Initialize Experiment Log ===\n",
        "\n",
        "# Create a simple experiment log to track data preparation decisions\n",
        "experiment_log = {\n",
        "    'activity': 'Week 3 Activity 1 - Data Preparation',\n",
        "    'case_study': 'Los Lagos ParcelizaciÃ³n',\n",
        "    'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'random_seed': RANDOM_SEED,\n",
        "    'classes': ['Forest', 'Agriculture', 'Parcels', 'Water', 'Urban'],\n",
        "    'decisions': {},  # Will be populated throughout notebook\n",
        "    'metrics': {}     # Will store data quality metrics\n",
        "}\n",
        "\n",
        "print('\\nðŸ“ Project Structure:')\n",
        "print(f'  Repository root: {REPO}')\n",
        "print(f'  Data directory: {DATA}')\n",
        "print(f'  Figures directory: {FIGS}')\n",
        "print(f'  Reports directory: {REPORTS}')\n",
        "print(f'\\nðŸ“ Key Files:')\n",
        "print(f'  AOI: {AOI_PATH}')\n",
        "print(f'  AOI exists: {AOI_PATH.exists()}')\n",
        "print(f'  Training polygons: {TRAINING_POLYGONS_PATH}')\n",
        "print(f'  Training polygons exist: {TRAINING_POLYGONS_PATH.exists()}')\n",
        "print(f'\\nâœ“ Setup complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outcome:**\n",
        "\n",
        "After running this cell, you should see:\n",
        "- âœ“ Random seed confirmation\n",
        "- ðŸ“ Project structure with all paths\n",
        "- ðŸ“ AOI file status (should show `True`)\n",
        "- ðŸ“ Training polygons status (will be `False` until you create them in QGIS)\n",
        "\n",
        "The `experiment_log` dictionary will track all your decisions throughout this activity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 2. Define Land-Cover Classes\n",
        "\n",
        "**Objective:**  \n",
        "Document clear, unambiguous definitions for each of your 5 land-cover classes. This is critical for consistent labeling and model performance.\n",
        "\n",
        "**Why this matters:**  \n",
        "Ambiguous class definitions lead to inconsistent labels, which confuse the model and reduce accuracy. Clear definitions ensure you (and others) label consistently.\n",
        "\n",
        "**Key Principles:**\n",
        "1. **Spectral distinctiveness**: Classes should have different spectral signatures\n",
        "2. **Spatial distinctiveness**: Classes should have different spatial patterns/textures\n",
        "3. **Practical relevance**: Classes should align with your research questions\n",
        "4. **Label feasibility**: You should be able to reliably identify each class in imagery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Define Land-Cover Classes\n",
        "\n",
        "# === Class Definitions for Los Lagos ParcelizaciÃ³n ===\n",
        "\n",
        "class_definitions = {\n",
        "    'Forest': {\n",
        "        'id': 0,\n",
        "        'color': '#228B22',  # Forest green\n",
        "        'description': 'Native and plantation forests with dense canopy cover',\n",
        "        'spectral_characteristics': [\n",
        "            'High NIR reflectance (vegetation)',\n",
        "            'Low red reflectance (chlorophyll absorption)',\n",
        "            'High NDVI (>0.6)',\n",
        "            'Moderate SWIR reflectance'\n",
        "        ],\n",
        "        'spatial_characteristics': [\n",
        "            'Continuous canopy texture',\n",
        "            'Irregular boundaries (native) or regular (plantation)',\n",
        "            'Large contiguous patches'\n",
        "        ],\n",
        "        'examples': [\n",
        "            'Temperate rainforest',\n",
        "            'Eucalyptus plantations',\n",
        "            'Mixed native forest'\n",
        "        ],\n",
        "        'exclusions': [\n",
        "            'Sparse trees in agricultural areas',\n",
        "            'Recently cleared forest (classify as Bare Soil or Parcels)',\n",
        "            'Shrubland with <30% canopy cover'\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    'Agriculture': {\n",
        "        'id': 1,\n",
        "        'color': '#FFD700',  # Gold\n",
        "        'description': 'Active agricultural fields including crops and pasture',\n",
        "        'spectral_characteristics': [\n",
        "            'Variable NDVI (0.3-0.7) depending on crop stage',\n",
        "            'Lower NIR than forest',\n",
        "            'Seasonal variability in all bands'\n",
        "        ],\n",
        "        'spatial_characteristics': [\n",
        "            'Regular field boundaries',\n",
        "            'Rectangular or geometric shapes',\n",
        "            'Uniform texture within fields',\n",
        "            'Medium patch sizes'\n",
        "        ],\n",
        "        'examples': [\n",
        "            'Wheat fields',\n",
        "            'Potato fields',\n",
        "            'Managed pasture',\n",
        "            'Hay fields'\n",
        "        ],\n",
        "        'exclusions': [\n",
        "            'Fallow fields (if bare, classify as Bare Soil)',\n",
        "            'Recently harvested fields',\n",
        "            'Residential gardens (classify as Parcels)'\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    'Parcels': {\n",
        "        'id': 2,\n",
        "        'color': '#FF6347',  # Tomato red\n",
        "        'description': 'Subdivided residential areas (parcelizaciÃ³n) â€” the key phenomenon of interest',\n",
        "        'spectral_characteristics': [\n",
        "            'Mixed spectral signature (buildings + vegetation + bare soil)',\n",
        "            'Moderate NDVI (0.2-0.5) due to mixed cover',\n",
        "            'High spatial heterogeneity',\n",
        "            'Often includes bright surfaces (roofs, roads)'\n",
        "        ],\n",
        "        'spatial_characteristics': [\n",
        "            'Small, regular subdivisions (typically <1 hectare)',\n",
        "            'Grid pattern or linear arrangement along roads',\n",
        "            'Mix of structures and vegetation',\n",
        "            'Access roads visible',\n",
        "            'Often at forest-agriculture interface'\n",
        "        ],\n",
        "        'examples': [\n",
        "            'Residential subdivisions',\n",
        "            'Rural housing developments',\n",
        "            'Parceled land with structures'\n",
        "        ],\n",
        "        'exclusions': [\n",
        "            'Traditional rural homesteads (isolated houses)',\n",
        "            'Urban areas (classify as Urban)',\n",
        "            'Agricultural buildings without subdivision pattern'\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    'Water': {\n",
        "        'id': 3,\n",
        "        'color': '#1E90FF',  # Dodger blue\n",
        "        'description': 'Water bodies including lakes, rivers, and coastal areas',\n",
        "        'spectral_characteristics': [\n",
        "            'Very low NIR reflectance (water absorbs NIR)',\n",
        "            'Negative NDVI',\n",
        "            'High NDWI (>0.3)',\n",
        "            'Low reflectance in all bands (clear water)',\n",
        "            'Higher visible reflectance if turbid'\n",
        "        ],\n",
        "        'spatial_characteristics': [\n",
        "            'Smooth, homogeneous texture',\n",
        "            'Irregular boundaries (natural) or regular (reservoirs)',\n",
        "            'Low spatial variability'\n",
        "        ],\n",
        "        'examples': [\n",
        "            'Lakes',\n",
        "            'Rivers',\n",
        "            'Coastal waters',\n",
        "            'Reservoirs'\n",
        "        ],\n",
        "        'exclusions': [\n",
        "            'Shadows (can have similar spectral signature)',\n",
        "            'Wetlands with emergent vegetation',\n",
        "            'Temporary flooding'\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    'Urban': {\n",
        "        'id': 4,\n",
        "        'color': '#808080',  # Gray\n",
        "        'description': 'Established urban and built-up areas',\n",
        "        'spectral_characteristics': [\n",
        "            'High reflectance in visible bands (concrete, asphalt)',\n",
        "            'Low NDVI (<0.2)',\n",
        "            'High NDBI (built-up index)',\n",
        "            'High spatial heterogeneity'\n",
        "        ],\n",
        "        'spatial_characteristics': [\n",
        "            'Dense building patterns',\n",
        "            'Road networks',\n",
        "            'Large contiguous built-up areas',\n",
        "            'Mix of bright (roofs) and dark (roads) surfaces'\n",
        "        ],\n",
        "        'examples': [\n",
        "            'Town centers',\n",
        "            'Industrial areas',\n",
        "            'Commercial districts',\n",
        "            'Dense residential areas'\n",
        "        ],\n",
        "        'exclusions': [\n",
        "            'Parcels (classify separately)',\n",
        "            'Isolated rural buildings',\n",
        "            'Agricultural infrastructure'\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Store in experiment log\n",
        "experiment_log['class_definitions'] = class_definitions\n",
        "\n",
        "# Display class summary\n",
        "print('ðŸ“Š Land-Cover Classes for Los Lagos ParcelizaciÃ³n\\n')\n",
        "print('=' * 80)\n",
        "for class_name, info in class_definitions.items():\n",
        "    print(f\"\\n{info['id']}. {class_name.upper()}\")\n",
        "    print(f\"   Color: {info['color']}\")\n",
        "    print(f\"   {info['description']}\")\n",
        "    print(f\"   Key spectral: {', '.join(info['spectral_characteristics'][:2])}\")\n",
        "    print(f\"   Key spatial: {info['spatial_characteristics'][0]}\")\n",
        "\n",
        "print('\\n' + '=' * 80)\n",
        "print('\\nâœ“ Class definitions documented')\n",
        "print('\\nðŸ’¡ TIP: Keep these definitions handy while digitizing in QGIS!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outcome:**\n",
        "\n",
        "You now have clear, documented definitions for all 5 classes. Notice:\n",
        "\n",
        "- **Spectral distinctiveness**: Water has negative NDVI, Forest has high NDVI, Urban has low NDVI\n",
        "- **Spatial distinctiveness**: Parcels have small regular subdivisions, Agriculture has larger geometric fields\n",
        "- **Explicit exclusions**: Helps avoid ambiguous cases (e.g., \"Is this a Parcel or Urban?\")\n",
        "\n",
        "**ðŸ¤” Reflection Question 1:**\n",
        "\n",
        "*Which two classes do you think will be most difficult to distinguish? Why? Consider both spectral and spatial characteristics.*\n",
        "\n",
        "[Write your answer here before proceeding]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 3. QGIS Digitization Guide\n",
        "\n",
        "**Objective:**  \n",
        "Provide step-by-step instructions for creating training polygons in QGIS with proper attribute structure for seamless import into this notebook.\n",
        "\n",
        "**Why QGIS:**  \n",
        "QGIS provides visual context (high-resolution imagery, Sentinel-2 composites) and precise digitization tools. Manual digitization ensures high-quality labels.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—ºï¸ QGIS Workflow\n",
        "\n",
        "### Step 1: Load Base Layers\n",
        "\n",
        "1. **Open QGIS** and create a new project\n",
        "2. **Load your AOI**: `Layer â†’ Add Layer â†’ Add Vector Layer` â†’ Select `data/external/aoi.geojson`\n",
        "3. **Add Google Satellite** (for reference):\n",
        "   - `Browser Panel â†’ XYZ Tiles â†’ Google Satellite` (drag to map)\n",
        "   - Or: `Web â†’ QuickMapServices â†’ Google â†’ Google Satellite`\n",
        "4. **Add Sentinel-2 via GEE Plugin**:\n",
        "   - `Plugins â†’ Python Console`\n",
        "   - Or use the GEE plugin to add a Sentinel-2 median composite for your AOI\n",
        "\n",
        "### Step 2: Create Training Polygon Layer\n",
        "\n",
        "1. **Create new GeoJSON layer**:\n",
        "   - `Layer â†’ Create Layer â†’ New GeoJSON Layer`\n",
        "   - **File name**: `data/labels/training_polygons.geojson`\n",
        "   - **Geometry type**: Polygon\n",
        "   - **CRS**: EPSG:4326 (WGS 84)\n",
        "\n",
        "2. **Add attribute fields** (CRITICAL for seamless import):\n",
        "\n",
        "   Click \"New Field\" for each of the following:\n",
        "\n",
        "   | Field Name | Type | Length | Description |\n",
        "   |------------|------|--------|-------------|\n",
        "   | **class_name** | Text | 50 | Land-cover class (Forest, Agriculture, Parcels, Water, Urban) |\n",
        "   | **class_id** | Integer | 10 | Numeric class ID (0-4) |\n",
        "   | **confidence** | Text | 20 | Your confidence (High, Medium, Low) |\n",
        "   | **notes** | Text | 200 | Any observations or uncertainties |\n",
        "   | **date_digitized** | Text | 20 | Date you created this polygon |\n",
        "\n",
        "3. **Click OK** to create the layer\n",
        "\n",
        "### Step 3: Configure Attribute Form (Optional but Recommended)\n",
        "\n",
        "This makes digitization faster and prevents typos:\n",
        "\n",
        "1. **Right-click layer** â†’ `Properties â†’ Attributes Form`\n",
        "2. **For class_name field**:\n",
        "   - Widget Type: `Value Map`\n",
        "   - Add values: `Forest`, `Agriculture`, `Parcels`, `Water`, `Urban`\n",
        "3. **For class_id field**:\n",
        "   - Widget Type: `Value Map`\n",
        "   - Add values: `0`, `1`, `2`, `3`, `4`\n",
        "4. **For confidence field**:\n",
        "   - Widget Type: `Value Map`\n",
        "   - Add values: `High`, `Medium`, `Low`\n",
        "5. **Click OK**\n",
        "\n",
        "Now when you digitize, you'll get dropdown menus instead of typing!\n",
        "\n",
        "### Step 4: Digitization Strategy\n",
        "\n",
        "**Goal**: 50-100 polygons per class, distributed across your AOI\n",
        "\n",
        "**Best Practices**:\n",
        "\n",
        "1. **Spatial distribution**: \n",
        "   - Don't cluster all polygons in one area\n",
        "   - Sample from north, south, east, west, and center of AOI\n",
        "   - Include different elevations/aspects if relevant\n",
        "\n",
        "2. **Polygon size**:\n",
        "   - **Minimum**: ~200m Ã— 200m (20 pixels Ã— 20 pixels at 10m resolution)\n",
        "   - **Ideal**: 500m Ã— 500m to 1km Ã— 1km\n",
        "   - Larger polygons = more training patches per polygon\n",
        "\n",
        "3. **Polygon purity**:\n",
        "   - Avoid mixed pixels at boundaries\n",
        "   - Draw polygons well inside homogeneous areas\n",
        "   - Leave buffer from class boundaries\n",
        "\n",
        "4. **Class balance**:\n",
        "   - Aim for roughly equal numbers of polygons per class\n",
        "   - If one class is rare, digitize smaller but more numerous polygons\n",
        "\n",
        "5. **Quality over quantity**:\n",
        "   - Better to have 50 high-quality polygons than 100 ambiguous ones\n",
        "   - Mark uncertain areas as `confidence: Low` or skip them\n",
        "\n",
        "### Step 5: Digitize Polygons\n",
        "\n",
        "1. **Enable editing**: Click the pencil icon or `Toggle Editing`\n",
        "2. **Add polygon**: Click `Add Polygon Feature` icon\n",
        "3. **Draw polygon**: Click to add vertices, right-click to finish\n",
        "4. **Fill attributes**:\n",
        "   - **class_name**: Select from dropdown (e.g., \"Forest\")\n",
        "   - **class_id**: Select corresponding ID (e.g., 0 for Forest)\n",
        "   - **confidence**: Select High/Medium/Low\n",
        "   - **notes**: Add any observations (e.g., \"Mixed native and plantation\")\n",
        "   - **date_digitized**: Enter today's date (e.g., \"2025-10-16\")\n",
        "5. **Click OK**\n",
        "6. **Repeat** for all training areas\n",
        "\n",
        "### Step 6: Save and Verify\n",
        "\n",
        "1. **Save edits**: Click `Save Layer Edits`\n",
        "2. **Toggle editing off**: Click pencil icon again\n",
        "3. **Open attribute table**: Right-click layer â†’ `Open Attribute Table`\n",
        "4. **Verify**:\n",
        "   - All polygons have class_name and class_id\n",
        "   - No typos in class names\n",
        "   - Roughly balanced class distribution\n",
        "\n",
        "### Step 7: Export (if needed)\n",
        "\n",
        "If you created the layer elsewhere, export to the correct location:\n",
        "\n",
        "1. **Right-click layer** â†’ `Export â†’ Save Features As`\n",
        "2. **Format**: GeoJSON\n",
        "3. **File name**: `data/labels/training_polygons.geojson`\n",
        "4. **CRS**: EPSG:4326\n",
        "5. **Click OK**\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… Digitization Checklist\n",
        "\n",
        "Before proceeding to the next section, verify:\n",
        "\n",
        "- [ ] Training polygon layer created with correct attribute fields\n",
        "- [ ] 50-100 polygons per class (or best effort)\n",
        "- [ ] Polygons distributed across AOI (not clustered)\n",
        "- [ ] All polygons have class_name and class_id\n",
        "- [ ] Polygons are in homogeneous areas (avoid mixed pixels)\n",
        "- [ ] File saved to `data/labels/training_polygons.geojson`\n",
        "- [ ] CRS is EPSG:4326\n",
        "\n",
        "---\n",
        "\n",
        "**Once you've completed digitization in QGIS, return to this notebook and run the next cell to load and validate your training polygons.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŒŸ The More You Know! â€” Why These Specific Attributes?\n",
        "\n",
        "### **class_name** (Text)\n",
        "Human-readable label for visualization and interpretation. Makes it easy to understand results without looking up numeric codes.\n",
        "\n",
        "### **class_id** (Integer)\n",
        "Numeric identifier required for machine learning. Models work with numbers, not text. The ID maps directly to model output classes.\n",
        "\n",
        "### **confidence** (Text)\n",
        "Tracks your certainty about each label. During training, you might:\n",
        "- Use only \"High\" confidence polygons initially\n",
        "- Add \"Medium\" confidence later if you need more data\n",
        "- Exclude \"Low\" confidence to avoid confusing the model\n",
        "\n",
        "### **notes** (Text)\n",
        "Captures important context:\n",
        "- \"Mixed native and plantation forest\"\n",
        "- \"Recently cleared, might be transitioning\"\n",
        "- \"Shadow from clouds, verify later\"\n",
        "\n",
        "These notes help you remember why you made certain decisions and identify potential issues.\n",
        "\n",
        "### **date_digitized** (Text)\n",
        "Tracks when labels were created. Useful if:\n",
        "- You digitize in multiple sessions\n",
        "- Land cover changes over time\n",
        "- You need to match labels to specific imagery dates\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ’¡ Pro Tip: Iterative Refinement\n",
        "\n",
        "Don't aim for perfection on first pass:\n",
        "1. **First pass**: Digitize obvious, high-confidence examples (30-50 per class)\n",
        "2. **Train initial model**: See what the model learns\n",
        "3. **Analyze errors**: Where does the model fail?\n",
        "4. **Second pass**: Add more examples in confused areas\n",
        "5. **Iterate**: Repeat until performance is acceptable\n",
        "\n",
        "This iterative approach is more efficient than trying to create perfect labels upfront."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 4. Load and Validate Training Polygons\n",
        "\n",
        "**Objective:**  \n",
        "Import your digitized training polygons, validate the attribute structure, and perform initial quality checks.\n",
        "\n",
        "**Key Validations:**\n",
        "- File exists and is readable\n",
        "- Required attributes present\n",
        "- CRS is correct (EPSG:4326)\n",
        "- No missing or invalid class labels\n",
        "- Class distribution is reasonable\n",
        "- Spatial distribution across AOI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Load and Validate Training Polygons\n",
        "\n",
        "if not TRAINING_POLYGONS_PATH.exists():\n",
        "    print('âŒ Training polygons file not found!')\n",
        "    print(f'   Expected: {TRAINING_POLYGONS_PATH}')\n",
        "    print('\\nðŸ“ Complete QGIS digitization (Section 3) before proceeding.')\n",
        "else:\n",
        "    # Load training polygons\n",
        "    training_polys = gpd.read_file(TRAINING_POLYGONS_PATH)\n",
        "    print(f'âœ“ Training polygons loaded: {len(training_polys)} polygons')\n",
        "    print(f'  CRS: {training_polys.crs}')\n",
        "    \n",
        "    # Validation 1: Check required attributes\n",
        "    required_attrs = ['class_name', 'class_id']\n",
        "    missing_attrs = [attr for attr in required_attrs if attr not in training_polys.columns]\n",
        "    if missing_attrs:\n",
        "        print(f'\\nâš ï¸  Missing attributes: {missing_attrs}')\n",
        "    else:\n",
        "        print('\\nâœ“ Required attributes present')\n",
        "    \n",
        "    # Validation 2: Check CRS\n",
        "    if training_polys.crs != 'EPSG:4326':\n",
        "        print(f'\\nâš ï¸  Reprojecting from {training_polys.crs} to EPSG:4326')\n",
        "        training_polys = training_polys.to_crs('EPSG:4326')\n",
        "        print('   âœ“ Reprojected')\n",
        "    \n",
        "    # Validation 3: Check for missing values\n",
        "    missing_class_name = training_polys['class_name'].isna().sum()\n",
        "    missing_class_id = training_polys['class_id'].isna().sum()\n",
        "    if missing_class_name > 0 or missing_class_id > 0:\n",
        "        print(f'\\nâš ï¸  Missing values: {missing_class_name} class_name, {missing_class_id} class_id')\n",
        "        training_polys = training_polys.dropna(subset=['class_name', 'class_id'])\n",
        "    \n",
        "    # Validation 4: Check class names\n",
        "    expected_classes = list(class_definitions.keys())\n",
        "    actual_classes = training_polys['class_name'].unique().tolist()\n",
        "    unexpected = [c for c in actual_classes if c not in expected_classes]\n",
        "    if unexpected:\n",
        "        print(f'\\nâš ï¸  Unexpected class names: {unexpected}')\n",
        "        print(f'   Expected: {expected_classes}')\n",
        "    else:\n",
        "        print('\\nâœ“ All class names valid')\n",
        "    \n",
        "    # Validation 5: Class distribution\n",
        "    class_counts = training_polys['class_name'].value_counts().sort_index()\n",
        "    print('\\nðŸ“Š Class Distribution:')\n",
        "    print('=' * 50)\n",
        "    for class_name, count in class_counts.items():\n",
        "        bar = 'â–ˆ' * int(count / 2)\n",
        "        print(f'  {class_name:15s}: {count:3d} {bar}')\n",
        "    print('=' * 50)\n",
        "    \n",
        "    min_count = class_counts.min()\n",
        "    max_count = class_counts.max()\n",
        "    imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
        "    if imbalance_ratio > 3:\n",
        "        print(f'\\nâš ï¸  Class imbalance (ratio: {imbalance_ratio:.1f}:1)')\n",
        "    else:\n",
        "        print(f'\\nâœ“ Class distribution reasonable (ratio: {imbalance_ratio:.1f}:1)')\n",
        "    \n",
        "    # Validation 6: Spatial distribution\n",
        "    aoi = gpd.read_file(AOI_PATH)\n",
        "    training_polys['centroid'] = training_polys.geometry.centroid\n",
        "    aoi_bounds = aoi.total_bounds\n",
        "    mid_x = (aoi_bounds[0] + aoi_bounds[2]) / 2\n",
        "    mid_y = (aoi_bounds[1] + aoi_bounds[3]) / 2\n",
        "    \n",
        "    training_polys['quadrant'] = training_polys['centroid'].apply(\n",
        "        lambda p: 'NE' if p.x >= mid_x and p.y >= mid_y else\n",
        "                  'NW' if p.x < mid_x and p.y >= mid_y else\n",
        "                  'SE' if p.x >= mid_x and p.y < mid_y else 'SW'\n",
        "    )\n",
        "    \n",
        "    quadrant_counts = training_polys['quadrant'].value_counts()\n",
        "    print('\\nðŸ—ºï¸  Spatial Distribution (Quadrants):')\n",
        "    print('=' * 50)\n",
        "    for quadrant in ['NW', 'NE', 'SW', 'SE']:\n",
        "        count = quadrant_counts.get(quadrant, 0)\n",
        "        bar = 'â–ˆ' * int(count / 2)\n",
        "        print(f'  {quadrant}: {count:3d} {bar}')\n",
        "    print('=' * 50)\n",
        "    \n",
        "    if quadrant_counts.min() < len(training_polys) * 0.1:\n",
        "        print('\\nâš ï¸  Uneven spatial distribution')\n",
        "    else:\n",
        "        print('\\nâœ“ Spatial distribution looks good')\n",
        "    \n",
        "    # Store metrics\n",
        "    experiment_log['metrics']['total_polygons'] = len(training_polys)\n",
        "    experiment_log['metrics']['class_distribution'] = class_counts.to_dict()\n",
        "    experiment_log['metrics']['imbalance_ratio'] = float(imbalance_ratio)\n",
        "    experiment_log['metrics']['spatial_distribution'] = quadrant_counts.to_dict()\n",
        "    \n",
        "    print(f'\\nâœ“ Validation complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outcome:**\n",
        "\n",
        "- âœ“ Training polygons loaded and validated\n",
        "- ðŸ“Š Class distribution analyzed\n",
        "- ðŸ—ºï¸ Spatial distribution checked\n",
        "\n",
        "**ðŸ¤” Reflection Question 2:**\n",
        "\n",
        "*Look at your class distribution. Is it balanced? If not, which class(es) need more examples? Why might some classes be harder to find than others in your AOI?*\n",
        "\n",
        "[Write your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 5. Earth Engine Initialization and Sentinel-2 Access\n",
        "\n",
        "**Objective:**  \n",
        "Connect to Google Earth Engine, load Sentinel-2 imagery for your AOI, and explore which spectral bands are most useful for classification.\n",
        "\n",
        "**Key Decisions:**\n",
        "1. **Time period**: Which date range to use?\n",
        "2. **Cloud filtering**: How to handle clouds?\n",
        "3. **Band selection**: Which bands to include?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Earth Engine Initialization\n",
        "\n",
        "try:\n",
        "    ee.Initialize()\n",
        "    print('âœ“ Earth Engine initialized')\n",
        "except Exception as e:\n",
        "    print('âŒ EE initialization failed. Attempting authentication...')\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize()\n",
        "    print('âœ“ EE initialized after authentication')\n",
        "\n",
        "# Load AOI\n",
        "aoi_gdf = gpd.read_file(AOI_PATH)\n",
        "aoi_ee = geemap.geopandas_to_ee(aoi_gdf)\n",
        "print(f'âœ“ AOI loaded')\n",
        "\n",
        "# Define temporal parameters (Austral growing season â€” expanded)\n",
        "START_DATE = '2018-10-01'\n",
        "END_DATE = '2019-04-30'\n",
        "print(f'\\nðŸ“… Time period: {START_DATE} to {END_DATE}')\n",
        "print('   (Austral growing season â€” expanded)')\n",
        "\n",
        "experiment_log['decisions']['temporal_range'] = {\n",
        "    'start': START_DATE,\n",
        "    'end': END_DATE,\n",
        "    'rationale': 'Austral growing season (expanded shoulder months)'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŒŸ The More You Know! â€” Sentinel-2 Bands\n",
        "\n",
        "Sentinel-2 has **13 spectral bands**:\n",
        "\n",
        "### **10m Resolution** (Highest detail)\n",
        "- **B2** (Blue, 490 nm)\n",
        "- **B3** (Green, 560 nm)\n",
        "- **B4** (Red, 665 nm)\n",
        "- **B8** (NIR, 842 nm)\n",
        "\n",
        "### **20m Resolution**\n",
        "- **B5-B7** (Red Edge)\n",
        "- **B8A** (Narrow NIR)\n",
        "- **B11-B12** (SWIR)\n",
        "\n",
        "### Band Selection Options:\n",
        "\n",
        "**Option 1: RGB Only** (B4, B3, B2)\n",
        "- âœ… Smallest, fastest\n",
        "- âŒ Misses vegetation info\n",
        "\n",
        "**Option 2: RGB + NIR** (B4, B3, B2, B8) â­ Recommended\n",
        "- âœ… Adds vegetation (NDVI)\n",
        "- âœ… Better water detection (NDWI)\n",
        "- âœ… Still manageable size\n",
        "\n",
        "**Option 3: 10-band Multi** (all bands)\n",
        "- âœ… Maximum information\n",
        "- âŒ Larger files, more complex\n",
        "\n",
        "We'll use **Option 2 (RGB + NIR)** for Week 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5 continued) Access Sentinel-2 Imagery\n",
        "\n",
        "# Load Sentinel-2 collection\n",
        "s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
        "    .filterBounds(aoi_ee) \\\n",
        "    .filterDate(START_DATE, END_DATE) \\\n",
        "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n",
        "\n",
        "image_count = s2.size().getInfo()\n",
        "print(f'\\nðŸ“¡ Found {image_count} Sentinel-2 images')\n",
        "\n",
        "if image_count == 0:\n",
        "    print('\\nâš ï¸  No images found! Try:')\n",
        "    print('   1. Expanding date range')\n",
        "    print('   2. Relaxing cloud filter')\n",
        "else:\n",
        "    # Cloud masking function\n",
        "    def mask_s2_clouds(image):\n",
        "        qa = image.select('QA60')\n",
        "        cloud_mask = qa.bitwiseAnd(1 << 10).eq(0).And(\n",
        "                     qa.bitwiseAnd(1 << 11).eq(0))\n",
        "        return image.updateMask(cloud_mask)\n",
        "    \n",
        "    # Apply cloud masking and create median composite\n",
        "    s2_masked = s2.map(mask_s2_clouds)\n",
        "    s2_median = s2_masked.median().clip(aoi_ee)\n",
        "    \n",
        "    print('âœ“ Cloud masking applied')\n",
        "    print('âœ“ Median composite created')\n",
        "    \n",
        "    # Select bands\n",
        "    bands_rgb_nir = ['B4', 'B3', 'B2', 'B8']  # Red, Green, Blue, NIR\n",
        "    s2_rgb_nir = s2_median.select(bands_rgb_nir)\n",
        "    \n",
        "    bands_multi = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12']\n",
        "    s2_multi = s2_median.select(bands_multi).resample('bilinear').reproject(\n",
        "        crs='EPSG:4326', scale=10\n",
        "    )\n",
        "    \n",
        "    print(f'\\nðŸ“Š Band Selections:')\n",
        "    print(f'   RGB+NIR (4 bands): {bands_rgb_nir}')\n",
        "    print(f'   Multi (10 bands): {bands_multi}')\n",
        "    \n",
        "    # Calculate spectral indices\n",
        "    nir = s2_median.select('B8')\n",
        "    red = s2_median.select('B4')\n",
        "    green = s2_median.select('B3')\n",
        "    swir1 = s2_median.select('B11')\n",
        "    \n",
        "    ndvi = nir.subtract(red).divide(nir.add(red)).rename('NDVI')\n",
        "    ndwi = green.subtract(nir).divide(green.add(nir)).rename('NDWI')\n",
        "    ndbi = swir1.subtract(nir).divide(swir1.add(nir)).rename('NDBI')\n",
        "    \n",
        "    s2_with_indices = s2_median.addBands([ndvi, ndwi, ndbi])\n",
        "    \n",
        "    print('âœ“ Spectral indices calculated (NDVI, NDWI, NDBI)')\n",
        "    \n",
        "    # Store decisions\n",
        "    experiment_log['decisions']['imagery'] = {\n",
        "        'collection': 'COPERNICUS/S2_SR_HARMONIZED',\n",
        "        'cloud_threshold': 20,\n",
        "        'composite_method': 'median',\n",
        "        'image_count': int(image_count),\n",
        "        'bands_rgb_nir': bands_rgb_nir,\n",
        "        'bands_multi': bands_multi\n",
        "    }\n",
        "    \n",
        "    print('\\nâœ“ Sentinel-2 data ready for patch extraction')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outcome:**\n",
        "\n",
        "- âœ“ Earth Engine connected\n",
        "- âœ“ Sentinel-2 composite created\n",
        "- âœ“ Two band configurations ready (RGB+NIR and 10-band)\n",
        "- âœ“ Spectral indices calculated\n",
        "\n",
        "**ðŸ¤” Reflection Question 3:**\n",
        "\n",
        "*Why use a median composite instead of a single image? What are the tradeoffs?*\n",
        "\n",
        "*Hint: Think about clouds, temporal variability, and phenology.*\n",
        "\n",
        "[Write your answer here]\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… Part A Complete!\n",
        "\n",
        "You've completed:\n",
        "1. âœ“ Environment setup and class definitions\n",
        "2. âœ“ QGIS digitization guide\n",
        "3. âœ“ Training polygon validation\n",
        "4. âœ“ Sentinel-2 data access\n",
        "\n",
        "**Next:** Continue to **Part B** for:\n",
        "- Patch extraction\n",
        "- Spectral analysis\n",
        "- Spatial train/val splitting\n",
        "- Storage format comparison\n",
        "- Experiment logging\n",
        "- Self-assessment"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (geoai)",
      "language": "python",
      "name": "geoai"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
